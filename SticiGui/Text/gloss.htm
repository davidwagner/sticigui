<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<script language="JavaScript1.4" type="text/javascript"><!--
	pageModDate = "19 March 2012 15:24 PDT";
	// copyright 1997-2012 by P.B. Stark, statistics.berkeley.edu/~stark.
    // All rights reserved.
// -->
</script>

<script type="text/javascript" src="../../Java/Jquery/Current/jquery.min.js"></script>
<script type="text/javascript" src="../../Java/Jquery/Current/jquery.bullseye-1.0.min.js"></script>

<html>

<head>
<link rel="stylesheet" type="text/css" href="../Graphics/sticiGuiDefault.css"/>
<link rel="canonical" href="http://statistics.berkeley.edu/~stark/SticiGui/Text/gloss.htm" />

<title>Glossary</title>
</head>

<script language="JavaScript1.2" src="../../Java/irGrade.js"></script>

<body onload="window.name='glossWin';" onUnload="killApplets()">

<h1>
    Glossary of Statistical Terms
</h1>

<p align="center">
    You can use the &quot;find&quot; (find in frame, find in page)
    feature of your browser to search the glossary.
</p>

<hr />
<p align="center">
<script language="JavaScript1.2"><!--
    for (var i=0; i < alphabet.length; i++) {
        document.write('<a class="glossRef" href="#' + alphabet[i] + '">' + Alphabet[i] + '</a> ');
    }
// -->
</script>
</p>

<dl>
<dd>
<hr />
    <p><a id="0to9"></a>0-9</p>
</dd>
<dt>
    <a id="0_1_box"></a>0-1 box
</dt>
<dd>
    A box of numbered tickets, in which each ticket is numbered either 0 or 1.
    See <a class="glossRef" href="#box_model">box model</a>.
</dd>

<dd>
<hr />
    <p><a id="a"></a>A </p>
</dd>

<dt>
   <a id="affine"></a>Affine transformation.
</dt>
<dd>
    See <a class="glossRef" href="#transformation">transformation</a>.
</dd>

<dt>
  <a id="affirming_the_antecedent"></a>Affirming the antecedent.
</dt>
<dd>
    A valid logical argument that concludes from the <a class="glossRef" href="#premise">premise</a>
    <span class="math">A &rarr; B</span>
    and the premise <span class="meth">A</span> that therefore, <span class="math">B</span>
    is true.
    The name comes from the fact that the argument affirms
    (i.e., asserts as true) the
    <a class="glossRef" href="#antecedent">antecedent</a> (<span class="math">A</span>) in the
    <a class="glossRef" href="#conditional">conditional</a>.
</dd>

<dt>
  <a id="affirming_the_consequent"></a>Affirming the consequent.
</dt>
<dd>
    A logical fallacy that argues from the <a class="glossRef" href="#premise">premise</a>
    <span class="math">A &rarr; B</span>
    and the premise <span class="meth">B</span> that therefore, <span class="math">A</span> is true.
    The name comes from the fact that the argument affirms
    (i.e., asserts as true) the
    <a class="glossRef" href="#consequent">consequent</a> (<span class="math">B</span>) in the
    <a class="glossRef" href="#conditional">conditional</a>.
</dd>


<dt>
   <a id="alternative"></a>Alternative Hypothesis.
</dt>
<dd>
    In <a class="glossRef" href="#hypothesis_test">hypothesis testing</a>, a <a class="glossRef" href="#null_hypothesis">null
    hypothesis</a> (typically that there is no effect) is compared with an alternative
    hypothesis (typically that there is an effect, or that there is an effect of a particular
    sign).
    For example, in evaluating whether a new cancer remedy works, the null hypothesis
    typically would be that the remedy does not work, while the alternative
    hypothesis would be that the remedy does work.
    When the data are sufficiently improbable under the assumption that the null
    hypothesis is true, the null hypothesis is rejected in favor of the alternative
    hypothesis. (This does not imply that the data are probable under the assumption that the
    alternative hypothesis is true, nor that the null hypothesis is false, nor that the
    alternative hypothesis is true. Confused? Take a course in Statistics!)
</dd>

<dt>
    <a id="#AND"></a><a id="#and"></a>and, &amp;, conjunction, logical conjunction, &and;.
</dt>
<dd>
    An operation on two logical <a class="glossRef" href="#proposition">propositions</a>.
    If <em>p</em> and <em>q</em> are two
    <a class="glossRef" href="#proposition">propositions</a>,
    (<em>p</em> &amp; <em>q</em>) is a proposition that
    is true if both <em>p</em> and <em>q</em>
    are true; otherwise, (<em>p</em> &amp; <em>q</em>) is false.
    The operation &amp; is sometimes represented by the symbol &and;.
</dd>

<dt>
   <a id="ante"></a>Ante.
</dt>
<dd>
    The up-front cost of a bet: the money you must pay to play the game. From Latin for
    &quot;before.&quot;
</dd>

<dt>
   <a id="antecedent"></a>Antecedent.
</dt>
<dd>
   In a <a class="glossRef" href="#conditional">conditional</a> <span class="math">p &rarr; q</span>,
   the antecedent is <span class="math">p</span>.
</dd>

<dt>
   <a id="appealToIgnorance"></a>Appeal to Ignorance.
</dt>
<dd>
    A logical fallacy: taking the absence of evidence to be evidence of absence.
    If something is not known to be false, assume that it is true; or if something is not known to
    be true, assume that it is false.
    For example, if I have no reason to think that anyone in Tajikistan wish me well,
    that is not evidence that nobody in Tajikistan wishes me well.
</dd>

<dt>
    <a id="applet"></a>Applet.
</dt>
<dd>
    An applet is a small program that is automatically downloaded from a website to your
    computer when you visit a particular web page; it allows a page to be interactive&mdash;to
    respond to your input. The applet runs on your computer, not the computer that hosted the
    web page. These materials contain many applets to illustrate
    statistical concepts and to help you to analyze data.
    Many of them are accessible directly from the
    <a href="../../Java/Html/index.htm" target="bookWin">tools page</a>.
</dd>

<dt>
    <a id="association"></a>Association.
</dt>
<dd>
    Two <a class="glossRef" href="#variable">variables</a> are associated if some of the variability of one
    can be accounted for by the other. In a <a class="glossRef" href="#scatterplot">scatterplot</a> of the two
    variables, if the scatter in the values of the variable plotted on the vertical axis is
    smaller in narrow ranges of the variable plotted on the horizontal axis (<em>i.e.,</em> in
    vertical &quot;slices&quot;) than it is overall, the two variables are associated.
    The <a class="glossRef" href="#correlation_coef">correlation coefficient</a> is a measure of linear
    association,
    which is a special case of association in which large values of one variable tend to occur
    with large values of the other, and small values of one tend to occur with small values of
    the other (positive association), or in which large values of one tend to occur with small
    values of the other, and <em>vice versa </em>(negative association).
</dd>

<dt>
   <a id="average"></a>Average.
</dt>
<dd>
    A sometimes vague term. It usually denotes the <a class="glossRef" href="#mean">arithmetic mean</a>, but
    it can also denote the <a class="glossRef" href="#median">median</a>, the <a class="glossRef" href="#mode">mode</a>, the
    <a class="glossRef" href="#geometric_mean">geometric mean</a>, and weighted means, among other things.
</dd>

<dt>
   <a id="axioms_of_probability"></a>Axioms of Probability.
</dt>
<dd>
    There are three axioms of probability: (1) Chances are always at least zero. (2) The
    chance that <em>something</em> happens is 100%. (3) If two events cannot both occur at the
    same time (if they are <a class="glossRef" href="#disjoint">disjoint or mutually exclusive</a>), the chance
    that either one occurs is the sum of the chances that each occurs. For example, consider
    an experiment that consists of tossing a coin once. The first axiom says that the chance
    that the coin lands heads, for instance, must be at least zero. The second axiom says that
    the chance that the coin either lands heads or lands tails or lands on its edge or doesn't
    land at all is 100%. The third axiom says that the chance that the coin either lands heads
    or lands tails is the sum of the chance that the coin lands heads and the chance that the
    coin lands tails, because both cannot occur in the same coin toss. All other mathematical
    facts about probability can be derived from these three axioms. For example, it is true
    that the chance that an event does not occur is (100% &minus; the chance that the event occurs).
    This is a consequence of the second and third axioms.
</dd>

<dt>&nbsp;
</dt>
<dd>
<hr />
    <p><a id="b"></a>B </p>

</dd>

<dt>
   <a id="base_rate_fallacy"></a>Base rate fallacy.
</dt>
<dd>
    The base rate fallacy consists of failing to take into account prior probabilities (base rates) when
    computing <a class="glossRef" href="#conditional_probability">conditional probabilities</>
    from other conditional probabilities.  It is related to
    <a class="glossRef" href="#prosecutors_fallacy">the Prosecutor's Fallacy</a>.
    For instance, suppose that a test for the presence of some condition has a 1% chance of a
    false positive result (the test says the condition is present when it is not) and a 1% chance
    of a false negative result (the test says the condition is absent when the condition is present),
    so the exam is 99% accurate.
    What is the chance that an item that tests positive really has the condition?
    The intuitive answer is 99%, but that is not necessarily true: the correct answer depends on the
    fraction <span class="math">f</span> of items in the population that have the condition
    (and on whether the item tested is selected at random from the population).
    The chance that a randomly selected item tests positive is
    <span class="math">0.99&times;f/(0.99&times;f + 0.01&times;(1&minus;f))</span>,
    which could be much smaller than
    99% if <span class="math">f</span> is small.
    See <a class="glossRef" href="#bayes_rule">Bayes' Rule</a>.
</dd>

<dt>
   <a id="bayes_rule"></a>Bayes' Rule.
</dt>
<dd>
    Bayes' rule expresses the <a class="glossRef" href="#conditional_probability">conditional
    probability</a> of the <a class="glossRef" href="#event">event</a>
    <span class="math">A</span> given the <a class="glossRef" href="#event">event</a>
    <span class="math">B</span>
    in terms of the <a class="glossRef" href="#conditional_probability">conditional probability</a>
    of the <a class="glossRef" href="#event">event</a>
    <span class="math">B</span> given the <a class="glossRef" href="#event">event</a> <span class="math">A</span>
    and the unconditional probability of <span class="math">A</span>:
    <p class="math">
    P(A|B) = P(B|A) <em>&times;</em>P(A)/<big>(</big>
    P(B|A)<em>&times;</em>P(A) + P(B|A<sup>c</sup>)
    &times;P(A<sup>c</sup>)
    <big>)</big>.
    </p>
    In this expression, the unconditional probability of <span class="math">A</span> is also
    called the <span class="termOfArt">prior probability</span> of <span class="math">A</span>,
    because it is the probability assigned to <span class="math">A</span> prior to observing
    any data.
    Similarly, in this context, <span class="math">P(A|B)</span> is called the
    <span class="termOfArt">posterior probability of <span class="math">A</span> given
    <span class="math">B</span></span>, because it is the probability of <span class="math">A</span>
    updated to reflect (i.e., to condition on) the fact that <span class="math">B</span> was observed
    to occur.
</dd>

<dt>
  <a id="bernoulli_inequality"></a>Bernoulli's Inequality.
</dt>
<dd>
   The Bernoulli Inequality says that if <span class="math">x &ge; &minus;1</span> then
   <span class="math">(1+x)<sup>n</sup> &ge; 1 + nx</span>
   for every integer <span class="math">n &ge; 0</span>.
   If <span class="math">n</span> is even, the inequality holds for all <span class="math">x</span>.
</dd>


<dt>
   <a id="bias"></a>Bias.
</dt>
<dd>
    A measurement procedure or <a class="glossRef" href="#estimator">estimator</a> is said to be biased if,
    on the average, it gives an answer that differs from the truth.
    The bias is the average (<a class="glossRef" href="#expectation">expected</a>) difference between the
    measurement and the truth. For
    example, if you get on the scale with clothes on, that biases the measurement to be larger
    than your true weight (this would be a positive bias). The design of an experiment or of a
    survey can also lead to bias. Bias can be deliberate, but it is not necessarily so. See
    also <a class="glossRef" href="#nonresponse_bias">nonresponse bias</a>.
</dd>

<dt>
   <a id="bimodal"></a>Bimodal.
</dt>
<dd>
    Having two <a class="glossRef" href="#mode">modes</a>.
</dd>

<dt>
   <a id="bin"></a>Bin.
</dt>
<dd>
    See <a class="glossRef" href="#class_interval">class interval</a>.
</dd>

<dt>
   <a id="binomial_coefficient"></a>Binomial Coefficient.
</dt>
<dd>
    See <a class="glossRef" href="#combinations">combinations</a>.
</dd>

<dt>
  <a id="binomial"></a>Binomial Distribution.
</dt>
<dd>
    A random variable has a binomial distribution (with parameters
    <em>n</em> and <em>p</em>) if
    it is the number of &quot;successes&quot; in a fixed number <em>n</em> of
    <a class="glossRef" href="#independent">independent</a> random trials, all of which have the same
    probability <em>p</em>
    of resulting in &quot;success.&quot; Under these assumptions, the probability of <em>k</em>
    successes (and <em>n&minus;k </em>failures) is
    <em><sub>n</sub>C<sub>k</sub> p<sup>k</sup></em>(1<em>&minus;p</em>)<sup><em>n&minus;k</em></sup>,
    where <em><sub>n</sub>C<sub>k</sub></em> is the number of
    <a class="glossRef" href="#combinations">combinations</a>
    of <em>n</em> objects taken <em>k</em> at a time:
    <em><sub>n</sub>C<sub>k</sub> = n</em>!<em>/</em>(<em>k</em>!(<em>n&minus;k</em>)!).
    The <a class="glossRef" href="#expectation">expected value</a> of a
    <a class="glossRef" href="#random_variable">random
    variable</a> with the Binomial distribution is <em>n&times;p</em>,
    and the standard error of a
    random variable with the Binomial distribution is
    <big>(</big><em>n&times;p&times;</em>(1
    &minus; <em>p</em>)<big>)</big><sup>&frac12;</sup>.
    <a href="../../Java/Html/BinHist.htm" target="lablet">This page</a>
    shows the <a class="glossRef" href="#probability_histogram">probability histogram</a> of the binomial
    distribution.
</dd>

<dt>
  <a id="binomial_theorem"></a>Binomial Theorem.
</dt>
<dd>
   The Binomial theorem says that <span class="math">(x+y)<sup>n</sup> = x<sup>n</sup> + nx<sup>n&minus;1</sup>y +
     &hellip; + <sub>n</sub>C<sub>k</sub>x<sup>n&minus;k</sup>y<sup>k</sup> + &hellip; + y<sup>n</sup></span>.
</dd>

<dt>
  <a id="bivariate"></a>Bivariate.
</dt>
<dd>
    Having or having to do with two <a class="glossRef" href="#variable">variables</a>.
    For example, bivariate data are data where we
    have two measurements of each &quot;individual.&quot; These measurements might be the
    heights and weights of a group of people (an &quot;individual&quot; is a person), the
    heights of fathers and sons (an &quot;individual&quot; is a father-son pair), the pressure
    and temperature of a fixed volume of gas (an &quot;individual&quot; is the volume of gas
    under a certain set of experimental conditions), <em>etc.</em>
    <a class="glossRef" href="#scatterplot">Scatterplots</a>,
    the <a class="glossRef" href="#correlation_coef">correlation coefficient</a>,
    and <a class="glossRef" href="#regression">regression</a>
    make sense for bivariate data but not <a class="glossRef" href="#univariate">univariate</a> data.
    <em>C.f.</em>
    <a class="glossRef" href="#univariate">univariate</a>.
</dd>

<dt>
   <a id="blind"></a>Blind, Blind Experiment.
</dt>
<dd>
    In a blind experiment, the <a class="glossRef" href="#subject">subjects</a> do not know whether they are
    in the <a class="glossRef" href="#treatment_group">treatment group</a> or the
    <a class="glossRef" href="#control_group">control
    group</a>. In order to have a blind experiment with human subjects, it is usually
    necessary to administer a <a class="glossRef" href="#placebo">placebo</a> to the control group.
</dd>

<dt>
   <a id="bootstrap"></a>Bootstrap estimate of <a class="glossRef" href="#se">Standard Error</a>.
</dt>
<dd>
    The name for this idea comes from the idiom &quot;to pull oneself up by one's
    bootstraps,&quot; which connotes getting out of a hole without anything to stand on.
    The idea of the bootstrap is to assume, for the purposes of estimating uncertainties,
    that the sample is the population, then use the <a class="glossRef" href="#se">SE</a> for sampling from the
    sample to estimate the <a class="glossRef" href="#se">SE</a> of sampling from the population.
    For sampling from a box of numbers,
    the SD of the sample is the bootstrap estimate of the SD of the box from which the
    sample is drawn.
    For <a class="glossRef" href="#sample_percentage">sample percentages</a>, this takes a particularly
    simple form:
    the <a class="glossRef" href="#se">SE</a> of the <a class="glossRef" href="#sample_percentage">sample percentage</a>
    of <em>n</em>
    draws from a box, with replacement, is
    <a class="glossRef" href="#sd">SD</a>(box)/<em>n</em><sup>&frac12;</sup>,
    where for a box that contains only zeros and ones, <a class="glossRef" href="#sd">SD</a>(box) =
    <big>(</big>(fraction
    of ones in box)<em>&times;</em>(fraction of zeros in box)
    <big>)</big><sup>&frac12;</sup>.
    The bootstrap estimate
    of the <a class="glossRef" href="#se">SE</a> of the <a class="glossRef" href="#sample_percentage">sample percentage</a>
    consists of estimating <a class="glossRef" href="#sd">SD</a>(box) by
    <big>(</big>(fraction of ones in sample)<em>&times;</em>(fraction
    of zeros in sample)<big>)</big><sup>&frac12;</sup>.
    When the sample size is large, this approximation is
    likely to be good.
</dd>

<dt>
   <a id="box_model"></a>Box model.
</dt>
<dd>
    An analogy between an experiment and drawing numbered tickets &quot;at random&quot; from
    a box with replacement. For example, suppose we are trying to evaluate a cold remedy by
    giving it or a placebo to a group of <em>n</em> individuals, randomly choosing half the
    individuals to receive the remedy and half to receive the placebo. Consider the median
    time to recovery for all the individuals (we assume everyone recovers from the cold
    eventually; to simplify things, we also assume that no one recovered in exactly the median
    time, and that <em>n</em> is even). By definition, half the individuals got better in less
    than the median time, and half in more than the median time. The individuals who received
    the treatment are a <a class="glossRef" href="#random_sample">random sample</a> of
    <a class="glossRef" href="#sample_size">size</a>
    <em>n</em>/2 from the set of <em>n</em> subjects, half of whom got better in less than
    median time, and half in longer than median time. If the remedy is ineffective, the number
    of subjects who received the remedy and who recovered in less than median time is like the
    sum of <em>n</em>/2 draws with replacement from a box with two tickets in it: one with a
    &quot;1&quot; on it, and one with a &quot;0&quot; on it.
    <a href="../../Java/Html/SampleDist.htm" target="lablet">This page</a> illustrates
    the sampling distribution of random draws with or without from a box of numbered tickets.
</dd>

<dt>
   <a id="breakdown_point"></a>Breakdown Point.
</dt>
<dd>
    The breakdown point of an <a class="glossRef" href="#estimator">estimator</a> is the smallest fraction of
    observations one must corrupt to make the estimator take any value one wants.
</dd>

<dd>
    <hr />
    <p><a id="c"></a>C </p>

</dd>

<dt>
  <a id="categorical"></a>Categorical Variable.
</dt>
<dd>
    A <a class="glossRef" href="#variable">variable</a> whose value ranges over categories, such as {red,
    green, blue}, {male, female}, {Arizona, California, Montana, New York}, {short, tall},
    {Asian, African-American, Caucasian, Hispanic, Native American, Polynesian}, {straight,
    curly}, <em>etc. </em>Some categorical variables are <a class="glossRef" href="#ordinal">ordinal</a>. The
    distinction between categorical variables and
    <a class="glossRef" href="#qualitative">qualitative variables</a>
    is a bit blurry. <em>C.f.</em> <a class="glossRef" href="#quantitative">quantitative variable</a>.
</dd>

<dt>
  <a id="causation"></a>Causation, causal relation.
</dt>
<dd>
    Two variables are causally related if changes in the value of one cause the other to
    change. For example, if one heats a rigid container filled with a gas, that causes the
    pressure of the gas in the container to increase.
    Two variables can be <a class="glossRef" href="#association">associated</a> without
    having any causal relation, and even if two
    variables have a causal relation, their <a class="glossRef" href="#correlation">correlation</a> can be
    small or zero.
</dd>

<dt>
   <a id="clt"></a>Central Limit Theorem.
</dt>
<dd>
    The central limit theorem states that the <a class="glossRef" href="#probability_histogram">probability
    histograms</a> of the <a class="glossRef" href="#sample_mean">sample mean</a>
    and <a class="glossRef" href="#sample_sum">sample sum</a> of <em>n</em> draws with replacement
    from a box of labeled tickets converge to a
    <a class="glossRef" href="#normal_curve">normal curve</a> as the
    <a class="glossRef" href="#sample_size">sample size</a> <em>n</em> grows, in the following sense:
    As <em>n</em> grows, the area of the probability histogram for any
    range of values approaches the area under the <a class="glossRef" href="#normal_curve">normal curve</a>
    for the same range of values, converted to <a class="glossRef" href="#standard_units">standard units</a>.
    See also <a class="glossRef" href="#normal_approximation">the normal approximation</a>.
</dd>

<dt>
  <a id="certain"></a>Certain Event.
</dt>
<dd>
    An <a class="glossRef" href="#event">event</a> is <em>certain</em> if its
    <a class="glossRef" href="#probability">probability</a> is 100%.
    Even if an event is certain, it might not occur.
    However, by the <a class="glossRef" href="#complement_rule">complement rule</a>,
    the chance that it does not occur is 0%.
</dd>

<dt>
   <a id="chance_variation"></a>Chance variation, chance error.
</dt>
<dd>
    A <a class="glossRef" href="#random_variable">random variable</a> can be decomposed into
    a sum of its <a class="glossRef" href="#expectation">expected value</a> and chance variation around
    its expected value.  The expected value of the chance variation is zero; the
    <a class="glossRef" href="#se">standard error</a> of the chance variation is the same as the
    <a class="glossRef" href="#se">standard error</a> of the random variable&mdash;the size of a
     &quot;typical&quot; difference between the <a class="glossRef" href="#random_variable">random variable</a>
    and its <a class="glossRef" href="#expectation">expected value</a>.
    See also <a class="glossRef" href="#sampling_error">sampling error</a>.
</dd>

<dt>
   <a id="changeofv"></a>Change of Units or Variables.
</dt>
<dd>
See also <a class="glossRef" href="#transformation">transformation</a>.
</dd>

<dt>
   <a id="chebychev"></a>Chebychev's Inequality.
</dt>
<dd>
    For lists: For every number <em>k</em>&gt;0, the fraction of elements in a list that are
    <em>k</em> <a class="glossRef" href="#sd">SD</a>'s or further from the
    <a class="glossRef" href="#mean">arithmetic mean</a> of
    the list is at most 1/<em>k<sup>2</sup></em>.<br />
    For <a class="glossRef" href="#random_variable">random variables</a>:
    For every number <em>k</em>&gt;0, the
    probability that a <a class="glossRef" href="#random_variable">random variable</a> X is <em>k</em>
    <a class="glossRef" href="#se">SE</a>s or further from its <a class="glossRef" href="#expectation">expected value</a> is at
    most 1/<em>k<sup>2</sup></em>.
</dd>

<dt>
   <a id="chi-square"></a>Chi-square curve.
</dt>
<dd>
    The chi-square curve is a family of curves that depend on a parameter called
    degrees of freedom (<em>d.f.</em>).
    The chi-square curve is an approximation to the
    <a class="glossRef" href="#probability_histogram">probability histogram</a> of the
    <a class="glossRef" href="#chi-square-statistic"><em>chi-square statistic</em></a>
    for <a class="glossRef" href="#multinomial_distribution">multinomial</a> model if the
    <a class="glossRef" href="#expectation">expected</a> number of outcomes in each category is
    large.
    The chi-square curve is positive, and its total area is 100%, so we can think of
    it as the probability histogram of a <a class="glossRef" href="#random_variable">random variable</a>.
    The balance point of the curve is <em>d.f.</em>, so the expected value of the
    corresponding random variable would equal <em>d.f.</em>.
    The <a class="glossRef" href="#se">standard error</a> of the corresponding random variable would be
    (2&times;<em>d.f.</em>)<sup>&frac12;</sup>.
    As <em>d.f.</em> grows, the shape of the chi-square curve approaches the shape of
    the <a class="glossRef" href="#normal_curve">normal curve</a>.
    <a href="../../Java/Html/chiHiLite.htm" target="lablet">This page</a> shows
    the chi-square curve.
</dd>

<dt>
    <a id="chi-square_statistic"></a>Chi-square Statistic.
</dt>
<dd>
    The <em>chi-square</em> statistic is used to measure the agreement between
    <a class="glossRef" href="#categorical">categorical</a> data and a
    <a class="glossRef" href="#multinomial_distribution">multinomial model</a> that predicts
    the relative frequency of outcomes in each possible category.
    Suppose there are <em>n</em> <a class="glossRef" href="#independent">independent</a> trials,
    each of which can result in one of <em>k</em> possible outcomes.
    Suppose that in each trial, the probability that outcome
    <em>i</em> occurs is <em>p</em><sub><em>i</em></sub>,
    for <em>i</em>&nbsp;=&nbsp;1, 2, &hellip; , <em>k</em>,
    and that these probabilities are the same in every trial.
    The expected number of times outcome 1 occurs in the <em>n</em> trials is
    <em>n</em>&times;<em>p</em><sub>1</sub>; more generally, the expected number of
    times outcome <em>i</em> occurs is
</dd>
<p class="math">
    expected<sub><em>i</em></sub>&nbsp;=&nbsp;<em>n</em>&times;<em>p</em><sub><em>i</em></sub>.
</p>
<dd>
    If the model be correct, we would expect the <em>n</em> trials to result in outcome
    <em>i</em> about <em>n</em>&times;<em>p</em><sub><em>i</em></sub> times, give or take
    a bit.
    Let observed<sub><em>i</em></sub> denote the number of times an outcome of type <em>i</em>
    occurs in the <em>n</em> trials, for <em>i</em>&nbsp;=&nbsp;1, 2,
    &hellip; , <em>k</em>.
    The <em>chi-squared statistic</em> summarizes the discrepancies between the
    expected number of times each outcome occurs (assuming that the model is true)
    and the observed number of times each outcome occurs, by summing
    the squares of the discrepancies, normalized by the expected numbers, over all
    the categories:
</dd>
<p class="math">
    <em>chi-squared</em> =
</p>

<p class="math">
    <big>(</big>observed<sub>1</sub>&nbsp;&minus;&nbsp;expected<sub>1</sub><big>)</big><sup>2</sup>/expected<sub>1</sub>
    +
    <big>(</big>observed<sub>2</sub>&nbsp;&minus;&nbsp;expected<sub>2</sub><big>)</big><sup>2</sup>/expected<sub>2</sub>
    +
    &hellip;
    +
    <big>(</big>observed<sub><em>k</em></sub>&nbsp;&minus;&nbsp;expected<sub><em>k</em></sub><big>)</big><sup>2</sup>/expected<sub><em>k</em></sub>.
</p>
<dd>
    As the sample size <em>n</em> increases, if the model is correct,
    the sampling distribution of the <em>chi-squared statistic</em>
    is approximated increasingly well by the chi-squared curve with
</dd>
<p class="math">
    (#categories &minus; 1) = <em>k</em> &minus; 1
</p>
<dd>
    degrees of
    freedom (<em>d.f.</em>), in the sense that the chance that the <em>chi-squared statistic</em>
    is in any given range grows closer and closer to the area under the Chi-Squared curve over
    the same range.
    <a href="../../Java/Html/SampleChi.htm" target="lablet">This page</a> illustrates
    the sampling distribution of the <em>chi-square statistic</em>.
</dd>

<dt>
   <a id="class_boundary"></a>Class Boundary.
</dt>
<dd>
    A point that is the left endpoint of one <a class="glossRef" href="#class_interval">class interval</a>,
    and the right endpoint of another <a class="glossRef" href="#class_interval">class interval</a>.
</dd>

<dt>
  <a id="class_interval"></a>Class Interval.
</dt>
<dd>
    In plotting a <a class="glossRef" href="#histogram">histogram</a>, one starts by dividing the range of
    values into a set of non-overlapping intervals, called<em> class intervals</em>, in such a
    way that every datum is contained in some class interval.
    See the related entries <a class="glossRef" href="#class_boundary">class boundary</a> and
    <a class="glossRef" href="#endpoint_convention">endpoint
    convention</a>.
</dd>

<dt>
   <a id="cluster_sample"></a>Cluster Sample.
</dt>
<dd>
    In a cluster sample, the <a class="glossRef" href="#sampling_unit">sampling unit</a> is a
    collection of population units, not single population units.
    For example, techniques for adjusting the U.S. census start with a sample of
    geographic blocks, then
    (try to) enumerate all inhabitants of the blocks in the sample to obtain a sample
    of people.
    This is an example of a cluster sample.
    (The blocks are chosen separately from different strata, so the overall design is a
    <a class="glossRef" href="#stratified_cluster_sample">stratified cluster sample</a>.)
</dd>

<dt>
  <a id="combinations"></a>Combinations.
</dt>
<dd>
    The number of combinations of <em>n </em>things taken <em>k</em> at a time is the number
    of ways of picking a subset of <em>k</em> of the <em>n</em> things, without replacement,
    and without regard to the order in which the elements of the subset are picked.
    The number
    of such combinations is <em><sub>n</sub>C<sub>k</sub></em> =
    <em>n</em>!/(<em>k</em>!(<em>n&minus;k</em>)!),
    where <em>k</em>! (pronounced &quot;<em>k</em> <a class="glossRef" href="#factorial">factorial</a>&quot;)
    is <em>k&times;</em>(<em>k</em>&minus;1)&times;(<em>k</em>&minus;2)&times; &hellip; &times; 1.
    The numbers <em><sub>n</sub>C<sub>k</sub></em>
    are also called the <em>Binomial coefficients</em>. From a set that has <em>n</em>
    elements one can form a total of 2<sup><em>n</em></sup> subsets of all sizes. For example,
    from the set {a, b, c}, which has 3 elements, one can form the 2<sup>3</sup> = 8 subsets
    {}, {a}, {b}, {c}, {a,b}, {a,c}, {b,c}, {a,b,c}.
    Because the number of subsets with <em>k</em>
    elements one can form from a set with <em>n</em>
    elements is <em><sub>n</sub>C<sub>k</sub></em>,
    and the total number of subsets of a set is the sum of the numbers of possible subsets of
    each size, it follows that
    <em><sub>n</sub>C</em><sub>0</sub>+<em><sub>n</sub>C</em><sub>1</sub>+<em><sub>n</sub>C</em><sub>2</sub>+
    <em>&hellip; +<sub>n</sub>C<sub>n</sub></em> = 2<sup><em>n</em></sup>.
    The <a href="../../Java/Html/StatCalc.htm" target="lablet">calculator</a>
    has a button (nCm) that lets you compute the number of combinations of
    <em>m</em> things chosen from a set of <em>n</em> things.
    To use the button, first
    type the value of <em>n</em>, then push the nCm button, then type the value of <em>m</em>,
    then press the &quot;=&quot; button.
</dd>

<dt>
   <a id="complement"></a>Complement.
</dt>
<dd>
    The complement of a <a class="glossRef" href="#subset">subset</a> of a given <a class="glossRef" href="#set">set</a> is
    the collection of all <a class="glossRef" href="#element">elements</a> of the <a class="glossRef" href="#set">set</a> that
    are not <a class="glossRef" href="#element">elements</a> of the <a class="glossRef" href="#subset">subset</a>.
</dd>

<dt>
   <a id="complement_rule"></a>Complement rule.
</dt>
<dd>
    The probability of the <a class="glossRef" href="#complement">complement</a>  of an event
    is 100% minus the probability of the event: P(A<sup>c</sup>) = 100% &minus; P(A).
</dd>

<dt>
   <a id="compoundProposition"></a>Compound proposition.
</dt>
<dd>
   A logical <a class="glossRef" href="#proposition">proposition</a> formed from other
   propositions using logical operations such as
   <a class="glossRef" href="#NOT">!</a>, <a class="glossRef" href="#OR">|</a>, <a class="glossRef" href="#XOR">XOR</a>,
  <a class="glossRef" href="#AND">&amp;</a>,
   <a class="glossRef" href="#IMPLIES">&rarr;</a> and <a class="glossRef" href="#IFF">&harr;</a>.
</dd>

<dt>
   <a id="conditional_probability"></a>Conditional Probability.
</dt>
<dd>
    Suppose we are interested in the probability that some <a class="glossRef" href="#event">event</a> A
    occurs, and we learn that the <a class="glossRef" href="#event">event</a> B occurred. How should we update
    the probability of A to reflect this new knowledge? This is what the conditional
    probability does: it says how the additional knowledge that B occurred should affect the
    probability that A occurred quantitatively. For example, suppose that A and B are
    <a class="glossRef" href="#disjoint">mutually exclusive</a>. Then if B occurred, A did not, so the
    <em>conditional
    probability that A occurred given that B occurred </em>is zero. At the other extreme,
    suppose that B is a <a class="glossRef" href="#subset">subset</a> of A, so that A must occur whenever B
    does. Then if we learn that B occurred, A must have occurred too, so the <em>conditional
    probability that A occurred given that B occurred </em>is 100%. For in-between cases,
    where A and B intersect, but B is not a <a class="glossRef" href="#subset">subset</a> of A, the conditional
    probability of A given B is a number between zero and 100%. Basically, one
    &quot;restricts&quot; the <a class="glossRef" href="#outcome_space">outcome space</a> <strong>S</strong> to
    consider only the part of <strong>S</strong> that is in B, because we know that B
    occurred. For A to have happened given that B happened requires that
    <a class="glossRef" href="#intersection">AB</a> happened, so we are interested in the event
    <a class="glossRef" href="#intersection">AB</a>. To have a legitimate probability requires that
    P(<strong>S</strong>)
    = 100%, so if we are restricting the outcome space to B, we need to divide by the
    probability of B to make the probability of this new <strong>S</strong> be 100%. On this
    scale, the probability that AB happened is P(AB)/P(B). This is the definition of the
    conditional probability of A given B, provided P(B) is not zero (division by zero is
    undefined). Note that the special cases AB = {} (A and B are <a class="glossRef" href="#disjoint">mutually
    exclusive</a>) and AB = B (B is a <a class="glossRef" href="#subset">subset</a> of A) agree with our
    intuition as described at the top of this paragraph. Conditional probabilities satisfy the
    <a class="glossRef" href="#axioms_of_probability">axioms of probability</a>, just as ordinary probabilities
    do.
</dd>

<dt>
   <a id="confidence_interval"></a>Confidence Interval.
</dt>
<dd>
    A confidence interval for a <a class="glossRef" href="#parameter">parameter</a> is a random interval
    constructed from data in such a way that the probability that the interval contains the
    true value of the parameter can be specified before the data are collected.
    Confidence intervals are demonstrated in <a href="../../Java/Html/Ci.htm" target="lablet">this
    page</a>.
</dd>

<dt>
   <a id="confidence_level"></a>Confidence Level.
</dt>
<dd>
    The confidence level of a <a class="glossRef" href="#confidence_interval">confidence interval</a> is the
    chance that the interval that will result once data are collected will contain the
    corresponding <a class="glossRef" href="#parameter">parameter</a>. If one computes confidence intervals
    again and again from independent data, the long-term limit of the fraction of intervals
    that contain the parameter is the confidence level.
</dd>

<dt>
   <a id="confounding"></a>Confounding.
</dt>
<dd>
    When the differences between the <a class="glossRef" href="#treatment_group">treatment</a> and
    <a class="glossRef" href="#control_group">control</a> groups other than the treatment produce differences in
    response that are not distinguishable from the effect of the
    <a class="glossRef" href="#treatment">treatment</a>,
    those differences between the groups are said to be <em>confounded</em> with the effect of
    the treatment (if any). For example, prominent statisticians questioned whether
    differences between individuals that led some to smoke and others not to (rather than the
    act of smoking itself) were responsible for the observed difference in the frequencies
    with which smokers and non-smokers contract various illnesses. If that were the case,
    those factors would be confounded with the effect of smoking. Confounding is quite likely
    to affect <a class="glossRef" href="#study">observational studies</a> and
    <a class="glossRef" href="#experiment">experiments</a>
    that are not <a class="glossRef" href="#randomized">randomized</a>.
    Confounding tends to be decreased by <a class="glossRef" href="#randomized">randomization</a>.
    See also <a class="glossRef" href="#simpson">Simpson's Paradox</a>.
</dd>

<dt>
   <a id="continuity_correction"></a>Continuity Correction.
</dt>
<dd>
    In using the <a class="glossRef" href="#normal_approximation">normal approximation</a> to the
    <a class="glossRef" href="#binomial">binomial</a> <a class="glossRef" href="#probability_histogram">probability histogram</a>,
    one can get more accurate answers by finding the area under the normal curve corresponding
    to half-integers, transformed to <a class="glossRef" href="#standard_units">standard units</a>.
    This is clearest if we are seeking the chance of a particular number of successes.
    For example, suppose we seek to approximate the chance of 10 successes in 25
    <a class="glossRef" href="#independent">independent</a>
    trials, each with probability <em>p </em>= 40% of success.
    The number of successes in this
    scenario has a <a class="glossRef" href="#binomial">binomial distribution</a> with parameters <em>n</em> =
    25 and <em>p </em>= 40%. The <a class="glossRef" href="#expectation">expected</a>
    number of successes is <em>np</em>
    = 10, and the <a class="glossRef" href="#se">standard error</a> is
    (<em>np</em>(1&minus;<em>p</em>))<sup>&frac12;</sup>
    = 6<sup>&frac12;</sup> = 2.45. If we consider the area under the
    <a class="glossRef" href="#normal_curve">normal
    curve</a> at the point 10 successes, transformed to <a class="glossRef" href="#standard_units">standard
    units</a>, we get zero: the area under a point is always zero. We get a better
    approximation by considering 10 successes to be the range from 9 1/2 to 10 1/2 successes.
    The only possible number of successes between 9 1/2 and 10 1/2 is 10, so this is exactly
    right for the <a class="glossRef" href="#binomial">binomial distribution</a>.
    Because the
    <a class="glossRef" href="#normal_curve">normal curve</a> is
    <a class="glossRef" href="#continuous">continuous</a>
    and a <a class="glossRef" href="#binomial">binomial</a> <a class="glossRef" href="#random_variable">random variable</a>
    is <a class="glossRef" href="#discrete">discrete</a>, we need to &quot;smear out&quot;
    the <a class="glossRef" href="#binomial">binomial</a>
    probability over an appropriate range. The lower endpoint of the range, 9 1/2 successes,
    is (9.5 &minus; 10)/2.45 = &minus;0.20 <a class="glossRef" href="#standard_units">standard units</a>.
    The upper endpoint of the range, 10 1/2 successes, is (10.5 &minus; 10)/2.45 = +0.20
    <a class="glossRef" href="#standard_units">standard units</a>.
    The area under the <a class="glossRef" href="#normal_curve">normal
    curve</a> between &minus;0.20 and +0.20 is about 15.8%.
    The true <a class="glossRef" href="#binomial">binomial</a>
    probability is
    <sub>25</sub><em>C</em><sub>10</sub>&times;(0.4)<sup>10</sup>&times;(0.6)<sup>15</sup>
    = 16%. In a similar way, if we seek the <a class="glossRef" href="#normal_approximation">normal
    approximation</a> to the probability that a <a class="glossRef" href="#binomial">binomial</a>
    <a class="glossRef" href="#random_variable">random variable</a> is in the range from
    <em>i</em> successes to <em>k</em>
    successes, inclusive, we should find the area under the <a class="glossRef" href="#normal_curve">normal
    curve</a> from <em>i</em>&minus;1/2 to <em>k+</em>1/2 successes, transformed to
    <a class="glossRef" href="#standard_units">standard units</a>.
    If we seek the probability of more than <em>i</em>
    successes and fewer than <em>k</em> successes, we should find the area under
    the <a class="glossRef" href="#normal_curve">normal curve</a> corresponding to the range
    <em>i</em>+1/2 to <em>k&minus;</em>1/2
    successes, transformed to <a class="glossRef" href="#standard_units">standard units</a>. If we seek the
    probability of more than <em>i</em> but no more than <em>k</em> successes, we should find
    the area under the <a class="glossRef" href="#normal_curve">normal curve</a> corresponding to
    the range <em>i</em>+1/2
    to <em>k+</em>1/2 successes, transformed to
    <a class="glossRef" href="#standard_units">standard units</a>.
    If we seek the probability of at least <em>i</em> but fewer than <em>k</em> successes, we
    should find the area under the <a class="glossRef" href="#normal_curve">normal curve</a> corresponding to
    the range <em>i</em>&minus;1/2 to <em>k&minus;</em>1/2 successes, transformed to
    <a class="glossRef" href="#standard_units">standard units</a>.
    Including or excluding the half-integer ranges
    at the ends of the interval in this manner is called the continuity correction.
</dd>

<dt>
   <a id="consequent"></a>Consequent.
</dt>
<dd>
   In a <a class="glossRef" href="#conditional">conditional</a> <span class="math">p &rarr; q</span>,
   the consequent is <span class="math">q</span>.
</dd>

<dt>
   <a id="continuous"></a>Continuous Variable.
</dt>
<dd>
A <a class="glossRef" href="#quantitative">quantitative variable</a> is <em>continuous</em> if its set of
    possible values is uncountable. Examples include temperature, exact height, exact age
    (including parts of a second). In practice, one can never measure a continuous variable to
    infinite precision, so continuous variables are sometimes approximated by
    <a class="glossRef" href="#discrete">discrete variables</a>.
    A <a class="glossRef" href="#random_variable">random variable</a>
    X is also called <em>continuous</em> if its set of possible values is uncountable, and the
    chance that it takes any particular value is zero (in symbols, if P(X = <em>x</em>) = 0
    for every real number <em>x</em>). A random variable is continuous if and
    only if its <a class="glossRef" href="#cdf">cumulative probability distribution function</a>
    is a continuous function (a function with no jumps).
</dd>

<dt>
    <a id="contrapositive"></a>Contrapositive.
</dt>
<dd>
    If <em>p</em> and <em>q</em> are two <a class="glossRef" href="#proposition">logical propositions</a>,
    then the <em>contrapositive</em> of the proposition
    (<em>p</em> <a class="glossRef" href="#IMPLIES">&rarr;</a> <em>q</em>)
    is the proposition
    <big>(</big>(<a class="glossRef" href="#NOT">!</a> <em>q</em>) &rarr;
    (!<em>p</em>) <big>)</big>.
    The contrapositive is <a class="glossRef" href="#logicallyEquivalent">logically equivalent</a>
    to the original proposition.
</dd>

<dt>
   <a id="control"></a>Control.
</dt>
<dd>
There are at least three senses of &quot;control&quot; in statistics: a
  member of the <em>control
    group,</em> to whom no <a class="glossRef" href="#treatment">treatment</a> is given;
    a <a class="glossRef" href="#controlled_experiment">controlled experiment</a>, and to
    <a class="glossRef" href="#control_for">control
    for</a> a possible confounding variable.
</dd>

<dt>
   <a id="controlled_experiment"></a>Controlled experiment.
</dt>
<dd>
An <a class="glossRef" href="#experiment">experiment</a> that uses the <a class="glossRef" href="#comparison">method of
    comparison</a> to evaluate the <a class="glossRef" href="#effect">effect</a> of a
    <a class="glossRef" href="#treatment">treatment</a>
    by comparing treated <a class="glossRef" href="#subject">subjects</a> with a control group, who do not
    receive the treatment.
</dd>

<dt>
   <a id="controlled_randomized_experiment"></a>Controlled, randomized experiment.
</dt>
<dd>
    A <a class="glossRef" href="#controlled_experiment">controlled experiment</a> in which the
    assignment of <a class="glossRef" href="#subject">subjects</a> to the <a class="glossRef" href="#treatment_group">treatment
    group</a> or <a class="glossRef" href="#control_group">control group</a> is done at random, for example,
    by tossing a coin.
</dd>

<dt>
   <a id="control_for"></a>Control for a variable.
</dt>
<dd>
    To control for a variable is to try to separate its effect from the treatment
    effect, so it will not <a class="glossRef" href="#confounding">confound</a> with the treatment.
    There are many methods that try to control for variables.
    Some are based on matching individuals between treatment and control; others
    use assumptions about the nature of the effects of the variables to try
    to model the effect mathematically, for example, using regression.
</dd>

<dt>
   <a id="control_group"></a>Control group.
</dt>
<dd>
    The <a class="glossRef" href="#subject">subjects</a> in a <a class="glossRef" href="#controlled_experiment">controlled
    experiment</a> who do not receive the <a class="glossRef" href="#treatment">treatment</a>.
</dd>

<dt>
   <a id="convenience_sample"></a>Convenience Sample.
</dt>
<dd>
    A sample drawn because of its convenience; it is not a
    <a class="glossRef" href="#probability_sample">probability
    sample</a>.
    For example, I might take a sample of opinions in Berkeley (where I live) by
    just asking my 10 nearest neighbors. That would be a sample of convenience, and would be
    unlikely to be representative of all of Berkeley.
    Samples of convenience are not typically representative, and it is not possible to quantify
    how unrepresentative results based on samples of convenience are likely to be.
    Convenience samples are to be avoided, and results based on convenience samples are to be
    viewed with suspicion.
    See also <a class="glossRef" href="#quota">quota sample</a>.
</dd>

<dt>
   <a id="converge"></a>Converge, convergence.
</dt>
<dd>
    A sequence of numbers <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>,
    <em>x</em><sub>3</sub>
    &hellip; <em>converges</em> if there is a number
    <em>x</em> such that for any number
    E&gt;0,
    there is a number <em>k</em> (which can depend on E) such that
    |<em>x<sub>j</sub> </em>&minus; <em>x</em>| &lt; E whenever <em>j</em> &gt;
    <em>k</em>. If such a number <em>x</em> exists, it is called the
    <a class="glossRef" href="#limit">limit</a> of the sequence <em>x</em><sub>1</sub>,
    <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub> &hellip; .
</dd>

<dt>
   <a id="converge_in_prob"></a>Convergence in probability.
</dt>
<dd>
    A sequence of <a class="glossRef" href="#random_variable">random variables</a>
    X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub>
    &hellip; <em>converges in probability</em> if there is a random
    variable X such that for any number E&gt;0, the sequence of numbers
</dd>

<p class="math">
    P(|X<sub>1</sub> &minus; X| &lt; e), P(|X<sub>2</sub> &minus; X| &lt; e),
    P(|X<sub>3</sub> &minus; X| &lt; e),
    &hellip;
</p>

<dd>
    converges to 100%.
</dd>

<dt>
    <a id="converse"></a>Converse.
</dt>
<dd>
    If <em>p</em> and <em>q</em> are two <a class="glossRef" href="#proposition">logical propositions</a>,
    then the <em>converse</em> of the proposition
    (<em>p</em> <a class="glossRef" href="#IMPLIES">&rarr;</a> <em>q</em>)
    is the proposition (<em>q</em> &rarr; <em>p</em>).
</dd>

<dt>
   <a id="correlation"></a>Correlation.
</dt>
<dd>
    A measure of linear <a class="glossRef" href="#association">association</a>
    between two (ordered) lists.
    Two variables can be strongly correlated without having any causal
    relationship, and two variables can have a causal
    relationship and yet be uncorrelated.
</dd>

<dt>
   <a id="correlation_coef"></a>Correlation coefficient.
</dt>
<dd>
    The correlation coefficient <em>r</em> is a measure of how nearly a
    <a class="glossRef" href="#scatterplot">scatterplot</a>
    falls on a straight line. The correlation coefficient is always between &minus;1 and +1. To
    compute the correlation coefficient of a list of pairs of measurements (X,Y),
    first <a class="glossRef" href="#trans">transform</a> X and Y individually into
    <a class="glossRef" href="#standard_units">standard
    units</a>.
    Multiply corresponding elements of the transformed pairs to get a single list
    of numbers.
    The correlation coefficient is the <a class="glossRef" href="#mean">mean</a> of that list of
    products.
    <a href="../../Java/Html/Correlation.htm" target="lablet">This page</a>
    contains a tool that lets you generate <a class="glossRef" href="#bivariate">bivariate</a>
    data with any correlation coefficient you want.
</dd>

<dt>
   <a id="counting"></a>Counting.
</dt>
<dd>
    To count a set of things is to put it in one to one correspondence with a consecutive subset of the
    positive integers, starting with 1.
</dd>

<dt>
   <a id="countable"></a>Countable Set.
</dt>
<dd>
    A set is countable if its elements can be put in one-to-one correspondence with a subset
    of the integers. For example, the sets {0, 1, 7, &minus;3}, {red, green, blue},
    {&hellip;,&minus;2, &minus;1, 0,
    1, 2, &hellip;}, {straight, curly}, and the set of all fractions,
    are countable.
    If a set is not countable, it is <a class="glossRef" href="#uncountable">uncountable</a>.
    The set of all real numbers is <a class="glossRef" href="#uncountable"><em>uncountable</em></a>.
</dd>

<dt>
   <a id="cover"></a>Cover.
</dt>
<dd>
    A <a class="glossRef" href="#confidence_interval">confidence interval</a> is said to <em>cover</em> if
    the interval contains the true value of the <a class="glossRef" href="#parameter">parameter</a>. Before the
    data are collected, the chance that the confidence interval will contain the parameter
    value is the <a class="glossRef" href="#coverage_probability">coverage probability</a>,
    which equals the <a class="glossRef" href="#confidence_level">confidence level</a>
    after the data are collected and the
    confidence interval is actually computed.
</dd>

<dt>
   <a id="coverage_probability"></a>Coverage probability.
</dt>
<dd>
    The <em>coverage probability</em> of a procedure for making
    <a class="glossRef" href="#confidence_interval">confidence intervals</a> is the chance that the
    procedure produces an interval that <a class="glossRef" href="#cover">covers</a> the truth.
</dd>

<dt>
    <a id="critical_value"></a>
    Critical value
</dt>
<dd>
    The <em>critical value</em> in an <a class="glossRef" href="#hypothesis_test">hypothesis test</a>
    is the value of the <a class="glossRef" href="#test_statistic">test statistic</em> beyond which we
    would reject the <a class="glossRef" href="#null_hypothesis">null hypothesis</a>.
    The critical value is set so that the probability that the
    <a class="glossRef" href="#test_statistic">test statistic</em> is beyond the critical value is
    at most equal to the <a class="glossRef" href="#significance">significance level</a> if the
    null hypothesis be true.
</dd>

<dt>
  <a id="cross_sectional"></a>
  Cross-sectional study.
</dt>
<dd>
   A cross-sectional <a class="glossRef" href="#study">study</a> compares different individuals to each
    other at the same time&mdash;it looks at a cross-section of a population. The differences
    between those individuals can <a class="glossRef" href="#confounding">confound</a> with the effect being
    explored. For example, in trying to determine the effect of age on sexual promiscuity, a
    cross-sectional study would be likely to <a class="glossRef" href="#confounding">confound</a>
    the effect of
    age with the effect of the mores the subjects were taught as children: the older
    individuals were probably raised with a very different attitude towards promiscuity than
    the younger subjects.
    Thus it would be imprudent to attribute differences in promiscuity
    to the aging process. <em>C.f. </em><a class="glossRef" href="#longitudinal">longitudinal study</a>.
</dd>

<dt>
   <a id="cdf"></a>Cumulative Probability Distribution Function (cdf).
</dt>
<dd>
    The cumulative distribution function of a <a class="glossRef" href="#random_variable">random variable</a>
    is the chance that the random variable is less than or equal to <em>x</em>, as a function
    of <em>x</em>. In symbols, if <em>F</em> is the cdf of the
    <a class="glossRef" href="#random_variable">random
    variable</a> X, then <em>F</em>(<em>x</em>) = P( X<em> &le; x</em>). The cumulative
    distribution function must tend to zero as <em>x</em> approaches minus infinity, and must
    tend to unity as <em>x</em> approaches infinity.
    It is a positive function, and <a class="glossRef" href="#monotonic">increases monotonically</a>:
    if <em>y</em> &gt; <em>x</em>, then
    <em>F</em>(<em>y</em>) &ge; <em>F</em>(<em>x</em>).
    The cumulative distribution function completely characterizes the
    <a class="glossRef" href="#prob_distribution">probability distribution</a> of a
    <a class="glossRef" href="#random_variable">random variable</a>.
</dd>

<p align="center"><!---D---> </p>
    <hr />
<dt>
   <a id="d"></a>D
</dt>

<dt>
   <a id="de_morgan"></a>de Morgan's Laws
</dt>
<dd>
    de Morgan's Laws are identities involving logical operations:
    the <a class="glossRef" href="#NOT">negation</a> of a <a class="glossRef" href="#AND">conjunction</a>
    is <a class="glossRef" href="#logically_equivalent">logically equivalent</a> to
    the <a class="glossRef" href="#OR">disjunction</a> of the negations, and the negation of
    a disjunction is logically equivalent to the conjunction of the negations.
    In symbols, <span class="math">!(p &amp; q) = !p | !q</span> and
    <span class="math">!(p | q) = !p &amp; !q</span>.
</dd>

<dt>
   <a id="deck_of_cards"></a>Deck of Cards.
</dt>
<dd>
    A standard deck of playing cards contains 52 cards, 13 each of four suits: spades,
    hearts, diamonds, and clubs. The thirteen cards of each suit are {ace, 2, 3, 4, 5, 6, 7,
    8, 9, 10, jack, queen, king}. The <em>face</em> cards are {jack, queen, king}. It is
    typically assumed that if a deck of cards is shuffled well, it is equally likely to be in
    each possible ordering. There are 52<a class="glossRef" href="#factorial">!</a>
    (52 <a class="glossRef" href="#factorial">factorial</a>)
    possible orderings.
</dd>

<dt>
   <a id="dependent"></a>Dependent <a class="glossRef" href="#event">Events</a>, Dependent
  <a class="glossRef" href="#random_variable">Random Variables</a>.
</dt>
<dd>
    Two <a class="glossRef" href="#event">events</a> or <a class="glossRef" href="#random_variable">random variables</a> are
    dependent if they are not <a class="glossRef" href="#independent">independent</a>.
</dd>

<dt>
   <a id="dependent_variable"></a>Dependent Variable.
</dt>
<dd>
    In <a class="glossRef" href="#regression">regression</a>, the variable whose values are supposed to be
    explained by changes in the other variable
    (the the <a class="glossRef" href="#independent_variable">independent</a>
    or <a class="glossRef" href="#explanatory_variable">explanatory variable</a>). Usually one regresses the
    dependent variable on the <a class="glossRef" href="#independent_variable">independent variable</a>.
</dd>

<dt>
  <a id="density"></a>Density, Density Scale.
</dt>
<dd>
    The vertical axis of a histogram has units of percent per unit of the horizontal axis.
    This is called a density scale; it measures how &quot;dense&quot; the observations are in
    each bin. See also <a class="glossRef" href="#prob_density">probability density</a>.
</dd>

<dt>
  <a id="denying_the_antecedent"></a>Denying the antecedent.
</dt>
<dd>
    A logical fallacy that argues from the <a class="glossRef" href="#premise">premise</a>
    <span class="math">A &rarr; B</span>
    and the premise <span class="meth">!A</span> that therefore, <span class="math">!B</span>.
    The name comes from the fact that the operation denies
    (i.e., asserts the <a class="glossRef" href="#NOT">negation</a> of) the
    <a class="glossRef" href="#antecedent">antecedent</a> (<span class="math">A</span>) in the
    <a class="glossRef" href="#conditional">conditional</a>.
</dd>

<dt>
  <a id="denying_the_consequent"></a>Denying the consequent.
</dt>
<dd>
    A valid logical argument that concludes from the <a class="glossRef" href="#premise">premise</a>
    <span class="math">A &rarr; B</span>
    and the premise <span class="meth">!B</span> that therefore, <span class="math">!A</span>.
    The name comes from the fact that the operation denies
    (i.e., asserts the logical <a class="glossRef" href="#NOT">negation</a>) the
    <a class="glossRef" href="#consequent">consequent</a> (<span class="math">B</span>) in the
    <a class="glossRef" href="#conditional">conditional</a>.
</dd>

<dt>
  <a id="deviation"></a>Deviation.
</dt>
<dd>
   A deviation is the difference between a datum and some reference value, typically the
    <a class="glossRef" href="#mean">mean</a>
    of the data. In computing the <a class="glossRef" href="#sd">SD</a>, one finds the <a class="glossRef" href="#rms">rms</a>
    of the deviations from the <a class="glossRef" href="#mean">mean</a>, the differences between the
    individual data and the <a class="glossRef" href="#mean">mean</a> of the data.
</dd>

<dt>
   <a id="discrete"></a>Discrete Variable.
</dt>
<dd>
    A <a class="glossRef" href="#quantitative">quantitative variable</a> whose set of possible
    values is <a class="glossRef" href="#countable">countable</a>. Typical examples of discrete
    variables are variables
    whose possible values are a subset of the integers, such as Social Security numbers, the
    number of people in a family, ages rounded to the nearest year, <em>etc. </em>Discrete
    variables are &quot;chunky.&quot; <em>C.f. </em><a class="glossRef" href="#continuous">continuous
    variable</a>.
    A discrete <a class="glossRef" href="#random_variable">random variable</a> is one whose set of possible
    values is <a class="glossRef" href="#countable">countable</a>. A random variable is discrete if and only if
    its <a class="glossRef" href="#cdf">cumulative probability distribution function</a> is a stair-step
    function; <em>i.e.</em>, if it is piecewise constant and only increases by jumps.
</dd>

<dt>
   <a id="disjoint"></a>Disjoint or Mutually Exclusive <a class="glossRef" href="#event">Events</a>.
</dt>
<dd>
    Two <a class="glossRef" href="#event">events</a> are disjoint or mutually exclusive if the occurrence of
    one is incompatible with the occurrence of the other; that is, if they can't both happen
    at once (if they have no outcome in common).
    Equivalently, two <a class="glossRef" href="#event">events</a>
    are disjoint if their <a class="glossRef" href="#intersection">intersection</a> is the
    <a class="glossRef" href="#empty_set">empty set</a>.
</dd>

<dt>
   <a id="disjoint_sets"></a>Disjoint or Mutually Exclusive <a class="glossRef" href="#set">Sets</a>.
</dt>
<dd>
    Two <a class="glossRef" href="#set">sets</a> are disjoint or mutually exclusive if they have no element
    in common. Equivalently, two <a class="glossRef" href="#set">sets</a> are disjoint if their
    <a class="glossRef" href="#intersection">intersection</a> is the <a class="glossRef" href="#empty_set">empty set</a>.
</dd>

<dt>
   <a id="distribution"></a>Distribution.
</dt>
<dd>
    The distribution of a set of numerical data is how their values are distributed over the
    real numbers. It is completely characterized by the <a class="glossRef" href="#ecdf">empirical distribution
    function</a>. Similarly, the <a class="glossRef" href="#prob_distribution">probability distribution</a> of
    a random variable is completely characterized by its <a class="glossRef" href="#cdf">probability
    distribution function</a>. Sometimes the word &quot;distribution&quot; is used as a
    synonym for the <a class="glossRef" href="#ecdf">empirical distribution function</a> or the
    <a class="glossRef" href="#cdf">probability
    distribution function</a>.
    If two or more random variables are defined for the same experiment, they have
    a <a class="glossRef" href="#joint_prob_dist">joint probability distribution</a>.
</dd>

<dt>
  <a id="ecdf"></a>Distribution Function, Empirical.
</dt>
<dd>
    The empirical (cumulative) distribution function of a set of numerical data is, for each
    real value of <em>x</em>, the fraction of observations that are less than or equal to
    <em>x</em>.
    A plot of the empirical distribution function is an uneven set of stairs. The width of the
    stairs is the spacing between adjacent data; the height of the stairs depends on how many
    data have exactly the same value. The distribution function is zero for small enough
    (negative) values of <em>x</em>, and is unity for large enough values of <em>x</em>. It
    increases <a class="glossRef" href="#monotonic">monotonically</a>:
    if <em>y &gt; x</em>, the empirical distribution function
    evaluated at <em>y</em> is at least as large as the empirical distribution function
    evaluated at <em>x</em>.
</dd>

<dt>
   <a id="double_blind"></a>Double-Blind, Double-Blind Experiment.
</dt>
<dd>
    In a double-blind experiment, neither the <a class="glossRef" href="#subject">subjects</a> nor the people
    evaluating the subjects knows who is in the <a class="glossRef" href="#treatment_group">treatment group</a>
    and who is in the <a class="glossRef" href="#control_group">control group</a>.
    This mitigates the <a class="glossRef" href="#placebo_effect">placebo effect</a> and guards
    against conscious and unconscious
    prejudice for or against the treatment on the part of the evaluators.
</dd>

<dd>
<p align="center"><!---E---> </p>
    <hr />
    <p><a id="e"></a>E </p>

</dd>

<dt>
   <a id="ecological_correlation"></a>Ecological Correlation.
</dt>
<dd>
    The <a class="glossRef" href="#correlation">correlation</a> between
    averages of groups of individuals, instead of individuals.
    Ecological correlation can be misleading about the association of individuals.
</dd>


<dt>
   <a id="element"></a>Element of a <a class="glossRef" href="#set">Set</a>.
</dt>
<dd>
    See <a class="glossRef" href="#member">member</a>.
</dd>

<dt>
   <a id="empirical_law_of_averages"></a>Empirical Law of Averages.
</dt>
<dd>
    The Empirical Law of Averages lies at the base of the
    <a class="glossRef" href="#frequency_theory">frequency
    theory</a> of probability. This law, which is, in fact, an assumption about how the world
    works, rather than a mathematical or physical law, states that if one repeats a
    <a class="glossRef" href="#random_experiment">random experiment</a>
    over and over, independently and under
    &quot;identical&quot; conditions, the fraction of trials that result in a given outcome
    converges to a limit as the number of trials grows without bound.
</dd>

<dt>
   <a id="empty_set"></a>Empty <a class="glossRef" href="#set">Set</a>.
</dt>
<dd>
    The empty <a class="glossRef" href="#set">set</a>, denoted {} or , is the <a class="glossRef" href="#set">set</a> that
    has no <a class="glossRef" href="#member">members</a>.
</dd>

<dt>
   <a id="endpoint_convention"></a>Endpoint Convention.
</dt>
<dd>
    In plotting a <a class="glossRef" href="#histogram">histogram</a>, one must decide whether to include a
    datum that lies at a class boundary with the <a class="glossRef" href="#class_interval">class interval</a>
    to the left or the right of the boundary. The rule for making this assignment is called an
    <em>endpoint convention</em>. The two standard endpoint conventions are (1) to include the
    left endpoint of all class intervals and exclude the right, except for the rightmost class
    interval, which includes both of its endpoints, and (2) to include the right endpoint of
    all class intervals and exclude the left, except for the leftmost interval, which includes
    both of its endpoints.
</dd>

<dt>
   <a id="estimator"></a>Estimator.
</dt>
<dd>
    An estimator is a rule for &quot;guessing&quot; the value of a population
    <a class="glossRef" href="#parameter">parameter</a> based on a <a class="glossRef" href="#random_sample">random sample</a>
    from the population. An estimator is a <a class="glossRef" href="#random_variable">random variable</a>,
    because its value depends on which particular sample is obtained, which is random.
    A canonical example of an estimator is the <a class="glossRef" href="#sample_mean">sample mean</a>,
    which is an estimator of the <a class="glossRef" href="#population_mean">population mean</a>.
</dd>

<dt>
   <a id="event"></a>Event.
</dt>
<dd>
  An <em>event</em> is a <a class="glossRef" href="#subset">subset</a> of
  <a class="glossRef" href="#outcome_space">outcome space</a>.
  An <em>event</em> determined by a <a class="glossRef" href="#random_variable">random variable</a></em>
  is an event of the form A=(X is in <em>A</em>). When the random variable X is observed, that
  <em>determines</em>
  whether or not A occurs: if the value of X happens to be in <em>A</em>, A occurs; if
  not, A does not occur.
</dd>

<dt>
   <a id="exhaustive"></a>Exhaustive.
</dt>
<dd>
    A collection of <a class="glossRef" href="#event">events</a> <span class="math">{A<sub>1</sub>, A<sub>2</sub>, A<sub>3</sub>,
    &hellip; }</span>
    <em>exhausts</em> the set <span class="math">A</span>
    if, for the event <span class="math">A</span> to occur, at least one of those sets must also
    occur; that is, if
<p class="math">
    <strong>S</strong> &sub; A<sub>1</sub> &cup; A<sub>2</sub>
    &cup; A<sub>3</sub> &cup; &hellip;
</p>
<p>
   If the event <span class="math">A</span> is not specified, it is assumed to be the entire
    <a class="glossRef" href="#outcome_space">outcome space</a> <strong>S</strong>.
</p>

<dt>
   <a id="expectation"></a>Expectation, Expected Value.
</dt>
<dd>
    The expected value of a <a class="glossRef" href="#random_variable">random variable</a> is the long-term
    limiting average of its values in independent repeated experiments. The expected value of
    the random variable X is denoted EX or E(X). For a discrete random variable (one that has
    a <a class="glossRef" href="#countable">countable</a> number of possible values) the expected value is the
    weighted average of its possible values, where the weight assigned to each possible value
    is the chance that the random variable takes that value. One can think of the expected
    value of a random variable as the point at which its
    <a class="glossRef" href="#probability_histogram">probability
    histogram</a> would balance, if it were cut out of a uniform material. Taking the expected
    value is a <a class="glossRef" href="#linear">linear</a> operation: if X and Y are two random variables,
    the expected value of their sum is the sum of their expected values (E(X+Y) = E(X) +
    E(Y)), and the expected value of a constant <em>a</em> times a random variable X is the
    constant times the expected value of X (E(<em>a</em>&times;X ) =
    <em>a</em>&times; E(X)).
</dd>

<dt>
   <a id="experiment"></a>Experiment.
</dt>
<dd>
What distinguishes an experiment from an <a class="glossRef" href="#study">observational study</a> is
    that in an experiment, the experimenter decides who receives the
    <a class="glossRef" href="#treatment">treatment</a>.

</dd>

<dt>
   <a id="explanatory_variable"></a>Explanatory Variable.
</dt>
<dd>
In regression, the explanatory or <a class="glossRef" href="#independent_variable">independent variable</a>
    is the one that is supposed to &quot;explain&quot; the other. For example, in examining
    crop yield versus quantity of fertilizer applied, the quantity of fertilizer would be the
    explanatory or <a class="glossRef" href="#independent_variable">independent variable</a>, and the crop
    yield would be the <a class="glossRef" href="#dependent_variable">dependent variable</a>. In
    <a class="glossRef" href="#experiment">experiments</a>, the explanatory variable is the one that is
    manipulated; the one that is observed is the <a class="glossRef" href="#dependent_variable">dependent
    variable</a>.
</dd>


<dt>
   <a id="extrapolation"></a>Extrapolation.
</dt>
<dd>
   See <a class="glossRef" href="#interpolation">interpolation</a>.
</dd>

<dd>
<p align="center"><!---F---> </p>

<hr />
<p>
    <a id="f"></a>F
</p>

</dd>

<dt>
   <a id="factorial"></a>Factorial.
</dt>
<dd>
    For an integer <em>k</em> that is greater than or equal to 1, <em>k</em>! (pronounced
    &quot;<em>k</em> factorial&quot;) is
    <em>k&times;</em>(<em>k</em>&minus;1)<em>&times;</em>(<em>k</em>&minus;2)<em>&times;</em>
    &hellip;<em>&times;</em>1. By convention, 0! = 1. There are <em>k</em>!
    ways of ordering <em>k</em>
    distinct objects. For example, 9! is the number of batting orders of 9 baseball players,
    and 52! is the number of different ways a standard deck of playing cards
    can be ordered.  The <a class="glossRef" href="#combinations">calculator</a> above has a button to compute
    the factorial of a number.  To compute <em>k</em>!, first type the value of <em>k</em>,
    then press the button labeled &quot;!&quot;.
</dd>

<dt>
   <a id="fair_bet"></a>Fair Bet.
</dt>
<dd>
    A fair bet is one for which the <a class="glossRef" href="#expectation">expected value</a> of the payoff
    is zero, after accounting for the cost of the bet. For example, suppose I offer to pay you
    $2 if a fair coin lands heads, but you must <a class="glossRef" href="#ante">ante</a> up $1 to play. Your
    expected payoff is
    &minus;$1<em>+ </em>$0<em>&times;</em>P(tails) + $2<em>&times;</em>P(heads)
    = &minus;$1 + $2<em>&times;</em>50%
    = $0. This is a fair bet&mdash;in the long run, if you made this bet over and over again, you
    would expect to break even.
</dd>

<dt>
   <a id="fdr"></a>False Discovery Rate.
</dt>
<dd>
    In testing a collection of hypotheses, the false discovery rate is the fraction of
    rejected null hypotheses that are rejected erroneously (the number of Type I errors
    divided by the number of rejected null hypotheses), with the convention that if no
    hypothesis is rejected, the false discovery rate is zero.
</dd>

<dt>
  <a id="finite_population_correction"></a>Finite Population Correction.
</dt>
<dd>
    When sampling without replacement, as in a <a class="glossRef" href="#simple_random_sample">simple random
    sample</a>, the <a class="glossRef" href="#se">SE</a> of sample sums and sample means depends on the
    fraction of the population that is in the sample: the greater the fraction, the smaller
    the <a class="glossRef" href="#se">SE</a>. Sampling with replacement is like sampling from an infinitely
    large population. The adjustment to the SE for sampling without replacement is called the
    finite population correction. The <a class="glossRef" href="#se">SE</a> for sampling without replacement is
    smaller than the <a class="glossRef" href="#se">SE</a> for sampling with replacement by the finite
    population correction factor <big>(</big>(<em>N</em> &minus;<em>n</em>)/(<em>N</em> &minus;
    1)<big>)</big><sup>&frac12;</sup>. Note that for sample size <em>n</em>=1,
    there is
    no difference between sampling with and without replacement; the finite population
    correction is then unity. If the sample size is the entire population of <em>N</em> units,
    there is no variability in the result of sampling without replacement (every member of the
    population is in the sample exactly once), and the <a class="glossRef" href="#se">SE</a> should be zero.
    This is indeed what the finite population correction gives (the numerator vanishes).
</dd>

<dt>
    <a id="fisher_exact_test"></a>Fisher's exact test (for the equality of two
    percentages)
</dt>
<dd>
    Consider two populations of zeros and ones.
    Let <em>p</em><sub>1</sub> be the proportion of ones in the first population,
    and let <em>p</em><sub>2</sub> be the proportion of ones in the second population.
    We would like to test the <a class="glossRef" href="#null_hypothesis">null hypothesis</a> that
    <em>p</em><sub>1</sub>&nbsp;=&nbsp;<em>p</em><sub>2</sub>
    on the basis of a <a class="glossRef" href="#simple_random_sample">simple random sample</a>
    from each population.
    Let <em>n</em><sub>1</sub> be the size of the sample from population 1, and
    let <em>n</em><sub>2</sub> be the size of the sample from population 2.
    Let <em>G</em> be the total number of ones in both samples.
    If the null hypothesis be true, the two samples are like one larger sample from
    a single population of zeros and ones.
    The allocation of ones between the two samples would be expected
    to be proportional to the relative sizes of the samples, but would have
    some chance variability.
    <a class="glossRef" href="#conditional_probability">Conditional</a> on <em>G</em> and the two
    sample sizes, under the null hypothesis, the tickets in the first sample are like
    a random sample of size <em>n</em><sub>1</sub> without replacement from a collection of
    <em>N</em>&nbsp;=&nbsp;<em>n</em><sub>1</sub>&nbsp;+&nbsp;<em>n</em><sub>2</sub> units of
    which <em>G</em> are labeled with ones.
    Thus, under the null hypothesis, the number of tickets labeled with ones
    in the first sample has (conditional on <em>G</em>)
    an <a class="glossRef" href="#hypergeometric_distrib">hypergeometric distribution</a>
    with parameters <em>N</em>, <em>G</em>, and <em>n</em><sub>1</sub>.
    Fisher's exact test uses this distribution to set the ranges of observed values of
    the number of ones in the first sample for which we would reject the null hypothesis.
</dd>

<dt>
   <a id="football_shaped"></a>Football-Shaped Scatterplot.
</dt>
<dd>
    In a football-shaped scatterplot, most of the points lie within a tilted oval, shaped
    more-or-less like a football. A football-shaped scatterplot is one in which the
    data are <a class="glossRef" href="#homoscedasticity" target="_self">homoscedastically</a>
    scattered about a straight
    line.
</dd>

<dt>
   <a id="frame"></a>Frame, sampling frame.
</dt>
<dd>
     A <em>sampling frame</em> is a collection of <a class="glossRef" href="#units">units</a> from which
     a sample will be drawn.  Ideally, the frame is identical to the
     <a class="glossRef" href="#population">population</a> we want to learn about; more typically, the frame
     is only a subset of the
     <a class="glossRef" href="#population">population</a> of interest.  The difference between the
     frame and the <a class="glossRef" href="#population">population</a> can be a source of
     <a class="glossRef" href="#bias">bias</a> in sampling design, if the <a class="glossRef" href="#parameter">parameter</a>
     of interest has a different value for the frame than it does for the
     <a class="glossRef" href="#population">population</a>.  For example, one might desire to estimate
     the current annual average income of 1998 graduates of the University of California
     at Berkeley.  I propose to use the <a class="glossRef" href="#sample_mean">sample mean</a> income
     of a sample of graduates drawn at random. To facilitate taking the sample and contacting
     the graduates to obtain income information from them,
     I might draw names at random from the list of 1998 graduates for whom the alumni
     association has an accurate current address.
     The population is the collection of 1998 graduates; the frame is those graduates
     who have current addresses on file with the alumni association.
     If there is a tendency for graduates with higher incomes to have up-to-date
     addresses on file with the alumni association,
     that would introduce a positive <a class="glossRef" href="#bias">bias</a> into the annual average
     income estimated from the sample by the <a class="glossRef" href="#sample_mean">sample mean</a>.
</dd>

<dt>
   <a id="FPP"></a>FPP.
</dt>
<dd>
<em>Statistics</em>, third edition, by Freedman, Pisani, and Purves,
    published by W.W. Norton, 1997.
</dd>

<dt>
   <a id="#frequency_theory"></a>Frequency theory of probability.
</dt>
<dd>
    See <a class="glossRef" href="#probability_theories">Probability, Theories of</a>.
</dd>

<dt>
   <a id="frequency_table"></a>Frequency table.
</dt>
<dd>
    A table listing the frequency (number) or relative frequency (fraction or percentage) of
    observations in different ranges, called
    <a class="glossRef" href="#class_interval">class intervals</a>.
</dd>

<dt>
   <a id="fundamental_rule_of_counting"></a>Fundamental Rule of Counting.
</dt>
<dd>
    If a sequence of experiments or trials T<sub>1</sub>, T<sub>2</sub>, T<sub>3</sub>,
    &hellip;, T<sub>k</sub> could result, respectively, in <em>n</em><sub>1</sub>,
    <em>n</em><sub>2</sub>
    <em>n</em><sub>3</sub>, &hellip;,<em> n</em><sub>k </sub>possible outcomes, and the
    numbers <em>n</em><sub>1</sub>,<em>
    n</em><sub>2</sub> <em>n</em><sub>3</sub>, &hellip;,<em> n</em><sub>k </sub>do not depend on
    which outcomes actually occurred, the entire <em>sequence</em> of k experiments has
    <em>n</em><sub>1</sub><em>&times; n</em><sub>2</sub><em> &times;</em>
    <em>n</em><sub>3</sub><em>&times;</em>
    &hellip;<em>&times; n</em><sub>k</sub> possible outcomes.

</dd>

<dd>
<p align="center"><!---G---> </p>
    <hr />
    <p><a id="g"></a>G <!---H---> </p>

</dd>

<dt>
   <a id="game_theory"></a>Game Theory.
</dt>
<dd>
    A field of study that bridges mathematics, statistics, economics, and psychology. It is
    used to study economic behavior, and to model conflict between nations, for example,
    &quot;nuclear stalemate&quot; during the Cold War.
</dd>

<dt>
  <a id="geometric_distrib"></a>Geometric Distribution.
</dt>
<dd>
    The geometric distribution describes the number of trials up to and including the first
    success, in independent trials with the same probability of success. The geometric
    distribution depends only on the single parameter <em>p</em>, the probability of success in
    each trial. For example, the number of times one must toss a fair coin until the first
    time the coin lands heads has a geometric distribution with parameter <em>p</em> = 50%.
    The geometric distribution assigns probability
    <em>p&times;</em>(1 &minus; <em>p</em>)<sup><em>k</em>&minus;1</sup>to
    the event that it takes <em>k</em> trials to the first success.
    The <a class="glossRef" href="#expectation">expected
    value</a> of the geometric distribution is 1/<em>p</em>, and its <a class="glossRef" href="#se">SE</a> is
    (1&minus;<em>p</em>)<sup>&frac12;</sup>/<em>p</em>.
</dd>

<dt>
   <a id="geometric_mean"></a>Geometric Mean.
</dt>
<dd>
    The geometric mean of <em>n</em> numbers {<em>x</em><sub>1</sub>,<em>
    x</em><sub>2</sub>,<em>
    x</em><sub>3</sub>,<em> </em>&hellip;, <em>x<sub>n</sub></em>}
    is the <em>n</em>th root of their product:
</dd>
<dd>
<p class="math">
    (<em>x</em><sub>1</sub><em>&times;x</em><sub>2</sub><em>&times;x</em><sub>3</sub><em>&times;
    &hellip;
    &times;x<sub>n</sub></em>)<sup>1/<em>n</em></sup>.
</p>
</dd>

<dt>
  <a id="graph_of_averages"></a>Graph of Averages.
</dt>
<dd>
    For <a class="glossRef" href="#bivariate">bivariate</a> data, a graph of averages is a plot of the
    average values of one variable (say <em>y</em>) for small ranges of values of the other
    variable (say <em>x</em>), against the value of the second variable (<em>x</em>) at the
    midpoints of the ranges.
</dd>

<dd>
<hr />
    <p><a id="h"></a>H </p>

</dd>

<dt>
   <a id="heteroscedasticity"></a>Heteroscedasticity.
</dt>
<dd>
    &quot;Mixed scatter.&quot; A <a class="glossRef" href="#scatterplot">scatterplot</a> or
    <a class="glossRef" href="#residual_plot">residual plot</a> shows heteroscedasticity if the scatter in
    vertical slices through the plot depends on where you take the slice.
    <a class="glossRef" href="#regression">Linear regression</a> is not usually a good idea if the data are
    heteroscedastic.
</dd>

<dt>
   <a id="histogram"></a>Histogram.
</dt>
<dd>
    A histogram is a kind of plot that summarizes how data are distributed. Starting with a
    set of <a class="glossRef" href="#class_interval">class intervals</a>, the histogram is a set of rectangles
    (&quot;<a class="glossRef" href="#bin">bins</a>&quot;) sitting on the horizontal axis. The bases of the
    rectangles are the <a class="glossRef" href="#class_interval">class intervals</a>, and their heights are
    such that their areas are proportional to the fraction of observations in the
    corresponding <a class="glossRef" href="#class_interval">class intervals</a>. That is, the height of a
    given rectangle is the fraction of observations in the corresponding
    <a class="glossRef" href="#class_interval">class interval</a>, divided by the length of the corresponding
    <a class="glossRef" href="#class_interval">class interval</a>. A histogram does not need a vertical scale,
    because the total area of the histogram must equal 100%. The units of the vertical axis
    are percent per unit of the horizontal axis. This is called the <em>density scale</em>.
    The horizontal axis of a histogram needs a scale. If any observations coincide with the
    endpoints of <a class="glossRef" href="#class_interval">class intervals</a>, the
    <a class="glossRef" href="#endpoint_convention">endpoint convention</a> is important.
    <a href="../../Java/Html/HistHiLite.htm" target="lablet">This page</a>
    contains a histogram tool, with controls to highlight ranges of values and read their
    areas.
</dd>

<dt>
  <a id="historical"></a>Historical Controls.
</dt>
<dd>
    Sometimes, the a <a class="glossRef" href="#treatment_group">treatment group</a> is compared with
    individuals from another epoch who did not receive the treatment; for example, in studying
    the possible effect of fluoridated water on childhood cancer, we might compare cancer
    rates in a community before and after fluorine was added to the water supply. Those
    individuals who were children before fluoridation started would comprise an historical
    control group. Experiments and studies with historical controls tend to be more
    susceptible to confounding than those with contemporary controls, because many factors
    that might affect the outcome other than the <a class="glossRef" href="#treatment">treatment</a> tend to
    change over time as well. (In this example, the level of other potential carcinogens in
    the environment also could have changed.)
</dd>

<dt>
   <a id="homoscedasticity"></a>Homoscedasticity.
</dt>
<dd>
    &quot;Same scatter.&quot; A <a class="glossRef" href="#scatterplot">scatterplot</a> or
    <a class="glossRef" href="#residual_plot">residual plot</a> shows homoscedasticity if the scatter
    in vertical slices through the plot does not depend much on where you take the slice.
    <em>C.f. </em><a class="glossRef" href="#heteroscedasticity">heteroscedasticity</a>.
</dd>

<dt>
   <a id="house_edge"></a>House Edge.
</dt>
<dd>
    In casino games, the expected payoff to the bettor
    is negative: the house (casino) tends to win money in the
    long run. The amount of money the house would expect to win for each $1 wagered on
    a particular bet (such as a bet on &quot;red&quot; in roulette) is
    called the <em>house edge</em> for that bet.
</dd>

<dt>
    <a id="htlws"></a>HTLWS.
</dt>
<dd>
    The book <a href="references.htm#huff" target="_self"><em>How to lie with
    Statistics</em></a> by D. Huff.
</dd>

<dt>
    <a id="hypergeometric_distrib"></a>Hypergeometric Distribution.
</dt>
<dd>
    The hypergeometric distribution with parameters <em>N</em>, <em>G</em> and
    <em>n</em> is the distribution of the number of &quot;good&quot;
    objects in a <a class="glossRef" href="#simple_random_sample">simple random sample</a> of size <em>n</em>
    (<em>i.e.</em>, a
    random sample without replacement in which every subset of size <em>n</em> has the same
    chance of occurring) from a population of <em>N</em> objects of which
    <em>G</em> are &quot;good.&quot;
    The chance of getting exactly <em>g</em> good objects in such a sample is
</dd>
<p class="math">
    <sub><em>G</em></sub>C<sub><em>g</em></sub> &times;
    <sub><em>N</em>&minus;<em>G</em></sub>C<sub><em>n</em>&minus;<em>g</em></sub>/<sub><em>N</em></sub>C<sub><em>n</em></sub>,
</p>
<dd>
    provided <em>g</em>&nbsp;&le;&nbsp;<em>n</em>, <em>g</em>&nbsp;&le;&nbsp;<em>G</em>, and
    <em>n</em>&nbsp;&minus;&nbsp;<em>g</em>&nbsp;&le;&nbsp;<em>N</em>&nbsp;&minus;&nbsp;<em>G</em>.
    (The probability is zero otherwise.)
    The expected value of the hypergeometric distribution is
    <em>n</em>&times;<em>G</em>/<em>N</em>,
    and its standard error is
</dd>
<p class="math">
    <big>(</big>(<em>N</em>&minus;<em>n</em>)/(<em>N</em>&minus;1)<big>)</big><sup>&frac12;</sup>
    &times; <big>(</big><em>n</em> &times;
    <em>G</em>/<em>N </em>&times; (1&minus;<em>G</em>/<em>N</em>)
    <big>)</big><sup>&frac12;</sup>.
</p>

<dt>
   <a id="hypothesis_test"></a>Hypothesis testing.
</dt>
<dd>
    Statistical hypothesis testing is formalized as making a decision between rejecting or
    not rejecting a <a class="glossRef" href="#null_hypothesis">null hypothesis</a>, on the basis of a set of
    observations.
    Two types of errors can result from any decision rule (test): rejecting the
    null hypothesis when it is true (a <a class="glossRef" href="#type_error">Type I error</a>), and failing to
    reject the null hypothesis when it is false (a <a class="glossRef" href="#type_error">Type II error</a>).
    For any hypothesis, it is possible to develop many different decision rules (tests).
    Typically, one specifies ahead of time the chance of a Type I error one is willing to
    allow.
    That chance is called the <a class="glossRef" href="#significance">significance level</a> of the
    test or decision rule.
    For a given significance level, one way of deciding which decision
    rule is best is to pick the one that has the smallest chance of a Type II error when a
    given <a class="glossRef" href="#alternative">alternative hypothesis</a> is true.
    The chance of correctly
    rejecting the null hypothesis when a given alternative hypothesis is true is
    called the <a class="glossRef" href="#power">power</a> of the test against that alternative.
<p>
    <!---I--->
</p>
    <hr />
    <p><a id="i"></a>I </p>
</dd>

<dt>
    <a id="iff"></a><a id="IFF"></a>iff, if and only if, &harr;
</dt>
<dd>
    If <em>p</em> and <em>q</em> are two <a class="glossRef" href="#proposition">logical propositions</a>,
    then(<em>p</em> &harr; <em>q</em>) is a proposition that is true when
    both <em>p</em> and <em>q</em> are true, and when both <em>p</em> and <em>q</em> are
    false.
    It is <a class="glossRef" href="#logicallyEquivalent">logically equivalent</a> to the proposition
</dd>
<dd>
<p class="math">
    <big>(</big> (<em>p</em> <a class="glossRef" href="#IMPLIES">&rarr;</a> <em>q</em>)
    <a class="glossRef" href="#AND">&amp;</a>
    (<em>q</em> <a class="glossRef" href="#IMPLIES">&rarr;</a> <em>p</em>) <big>)</big>
</p>
</dd>
<dd>
    and to the proposition
</dd>
<dd>
<p class="math">
    <big>(</big> (<em>p</em> &amp; <em>q</em>)
    <a class="glossRef" href="#OR">|</a> ((<a class="glossRef" href="#NOT">!</a>
    <em>p</em>) &amp; (!<em>q</em>)) <big>)</big>.
</p>
</dd>

<dt>
    <a id="implies"></a><a id="IMPLIES"></a><a id="conditional"></a>Implies,
    logical implication, &rarr; , conditional, if-then
</dt>
<dd>
    Logical implication is an operation on two <a class="glossRef" href="#proposition">logical propositions</a>.
    If <em>p</em> and <em>q</em> are two logical propositions,
    (<em>p</em> &rarr; <em>q</em>), pronounced &quot;p implies q&quot; or &quot;if p then q&quot;
    is a logical proposition that is
    true if <em>p</em> is false, or if both <em>p</em> and <em>q</em> are true.
    The proposition (<em>p</em> &rarr; <em>q</em>) is
    <a class="glossRef" href="#logicallyEquivalent">logically equivalent</a> to the proposition
    ((<a class="glossRef" href="#NOT">!</a><em>p</em>) <a class="glossRef" href="#OR">|</a> <em>q</em>).
    In the conditional <span class="math">p &rarr; q</math>, the
    <a class="glossRef" href="#antecedent">antecedent</a> is <span class="math">p</span>
    and the <a class="glossRef" href="#consequent">consequent</a> is <span class="math">q</span>.
</dd>

<dt>
   <a id="independent"></a>Independent, independence.
</dt>
<dd>
    Two <a class="glossRef" href="#event">events</a> A and B are (statistically) independent if the chance
    that they both happen simultaneously is the product of the chances that each occurs
    individually; <em>i.e.</em>, if P(AB) = P(A)P(B). This is essentially equivalent to saying
    that learning that one event occurs does not give any information about whether the other
    event occurred too: the conditional probability of A given B is the same as the
    unconditional probability of A, <em>i.e.</em>, P(A|B) = P(A). Two
    <a class="glossRef" href="#random_variable">random variables</a> X and Y are independent if all events
    they
    <a class="glossRef" href="#event">determine</a> are independent, for example, if the event
    {<em>a</em> &lt; X &le; <em>b</em>}
    is independent of the event {<em>c</em> &lt; Y &le; <em>d</em>} for
    <em>all</em>
    choices of <em>a</em>, <em>b</em>, <em>c</em>, and <em>d</em>.
    A collection of more than two random variables is independent if for every proper subset
    of the variables, every event determined
    by that subset of the variables is independent of every event determined by the variables
    in the complement of the subset. For example, the three random variables X, Y, and Z are
    independent if every event determined by X is independent of every event
    determined by Y and
    every event determined by X is independent of every event determined by Y and Z
    and every event determined by Y is
    independent of every event determined by X and Z and every event determined by Z
    is independent of every event determined by X and Y.
</dd>

<dt>
   <a id="iid"></a>Independent and identically distributed (iid).
</dt>
<dd>
    A collection of two or more random variables {X<sub>1</sub>, X<sub>2</sub>,
    &hellip; , }
    is <em>independent and identically distributed</em> if the variables have the same
    probability distribution,
    and are <a class="glossRef" href="#independent">independent</a>.
</dd>

<dt>
   <a id="independent_variable"></a>Independent Variable.
</dt>
<dd>
    In <a class="glossRef" href="#regression">regression</a>, the independent variable is the one that is
    supposed to explain the other; the term is a synonym for &quot;explanatory variable.&quot;
    Usually, one regresses the &quot;dependent variable&quot; on the &quot;independent
    variable.&quot; There is not always a clear choice of the independent variable. The
    independent variable is usually plotted on the horizontal axis. Independent in this
    context does not mean the same thing as
    <a class="glossRef" href="#independent">statistically independent</a>.
</dd>

<dt>
   <a id="indicator"></a>Indicator <a class="glossRef" href="#random_variable">Random Variable</a>.
</dt>
<dd>
    The indicator [<a class="glossRef" href="#random_variable">random variable</a>] of the
    <a class="glossRef" href="#event">event</a> A, often written 1<sub>A</sub>, is the
    <a class="glossRef" href="#random_variable">random variable</a> that
    equals unity if A occurs, and zero if A does not occur.
    The <a class="glossRef" href="#expectation">expected
    value</a> of the indicator of A is the probability of A, P(A), and the
    <a class="glossRef" href="#se">standard error</a> of the indicator of A is<big>
    (</big>P(A)&times;(1&minus;P(A)<big>)</big><sup>&frac12;</sup>.
    The sum
</dd>
<dd>
<p class="math">
    1<sub>A</sub> + <sub></sub>1<sub>B</sub> + 1<sub>C</sub> +
    &hellip;
</p>
</dd>
<dd>
    of the indicators of a
    collection of <a class="glossRef" href="#event">events</a> {A, B, C, &hellip;}
    counts how many of the
    <a class="glossRef" href="#event">events</a> {A, B, C, &hellip;} occur in a given
    trial.
    The product of the indicators of a collection of events is the indicator of the
    intersection of the events (the product equals one if and only if all of
    indicators equal one).
    The maximum of the indicators of a collection of events is the indicator
    of the union of the events (the maximum equals one if any of the indicators equals one).
</dd>

<dt>
    <a id="IQR"></a>Inter-quartile Range (IQR).
</dt>
<dd>
    The inter-quartile range of a list of numbers is the <a class="glossRef" href="#quartiles">upper
    quartile</a>
    minus the <a class="glossRef" href="#quartiles">lower quartile</a>.
</dd>

<dt>
    <a id="interpolation"></a>Interpolation.
</dt>
<dd>
    Given a set of <a class="glossRef" href="#bivariate">bivariate data</a> (<em>x</em>, <em>y</em>), to
    impute a value of <em>y</em> corresponding to some value of <em>x</em> at which there is
    no measurement of <em>y</em> is called interpolation, if the value of <em>x</em> is within
    the range of the measured values of <em>x</em>. If the value of <em>x</em> is outside the
    range of measured values, imputing a corresponding value of <em>y</em> is called
    <a class="glossRef" href="#extrapolation">extrapolation</a>.
</dd>

<dt>
    <a id="intersection"></a>Intersection.
</dt>
<dd>
    The intersection of two or more sets is the set of elements that all the sets have in
    common; the elements contained in every one of the sets.
    The intersection of the <a class="glossRef" href="#event">events</a> A and B is written &quot;A&cap;B,&quot;
    &quot;A and B,&quot; and &quot;AB.&quot; <em>C.f.</em>
    <a class="glossRef" href="#union">union</a>. See also <a class="glossRef" href="#venn_diagram">Venn diagrams</a>.
</dd>

<dt>
    <a id="invalidArgument"></a>Invalid (logical) argument.
</dt>
<dd>
    An invalid <a class="glossRef" href="#logicalArgument">logical argument</a> is one in which
    the truth of the <a class="glossRef" href="#premises">premises</a> does not guarantee the truth
    of the <a class="glossRef" href="#conclusion">conclusion</a>.
    For example, the following logical argument is invalid:
    If the forecast calls for rain, I will not wear sandals.
    The forecast does not call for rain.
    Therefore, I will wear sandals.
    See also <a class="glossRef" href="#validArgument">valid argument</a>.
</dd>

<dd>
<p align="center"><!---J---> </p>
    <hr />
<p><a id="j"></a>J</p>

</dd>

<dt>
   <a id="joint_prob_dist"></a>Joint Probability Distribution.
</dt>
<dd>
    If <em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, &hellip; ,
    <em>X</em><sub><em>k</em></sub> are
    <a class="glossRef" href="#random_variable">random variables</a> defined for the same experiment,
    their <em>joint probability distribution</em> gives the probability
    of events determined by the collection of random variables:
    for any collection of sets of numbers
    {<em>A</em><sub>1</sub>, &hellip; , <em>A</em><sub><em>k</em></sub>},
    the joint probability distribution determines
<p class="math">
    P<big>(</big> (<em>X</em><sub>1</sub> is in <em>A</em><sub>1</sub>) and
              (<em>X</em><sub>2</sub> is in <em>A</em><sub>2</sub>) and &hellip; and
              (<em>X</em><sub><em>k</em></sub> is in <em>A</em><sub><em>k</em></sub>)
     <big>)</big>.
    </p>
    For example, suppose we roll two fair dice independently.
    Let <span class="math">X<sub>1</sub></span> be the number of spots that show on the first die,
    and let <span class="math">X<sub>2</sub></span> be the total number of spots that show on both dice.
    Then the joint distribution of <span class="math">X<sub>1</sub></span> and
    <span class="math">X<sub>2</sub></span>
    is as follows:
    </p>
    <p class="math">
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 2) =
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 3) =
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 4) =
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 5) =
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 6) =
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 7) =
    </p>
    <p class="math">
        P(X<sub>1</sub> = 2, X<sub>2</sub> = 3) =
        P(X<sub>1</sub> = 2, X<sub>2</sub> = 4) =
        P(X<sub>1</sub> = 2, X<sub>2</sub> = 5) =
        P(X<sub>1</sub> = 2, X<sub>2</sub> = 6) =
        P(X<sub>1</sub> = 2, X<sub>2</sub> = 7) =
        P(X<sub>1</sub> = 2, X<sub>2</sub> = 8) = &hellip;
    </p>
    <p class="math">
        &hellip; P(X<sub>1</sub> = 6, X<sub>2</sub> = 7) =
        P(X<sub>1</sub> = 6, X<sub>2</sub> = 8) =
        P(X<sub>1</sub> = 6, X<sub>2</sub> = 9) =
        P(X<sub>1</sub> = 6, X<sub>2</sub> = 10) =
        P(X<sub>1</sub> = 6, X<sub>2</sub> = 11) =
        P(X<sub>1</sub> = 6, X<sub>2</sub> = 12) = 1/36.
    </p>
       If a collection of random variables is <a class="glossRef" href="#independent">independent</a>,
       their joint probability distribution is the product of their
       <a class="glossRef" href="#marginal_prob_dist">marginal probability distributions</a>, their
       individual probability distributions without regard for the value of the other variables.
       In this example, the marginal probability distribution of <span class="math">X<sub>1</sub></span>
       is
    <p class="math">
       P(X<sub>1</sub> = 1) = P(X<sub>1</sub> = 2) = P(X<sub>1</sub> = 3) =
       P(X<sub>1</sub> = 4) = P(X<sub>1</sub> = 5) = P(X<sub>1</sub> = 6) = 1/6,
    </p>
    and the marginal probability distribution of <span class="math">X<sub>2</sub></span> is
    <p class="math">
       P(X<sub>2</sub> = 2) = P(X<sub>2</sub> = 12) = 1/36
    </p>
    <p class="math">
       P(X<sub>2</sub> = 3) = P(X<sub>2</sub> = 11) = 1/18
    </p>
    <p class="math">
        P(X<sub>2</sub> = 4) =  P(X<sub>2</sub> = 10) = 3/36
    </p>
    <p class="math">
        P(X<sub>2</sub> = 5) =  P(X<sub>2</sub> = 9) = 1/9
    </p>
    <p class="math">
        P(X<sub>2</sub> = 6) =  P(X<sub>2</sub> = 8) = 5/36
    </p>
    <p class="math">
        P(X<sub>2</sub> = 7) = 1/6.
    </p>
    Note that <span class="math">P(X<sub>1</sub> = 1, X<sub>2</sub> = 10) = 0</span>,
    while <span class="math">P(X<sub>1</sub> = 1)&times;P(X<sub>2</sub> = 10) = (1/6)(3/36) = 1/72</span>.
    The joint probability is not equal to the product of the marginal probabilities:
    <span class="math">X<sub>1</sub></span> and <span class="math">X<sub>2</sub></span>
    are <a class="glossRef" href="#dependent">dependent</a> random variables.
</dd>

<hr />
<p>
    <a id="k"></a>K <!---K--->
</p>

<hr />
<p>
    <a id="l"></a>L <!---L--->
</p>

<dt>
   <a id="law_of_averages"></a>Law of Averages.
</dt>
<dd>
    The Law of Averages says that the <a class="glossRef" href="#mean">average</a> of
    <a class="glossRef" href="#independent">independent</a>
    observations of <a class="glossRef" href="#random_variable">random variables</a>
    that have the same <a class="glossRef" href="#prob_distribution">probability distribution</a> is
    increasingly likely to be close
    to the <a class="glossRef" href="#expectation">expected value</a> of the
    <a class="glossRef" href="#random_variable">random
    variables</a> as the number of observations grows.
    More precisely, if <span class="math">X<sub>1</sub></span>, <span class="math">X<sub>2</sub></span>,
    <span class="math">X<sub>3</sub></span>, &hellip;, are independent
    <a class="glossRef" href="#random_variable">random variables</a> with
    the same <a class="glossRef" href="#prob_distribution">probability distribution</a>, and
    <span class="math">E(X)</span> is their
    common <a class="glossRef" href="#expectation">expected value</a>, then for
    every number &epsilon; &gt; 0,
</dd>
<dd>
<p class="math">
    P{|(X<sub>1</sub> + X<sub>2</sub> + &hellip; +
    X<sub><em>n</em></sub>)/<em>n</em>
       &minus; E(X) | &lt; &epsilon;}
</p>
</dd>
<dd>
    converges to 100% as <span class="math">n</span> grows.
    This is equivalent to saying that the sequence of sample means
</dd>
<dd>
    <p class="math">
    X<sub>1</sub>, (X<sub>1</sub>+X<sub>2</sub>)/2,
    (X<sub>1</sub>+X<sub>2</sub>+X<sub>3</sub>)/3, &hellip;
    </p>
</dd>
<dd>
    <a class="glossRef" href="#converge_in_prob">converges in probability</a> to <span class="math">E(X)</span>.
</dd>

<dt>
  <a id="law_of_large_numbers"></a>Law of Large Numbers.
</dt>
<dd>
    The Law of Large Numbers says that in repeated, <a class="glossRef" href="#independent">independent</a>
    trials with the same probability <em>p</em> of success in each trial, the percentage of
    successes is increasingly likely to be close to the chance of success as the number of
    trials increases. More precisely, the chance that the percentage of successes differs from
    the probability <em>p</em> by more than a fixed positive amount, <span class="math">e &gt; 0</span>,
    converges to zero as the number of trials <em>n</em> goes to infinity, for every number
    <span class="math">e &gt; 0</span>. Note that in contrast to the difference between the <em>percentage</em> of
    successes and the probability of success, the difference between the <em>number</em> of
    successes and the <a class="glossRef" href="#expected_value">expected</a> number of successes,
    <span class="math">n&times;p</span>,
    tends to grow as <span class="math">n</span> grows.
    The following tool illustrates the law of large numbers; the button toggles between
    displaying the difference between the number of successes and the expected number of
    successes, and the difference between the percentage of successes and the expected
    percentage of successes.
    The tool on <a href="../../Java/Html/lln.htm" target="lablet">this page</a> illustrates
    the law of large numbers.
</dd>

<dt>
   <a id="limit"></a>Limit.
</dt>
<dd>
    See <a class="glossRef" href="#converge"><em>converge</em></a>.
</dd>

<dt>
   <a id="linear"></a>Linear Operation.
</dt>
<dd>
    Suppose <em>f</em> is a function or operation that acts on things we shall denote
    generically by the lower-case Roman letters <em>x</em> and <em>y</em>. Suppose it makes
    sense to multiply <em>x</em> and <em>y</em> by numbers (which we denote by <em>a</em>),
    and that it makes sense to add things like <em>x</em> and <em>y</em> together. We say that
    <em>f</em> is <em>linear</em> if for every number <em>a</em> and every value of <em>x</em>
    and <em>y</em> for which <em>f</em>(<em>x</em>) and <em>f</em>(<em>y</em>) are defined,
    (i) <em>f</em>( <em>a</em>&times;<em>x</em> ) is defined and equals
    <em>a</em>&times;<em>f</em>(<em>x</em>),
    and (ii) <em>f</em>( <em>x</em> + <em>y</em> ) is defined and equals
    <em>f</em>(<em>x</em>)
    + <em>f</em>(<em>y</em>). <em>C.f. </em><a class="glossRef" href="#affine">affine</a>.
</dd>

<dt>
   <a id="linear_association"></a>Linear association.
</dt>
<dd>
    Two variables are linearly associated if a change in one is associated with a
    proportional change in the other, with the same constant of proportionality throughout the
    range of measurement. The <a class="glossRef" href="#correlation_coef">correlation coefficient</a> measures
    the degree of linear association on a scale of &minus;1 to 1.
</dd>

<dt>
    <a id="location"></a>Location, Measure of.
</dt>
<dd>
    A measure of location is a way of summarizing what a &quot;typical&quot; element of a
    list is&mdash;it is a one-number summary of a <a class="glossRef" href="#distribution">distribution</a>. See
    also <a class="glossRef" href="#mean">arithmetic mean</a>, <a class="glossRef" href="#median">median</a>, and
    <a class="glossRef" href="#mode">mode</a>.
</dd>

<dt>
  <a id="logicalArgument"></a>Logical argument.
</dt>
<dd>
   A logical argument consists of one or more <a class="glossRef" href="#premise">premises</a>,
   <a class="glossRef" href="#proposition">propositions</a>
   that are assumed to be true, and a <em>conclusion</em>, a proposition that is
   supposed to be guaranteed to be true (as a matter of pure logic) if the premises
   are true.
   For example, the following is a logical argument:
<ul>
   <li> <em>p</em> <a class="glossRef" href="#IMPLIES">&rarr;</a> <em>q</em></li>
   <li> <em>p</em></li>
   <li> Therefore, <em>q</em>. </li>
</ul>
   This argument has two premises: <em>p</em> <a class="glossRef" href="#IMPLIES">&rarr;</a> <em>q</em>,
   and <em>p</em>.
   The conclusion of the argument is <em>q</em>.
   If a logical argument is <a class="glossRef" href="#validArgument">valid</a> if the truth of the premises
   guarantees the truth of the conclusion; otherwise, the argument is
   <a class="glossRef" href="#invalidArgument">invalid</a>.
   That is, an argument with premises <em>p</em><sub>1</sub>, <em>p</em><sub>1</sub>,
   &hellip; <em>p</em><sub>n</sub> and conclusion <em>q</em> is valid if the
   <a class="glossRef" href="#compoundProposition">compound proposition</a>
<p class="math">
    (<em>p</em><sub>1</sub> &amp; <em>p</em><sub>2</sub> &amp; &hellip; &amp; <em>p</em><sub>n</sub>)
      &rarr; <em>q</em>
</p>
   is <a class="glossRef" href="#logicallyEquivalent">logically equivalent</a> to TRUE.
   The argument given above is valid because if it is true that <em>p</em> &rarr; <em>q</em>
   and that <em>p</em> is true (the two premises), then <em>q</em>
   (the conclusion of the argument) must also be true.
</dd>

<dt>
  <a id="logicallyEquivalent"></a>Logically equivalent, logical equivalence.
</dt>
<dd>
   Two <a class="glossRef" href="#proposition">propositions</a> are logically equivalent if they always
   have the same truth value.
   That is, the propositions <em>p</em> and <em>q</em> are logically equivalent
   if <em>p</em> is true
   whenever <em>q</em> is true and <em>p</em> is false whenever <em>q</em> is false.
   The proposition (<em>p</em> &harr; <em>q</em>) is always true if and only if <em>p</em> and
   <em>q</em> are logically equivalent.
   For example, <em>p</em> is logically equivalent to <em>p</em>, to
   (<em>p</em> &amp; <em>p</em>), and to (<em>p</em> | <em>p</em>);
   (<em>p</em> <a class="glossRef" href="#or">|</a> (<a class="glossRef" href="#not">!</a><em>p</em>))
   is logically equivalent to TRUE;
   (<em>p</em> <a class="glossRef" href="#and">&amp;</a> !<em>p</em>) is logically equivalent to
   FALSE;
   (<em>p</em> <a class="glossRef" href="#iff">&harr;</a> <em>p</em>) is logically equivalent to TRUE;
   and (<em>p</em> <a class="glossRef" href="#IMPLIES">&rarr;</a> <em>q</em>) is
   logically equivalent to (!<em>p</em> | <em>q</em>).
</dd>

<dt>
  <a id="longitudinal"></a>Longitudinal study.
</dt>
<dd>
    A <a class="glossRef" href="#study">study</a> in which individuals are followed over time, and compared
    with themselves at different times, to determine, for example, the effect of aging on some
    measured variable. Longitudinal <a class="glossRef" href="#study">studies</a> provide much more persuasive
    evidence about the effect of aging than do <a class="glossRef" href="#cross_sectional">cross-sectional
    studies</a>.
</dd>

<dt>
   <a id="lq"></a>Lower Quartile (LQ).
</dt>
<dd>
    See <a class="glossRef" href="#quartiles">quartiles</a>.
</dd>

<dd>
<hr />
    <p><a id="m"></a>M </p>

</dd>

<dt>
    <a id="margin_of_error"></a>Margin of error.
</dt>
<dd>
    A measure of the uncertainty in an <a class="glossRef" href="#estimate">estimate</a> of a
    <a class="glossRef" href="#parameter">parameter</a>; unfortunately, not everyone
    agrees what it should mean.
    The <em>margin of error</em> of an estimate is typically
    one or two times the estimated <a class="glossRef" href="#se">standard error</a> of the estimate.
</dd>

<dt>
   <a id="marginal_prob_dist"></a>Marginal probability distribution.
</dt>
<dd>
    The marginal probability distribution of a random variable that has a
    <a class="glossRef" href="#joint_prob_dist">joint probability distribution</a>
    with some other random variables is the probability distribution of that
    random variable without regard for the values that the other random variables take.
    The marginal distribution of a discrete random variable <span class="math">X<sub>1</sub></span>
    that has a joint distribution with other discrete random variables can be found from the
    joint distribution by summing over all possible values of the other variables.
    For example, suppose we roll two fair dice independently.
    Let <span class="math">X<sub>1</sub></span> be the number of spots that show on the first die,
    and let <span class="math">X<sub>2</sub></span> be the total number of spots that show on both dice.
    Then the joint distribution of <span class="math">X<sub>1</sub></span> and
    <span class="math">X<sub>2</sub></span>
    is as follows:
    </p>
    <p class="math">
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 2) =
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 3) =
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 4) =
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 5) =
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 6) =
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 7) =
    </p>
    <p class="math">
        P(X<sub>1</sub> = 2, X<sub>2</sub> = 3) =
        P(X<sub>1</sub> = 2, X<sub>2</sub> = 4) =
        P(X<sub>1</sub> = 2, X<sub>2</sub> = 5) =
        P(X<sub>1</sub> = 2, X<sub>2</sub> = 6) =
        P(X<sub>1</sub> = 2, X<sub>2</sub> = 7) =
        P(X<sub>1</sub> = 2, X<sub>2</sub> = 8) = &hellip;
    </p>
    <p class="math">
        &hellip; P(X<sub>1</sub> = 6, X<sub>2</sub> = 7) =
        P(X<sub>1</sub> = 6, X<sub>2</sub> = 8) =
        P(X<sub>1</sub> = 6, X<sub>2</sub> = 9) =
        P(X<sub>1</sub> = 6, X<sub>2</sub> = 10) =
        P(X<sub>1</sub> = 6, X<sub>2</sub> = 11) =
        P(X<sub>1</sub> = 6, X<sub>2</sub> = 12) = 1/36.
    </p>
       The marginal probability distribution of <span class="math">X<sub>1</sub></span>
       is
    <p class="math">
       P(X<sub>1</sub> = 1) = P(X<sub>1</sub> = 2) = P(X<sub>1</sub> = 3) =
       P(X<sub>1</sub> = 4) = P(X<sub>1</sub> = 5) = P(X<sub>1</sub> = 6) = 1/6.
    </p>
    We can verify that the marginal probability that <span class="math">X<sub>1</sub> = 1</span>
    is indeed the sum of the joint probability distribution
    over all possible values of <span class="math">X<sub>2</sub></span> for which
    <span class="math">X<sub>1</sub> = 1</span>:
    <p class="math">
        P(X<sub>1</sub> = 1) = P(X<sub>1</sub> = 1, X<sub>2</sub> = 2) +
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 3) +
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 4) +
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 5) +
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 6) +
        P(X<sub>1</sub> = 1, X<sub>2</sub> = 7) = 6/36 = 1/6.
    </p>
    Similarly, the marginal probability distribution of <span class="math">X<sub>2</sub></span> is
    <p class="math">
       P(X<sub>2</sub> = 2) = P(X<sub>2</sub> = 12) = 1/36
    </p>
    <p class="math">
       P(X<sub>2</sub> = 3) = P(X<sub>2</sub> = 11) = 1/18
    </p>
    <p class="math">
        P(X<sub>2</sub> = 4) =  P(X<sub>2</sub> = 10) = 3/36
    </p>
    <p class="math">
        P(X<sub>2</sub> = 5) =  P(X<sub>2</sub> = 9) = 1/9
    </p>
    <p class="math">
        P(X<sub>2</sub> = 6) =  P(X<sub>2</sub> = 8) = 5/36
    </p>
    <p class="math">
        P(X<sub>2</sub> = 7) = 1/6.
    </p>
    Again, we can verify that the marginal probability that <span class="math">X<sub>2</sub> = 4</span>
    is <span class="math">3/36</span> by adding the joint probabilities for all possible values of
    <span class="math">X<sub>1</sub></span> for which <span class="math">X<sub>2</sub> = 4</span>:
    <p class="math">
       P(X<sub>2</sub> = 4) = P(X<sub>1</sub> = 1, X<sub>2</sub> = 4) +
       P(X<sub>1</sub> = 2, X<sub>2</sub> = 4) +
       P(X<sub>1</sub> = 3, X<sub>2</sub> = 4) = 3/36.
    </p>
</dd>

<dt>
   <a id="markov"></a>Markov's Inequality.
</dt>
<dd>
    For lists: If a list contains no negative numbers, the fraction of numbers in the list
    at least as large as any given constant <em>a</em>&gt;0 is no larger than the
    <a class="glossRef" href="#mean">arithmetic mean</a> of the list, divided by <em>a</em>.<br />
    For <a class="glossRef" href="#random_variable">random variables</a>: if a random variable X must be
    nonnegative, the chance that X exceeds any given constant <em>a</em>&gt;0 is no larger than
    the <a class="glossRef" href="#expectation">expected value</a> of X, divided by <em>a</em>.
</dd>

<dt>
   <a id="mle"></a>Maximum Likelihood Estimate (MLE).
</dt>
<dd>
    The maximum likelihood estimate of a <a class="glossRef" href="#parameter">parameter</a> from data is the
    possible value of the <a class="glossRef" href="#parameter">parameter</a> for which the chance of observing
    the data largest. That is, suppose that the <a class="glossRef" href="#parameter">parameter</a> is <em>p</em>,
    and that we observe data <em>x</em>. Then the maximum likelihood estimate of
    <em>p</em> is
</dd>
<dd>
<p class="math">
    estimate <em>p</em> by the value <em>q</em> that makes P(observing <em>x</em> when the
    value of <em>p</em> is <em>q</em>) as large as possible.
</p>
</dd>
<dd>
    For example, suppose we are trying to estimate the chance that a (possibly biased) coin
    lands heads when it is tossed. Our data will be the number of times <em>x</em> the coin
    lands heads in <em>n</em> independent tosses of the coin. The distribution of the number
    of times the coin lands heads is <a class="glossRef" href="#binomial">binomial</a> with
    <a class="glossRef" href="#parameter">parameters</a> <em>n</em> (known) and <em>p</em> (unknown). The chance
    of observing <em>x</em> heads in <em>n</em> trials if the chance of heads in a given trial
    is <em>q</em> is
    <em><sub>n</sub>C<sub>x</sub> q<sup>x</sup></em>(1<em>&minus;q</em>)<em><sup>n&minus;x</sup>.
    </em>The maximum likelihood estimate of <em>p</em> would be the value of <em>q</em> that
    makes that chance largest. We can find that value of <em>q</em> explicitly using calculus;
    it turns out to be <em>q</em> = <em>x</em>/<em>n</em>, the fraction of times the coin is
    observed to land heads in the <em>n</em> tosses. Thus the maximum likelihood estimate of
    the chance of heads from the number of heads in <em>n</em> independent tosses of the coin
    is the observed fraction of tosses in which the coin lands heads.
</dd>

<dt>
    <a id="mean"></a>Mean, Arithmetic mean.
</dt>
<dd>
    The sum of a list of numbers, divided by the number of numbers.
    See also <a class="glossRef" href="#average">average</a>.
</dd>

<dt>
    <a id="mse"></a>Mean Squared Error (MSE).
</dt>
<dd>
    The mean squared error of an <a class="glossRef" href="#estimator">estimator</a> of a
    <a class="glossRef" href="#parameter">parameter</a> is the <a class="glossRef" href="#expectation">expected value</a> of the
    square of the difference between the estimator and the parameter. In symbols, if X is an
    estimator of the parameter <em>t</em>, then
</dd>

<p class="math">
    MSE(X) = <a class="glossRef" href="#expectation">E</a><big>(</big> (X&minus;<em>t</em>)<sup>2</sup> <big>)</big>.
</p>
<dd>
    The MSE measures how far the estimator is off from what it is trying to estimate, on the
    average in repeated experiments. It is a summary measure of the accuracy of the estimator.
    It combines any tendency of the estimator to overshoot or undershoot the truth
    (<a class="glossRef" href="#bias">bias</a>), and the variability of the estimator (<a class="glossRef" href="#se">SE</a>).
    The MSE can be written in terms of the <a class="glossRef" href="#bias">bias</a> and
    <a class="glossRef" href="#se">SE</a> of the estimator:
</dd>
<dd>
    <p class="math">
       MSE(X) = (<a class="glossRef" href="#bias">bias</a>(X))<sup>2</sup> +
      (<a class="glossRef" href="#se">SE</a>(X))<sup>2</sup>.
    </p>
</dd>

<dt>
   <a id="median"></a>Median.
</dt>
<dd>
    &quot;Middle value&quot; of a list. The smallest number such that at least half the
    numbers in the list are no greater than it. If the list has an odd number of entries, the
    median is the middle entry in the list after sorting the list into increasing order. If
    the list has an even number of entries, the median is the smaller of the two middle
    numbers after sorting. The median can be estimated from a histogram by finding the
    smallest number such that the area under the histogram to the left of that
    number is 50%.
</dd>

<dt>
   <a id="member"></a>Member of a set.
</dt>
<dd>
    Something is a member (or element) of a <a class="glossRef" href="#set">set</a> if it is one of the
    things in the <a class="glossRef" href="#set">set</a>.
</dd>

<dt>
  <a id="comparison"></a>Method of Comparison.
</dt>
<dd>
    The most basic and important method of determining whether a
    <a class="glossRef" href="#treatment">treatment
    </a>has an effect: compare what happens to individuals who are treated
    (the <a class="glossRef" href="#treatment_group">treatment group</a>) with what happens to
    individuals who are not
    treated (the <a class="glossRef" href="#control_group">control group</a>).
</dd>

<dt>
   <a id="minimax_strategy"></a>Minimax Strategy.
</dt>
<dd>
    In game theory, a minimax strategy is one that minimizes one's maximum loss, whatever
    the opponent might do (whatever strategy the opponent might choose).
</dd>

<dt>
  <a id="mode"></a>Mode.
</dt>
<dd>
    For lists, the mode is a most common (frequent) value. A list can have more than one
    mode. For <a class="glossRef" href="#histogram">histograms</a>, a mode is a relative maximum
    (&quot;bump&quot;).
</dd>

<dt>
    <a id="moment"></a>Moment.
</dt>
<dd>
    The <em>k</em>th moment of a list is the average value of the elements raised to
    the <em>k</em>th power; that is, if the list consists of the <em>N</em> elements
    <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, &hellip; ,
    <em>x</em><sub><em>N</em></sub>,
    the <em>k</em>th moment of the list is
</dd>
<dd>
    <p class="math">
    <big>(</big>
        <em>x</em><sub>1</sub><sup><em>k</em></sup> +
        <em>x</em><sub>2</sub><sup><em>k</em></sup> +
        <em>x</em><sub><em>N</em></sub><sup><em>k</em></sup>
    <big>)</big>/<em>N</em>.
    </p>
</dd>
<dd>
    The <em>k</em>th moment of a <a class="glossRef" href="#random_variable">random variable</a> X is
    the <a class="glossRef" href="#expectation">expected value</a> of X<sup><em>k</em></sup>,
    E<big>(</big>X<sup><em>k</em></sup><big>)</big>.
</dd>


<dt>
   <a id="monotonic"></a>Monotone, monotonic function.
</dt>
<dd>
    A function is monotone if it only increases or only decreases:
    <em>f</em> increases monotonically (is monotonic increasing)
    if <em>x</em> &gt; <em>y</em>, implies that<em>f</em>(<em>x</em>)
    &ge; <em>f</em>(<em>y</em>).
    A function <em>f</em> decreases monotonically (is monotonic decreasing)
    if <em>x</em> &gt; <em>y</em>, implies that<em>f</em>(<em>x</em>)
    &le; <em>f</em>(<em>y</em>).
    A function <em>f</em> is strictly monotonically increasing
    if <em>x</em> &gt; <em>y</em>, implies that<em>f</em>(<em>x</em>)
    &gt; <em>f</em>(<em>y</em>), and strictly monotonically decreasing if
    if <em>x</em> &gt; <em>y</em>, implies that<em>f</em>(<em>x</em>)
    &lt; <em>f</em>(<em>y</em>).
</dd>

<dt>
   <a id="multimodal"></a>Multimodal Distribution.
</dt>
<dd>
    A distribution with more than one <a class="glossRef" href="#mode">mode</a>.
    The <a class="glossRef" href="#histogram">histogram</a>
    of a multimodal distribution has more than one &quot;bump.&quot;
</dd>

<dt>
    <a id="multinomial_distribution"></a>
    Multinomial Distribution
</dt>
<dd>
    Consider a sequence of <em>n</em> <a class="glossRef" href="#independent">independent</a> trials,
    each of which can result in an outcome in any of <em>k</em> categories.
    Let <em>p</em><sub><em>j</em></sub> be the probability that each trial results
    in an outcome in category <em>j</em>, <em>j</em> = 1, 2, &hellip; ,
    <em>k</em>,
    so
</dd>
<dd>
    <p class="math">
    <em>p</em><sub>1</sub> + <em>p</em><sub>2</sub> + &hellip; +
    <em>p</em><sub><em>k</em></sub>
    = 100%.
    </p>
</dd>
<dd>
    The number of outcomes of each type has a <em>multinomial distribution</em>.
    In particular, the probability that the <em>n</em> trials result in
    <em>n</em><sub>1</sub>
    outcomes of type 1, <em>n</em><sub>2</sub> outcomes of type 2,
    &hellip; , and
    <em>n</em><sub><em>k</em></sub> outcomes of type <em>k</em> is
</dd>
<dd>
    <p class="math">
    <em>n</em>!/(<em>n</em><sub>1</sub>! &times; <em>n</em><sub>2</sub>! &times;
    &hellip; &times; <em>n</em><sub><em>k</em></sub>!) &times;
    <em>p</em><sub>1</sub><sup><em>n</em><sub>1</sub></sup> &times;
    <em>p</em><sub>2</sub><sup><em>n</em><sub>2</sub></sup> &times;
    &hellip; &times;
    <em>p</em><sub><em>k</em></sub><sup><em>n</em><sub><em>k</em></sub></sup>,
    </p>
</dd>
<dd>
    if <em>n</em><sub>1</sub>, &hellip; , <em>n</em><sub><em>k</em></sub> are
    nonnegative integers that sum to <em>n</em>; the chance is zero otherwise.
</dd>

<dt>
   <a id="multiplication_rule"></a>Multiplication rule.
</dt>
<dd>
    The chance that <a class="glossRef" href="#event">events</a> A and B both occur (<em>i.e.</em>,
    that <a class="glossRef" href="#event">event</a> <a class="glossRef" href="#intersection">AB</a> occurs), is the
    <a class="glossRef" href="#conditional_probability">conditional probability</a> that A occurs given that B
    occurs, times the unconditional probability that B occurs.
</dd>

<dt>
   <a id="multiple_tests"></a>Multiplicity in hypothesis tests.
</dt>
<dd>
    In <a class="glossRef" href="#hypothesis_test">hypothesis testing</a>,
    if more than one hypothesis is tested, the actual
    <a class="glossRef" href="#significance">significance level</a> of
    the combined tests is not equal to the nominal
    <a class="glossRef" href="#significance">significance level</a> of the individual tests.
    See also <a class="glossRef" href="#fdr">false discovery rate</a>.
</dd>

<dt>
   <a id="multivariate"></a>Multivariate Data.
</dt>
<dd>
    A set of measurements of two or more <a class="glossRef" href="#variable">variables</a> per individual.
    See <a class="glossRef" href="#bivariate">bivariate</a>.
</dd>

<dt>
  <a id="mutually_exclusive"></a>Mutually Exclusive.
</dt>
<dd>
   See <a class="glossRef" href="#disjoint">disjoint events</a> or
  <a class="glossRef" href="#disjoint_sets">disjoint sets</a>.
</dd>

<dd>
<p align="center"><!---N---> </p>
    <hr />
<p>
    <a id="n"></a>N
</p>

</dd>

<dt>
    <a id="nearly_normal"></a>
    Nearly normal distribution.
</dt>
<dd>
    A population of numbers (a list of numbers) is said to have a <em>nearly normal
    distribution</em> if the <a class="glossRef" href="#histogram">histogram</a> of its values in
    <a class="glossRef" href="#standard_units">standard units</a> nearly
    follows a <a class="glossRef" href="#normal_curve">normal curve</a>.
    More precisely, suppose that the <a class="glossRef" href="#population_mean">mean</a> of the
    list is &mu; and the <a id="#population_sd">standard deviation</a>
    of the list is SD.
    Then the list is nearly normally distributed if, for every two numbers
    <em>a</em>&nbsp;&lt;&nbsp;<em>b</em>, the fraction of numbers in the list that are
    between <em>a</em> and <em>b</em> is approximately equal to the area under the normal
    curve between (<em>a</em>&nbsp;&minus;&nbsp;&mu;)/SD and
    (<em>a</em>&nbsp;&minus;&nbsp;&mu;)/SD.
</dd>

<dt>
   <a id="negative_binomial"></a>Negative Binomial Distribution.
</dt>
<dd>
    Consider a sequence of <a class="glossRef" href="#independent">independent</a> trials with the same
    probability <em>p</em> of success in each trial. The number of trials up to and including
    the <em>r</em>th success has the negative Binomial distribution with parameters <em>n</em>
    and <em>r</em>. If the <a class="glossRef" href="#random_variable">random variable</a> N has the negative
    binomial distribution with parameters <em>n</em> and <em>r</em>, then
</dd>
<p class="math">
    P(N=<em>k</em>) =
    <sub><em>k</em>&minus;1</sub><em>C<sub>r</em>&minus;1</sub><em> &times; <em>p</em><sup>r</sup> &times;
    </em>(1&minus;<em>p</em>)<sup><em>k&minus;r</em></sup>,
</p>
<dd>
    for<em> k </em>= <em>r</em>, <em>r</em>+1, <em>r+</em>2, &hellip;, and zero for <em>k</em>
    &lt; <em>r</em>, because there must be at least <em>r</em> trials to have <em>r</em>
    successes. The negative binomial distribution is derived as follows: for the <em>r</em>th
    success to occur on the <em>k</em>th trial, there must have been <em>r</em>&minus;1 successes in
    the first <em>k</em>&minus;1 trials, and the <em>k</em>th trial must result in success. The
    chance of the former is the chance of <em>r</em>&minus;1 successes in <em>k</em>&minus;1
    <a class="glossRef" href="#independent">independent</a> trials with the same probability of success in each
    trial, which, according to the <a class="glossRef" href="#binomial">Binomial distribution</a> with
    parameters <em>n</em>=<em>k</em>&minus;1 and <em>p</em>, has probability
</dd>
<p class="math">
    <sub><em>k</em>&minus;1</sub><em>C<sub>r</em>&minus;1</sub> &times;
    <em>p<sup>r&minus;</em>1</sup> &times; (1<em>&minus;p</em>)<sup><em>k&minus;r</em></sup>.
</p>
<dd>
    The chance of the latter event is <em>p</em>, by assumption. Because the trials are
    <a class="glossRef" href="#independent">independent</a>, we can find the chance that both
    <a class="glossRef" href="#event">events</a>
    occur by multiplying their chances together, which gives the expression for P(N=<em>k</em>)
    above.
</dd>

<dt>
   <a id="no_causation"></a>No causation without manipulation.
</dt>
<dd>
    A slogan attributed to <a href="http://statistics.berkeley.edu/brochure/holland.html">Paul
    Holland</a>. If the conditions were not deliberately manipulated (for example, if the
    situation is an <a class="glossRef" href="#study">observational study</a> rather than an
    <a class="glossRef" href="#experiment">experiment</a>),
    it is unwise to conclude that there is any causal relationship between the outcome and the
    conditions. See <a class="glossRef" href="#post_hoc">post hoc ergo propter hoc</a>
    and <a class="glossRef" href="#cum_hoc">cum hoc ergo propter hoc</a>.
</dd>

<dt>
   <a id="nonlinearity"></a>Nonlinear Association.
</dt>
<dd>
    The relationship between two variables is nonlinear if a change in one is associated
    with a change in the other that is depends on the value of the first; that is, if the
    change in the second is not simply proportional to the change in the first, independent of
    the value of the first variable.
</dd>

<dt>
   <a id="nonresponse"></a>Nonresponse.
</dt>
<dd>
    In surveys, it is rare that everyone who is ``invited'' to participate (everyone whose
    phone number is called, everyone who is mailed a questionnaire, everyone an interviewer
    tries to stop on the street&hellip;) in fact responds. The difference between the
    &quot;invited&quot; sample sought, and that obtained, is the nonresponse.
</dd>

<dt>
   <a id="nonresponse_bias"></a>Nonresponse bias.
</dt>
<dd>
    In a survey, those who respond may differ from those who do not, in ways that are
    related to the effect one is trying to measure. For example, a telephone survey of how
    many hours people work is likely to miss people who are working late, and are therefore
    not at home to answer the phone. When that happens, the survey may suffer from nonresponse
    bias. Nonresponse bias makes the result of a survey differ systematically
    from the truth.
</dd>

<dt>
   <a id="nonresponse_rate"></a>Nonresponse rate.
</dt>
<dd>
    The fraction of <a class="glossRef" href="#nonresponse">nonresponders</em> in a survey:
    the number of nonresponders divided by the number of people invited to participate
    (the number sent questionnaires, the number of interview attempts, etc.)
    If the nonresponse rate is appreciable, the survey suffer from large
    <a class="glossRef" href="#nonresponse_bias">nonresponse bias</a>.
</dd>

<dt>
    <a id="normal_approximation"></a>
    Normal approximation.
</dt>
<dd>
    The normal approximation to data is to approximate areas under the
    <a class="glossRef" href="#histogram">histogram</a>
    of data, transformed into <a class="glossRef" href="#standard_units">standard units</a>, by the
    corresponding areas under the <a class="glossRef" href="#normal_curve">normal curve</a>.
</dd>
<dd>
    Many probability distributions can be approximated by a normal distribution, in the
    sense that the area
    under the probability histogram is close to the area under a corresponding part of the
    normal curve. To find the corresponding part of the normal curve, the range must be
    converted to standard units, by subtracting the <a class="glossRef" href="#expectation">expected value</a>
    and dividing by the <a class="glossRef" href="#se">standard error</a>.
    For example, the area under the <a class="glossRef" href="#binomial">binomial</a>
    probability histogram for <em>n</em> = 50 and <em>p</em> =
    30% between 9.5 and 17.5 is 74.2%.  To use the normal approximation, we transform
    the endpoints to standard units, by subtracting the
    <a class="glossRef" href="#expectation" target="self">expected value</a> (for the Binomial
    random variable, <em>n</em>&times;<em>p</em> = 15
    for these values of <em>n</em> and <em>p</em>) and dividing the
    result by the <a class="glossRef" href="#se" target="self">standard error</a>
    (for a Binomial,
    <big>(</big><em>n</em> &times; <em>p</em> &times;
    (1&minus;<em>p</em>)<big>)</big><sup>1/2</sup>
    = 3.24 for these values of <em>n</em> and <em>p</em>).
    The area normal approximation is the area under the normal curve between
    (9.5 &minus; 15)/3.24 = &minus;1.697 and (17.5 &minus; 15)/3.24 = 0.772; that area is 73.5%, slightly
    smaller than the corresponding area under the binomial histogram. See also the
    <a class="glossRef" href="#continuity_correction">continuity
    correction</a>.
    The tool on <a href="../../Java/Html/BinHist.htm" target="lablet">this page</a>
    illustrates the normal approximation to the
    <a class="glossRef" href="#binomial">binomial</a>
    <a class="glossRef" href="#probability_histogram">probability histogram</a>.
    Note that the approximation gets worse when <em>p</em> gets close to 0 or 1, and
    that the approximation improves as <em>n</em> increases.
</dd>

<dt>
   <a id="normal_curve"></a>Normal curve.
</dt>
<dd>
    The normal curve is the familiar
    &quot;bell curve:,&quot; illustrated on
    <a href="../../Java/Html/NormHiLite.htm" target="lablet">this page</a>.
    The mathematical expression for the normal curve is
    <em>y</em> = (2&times;pi)<sup>&minus;&frac12;</sup>e<sup>&minus;<em>x<sup>2</sup>/2</sup></em>,
    where pi is the ratio of the circumference of a circle to its diameter
    (3.14159265&hellip;),
    and e is the base
    of the natural logarithm (2.71828&hellip;).
    The normal curve is symmetric around the point <em>x</em>=0, and
    positive for every value of <em>x</em>. The area under the normal curve is unity, and the
    SD of the normal curve, suitably defined, is also unity. Many (but not most)
    <a class="glossRef" href="#histogram">histograms</a>, converted into
    <a class="glossRef" href="#standard_units">standard units</a>,
    approximately follow the normal curve.
</dd>

<dt>
   <a id="normal_distrib"></a>Normal distribution.
</dt>
<dd>
A random variable X has a normal distribution with mean <em>m</em> and
  standard error <em>s</em> if for every pair of numbers <em>a</em> &le;
  <em>b</em>, the chance that <em>a</em> &lt; (X&minus;m)/s &lt; <em>b</em> is
  <p class="math">
  P(<em>a</em> &lt; (X&minus;m)/s &lt; <em>b</em>) = area under the normal curve between <em>a</em>
    and <em>b</em>.</p>
  If there are numbers <em>m</em> and <em>s</em> such that X has a normal
  distribution with mean <em>m</em> and standard error <em>s</em>, then X is said to have
  a normal distribution or to be normally distributed.  If X has a normal
  distribution with mean <em>m</em>=0 and standard error <em>s</em>=1, then X is said
  to have a standard normal distribution.  The notation X~N(m,s<sup>2</sup>) means that
  X has a normal distribution with mean <em>m</em> and
  standard error <em>s</em>; for example, X~N(0,1), means X has a standard normal distribution.

</dd>

<dt>
    <a id="NOT"></a>NOT, !, Negation, Logical Negation.
</dt>
<dd>
    The negation of a <a class="glossRef" href="#proposition">logical proposition</a> <em>p</em>,
    !<em>p</em>, is a proposition that is the logical opposite of
    <em>p</em>.
    That is, if <em>p</em> is true, !<em>p</em> is false, and
    if <em>p</em> is false, !<em>p</em> is true. Negation takes
    precedence over other logical operations.
    Other common symbols for the negation operator include &not;, &minus; and &tilde;.
</dd>

<dt>
   <a id="null_hypothesis"></a>Null hypothesis.
</dt>
<dd>
In <a class="glossRef" href="#hypothesis_test">hypothesis testing</a>, the hypothesis we wish to falsify
    on the basis of the data.  The null hypothesis is typically that something is not present,
    that there is no effect, or that there is no difference between treatment and control.
</dd>

<dd>
<p align="center"><!---O---> </p>
    <hr />
    <p><a id="o"></a>O </p>

</dd>

<dt>
   <a id="study"></a>Observational Study.
</dt>
<dd>
    <em>C.f.</em> <a class="glossRef" href="#controlled_experiment">controlled experiment</a>.
</dd>

<dt>
   <a id="odds"></a>Odds.
</dt>
<dd>
    The <em>odds in favor of an <a class="glossRef" href="#event">event</a></em> is the ratio
    of the <a class="glossRef" href="#probability">probability</a> that the event occurs to the
    probability that the
    event does not occur. For example, suppose an experiment can result in any of <em>n</em>
    possible outcomes, all equally likely, and that <em>k </em>of the outcomes result in a
    &quot;win&quot; and <em>n</em>&minus;<em>k</em> result in a &quot;loss.&quot; Then the chance of
    winning is <em>k</em>/<em>n</em>; the chance of not winning is
    (<em>n</em>&minus;<em>k</em>)/<em>n</em>;
    and the odds in favor of winning are
    (<em>k</em>/<em>n</em>)/<big>(</big>(<em>n</em>&minus;<em>k</em>)/<em>n</em><big>)</big>
    = <em>k</em>/(<em>n&minus;k</em>), which is the number of favorable outcomes divided by the
    number of unfavorable outcomes. Note that odds are not synonymous with probability, but
    the two can be converted back and forth. If the odds in favor of an event are <em>q</em>,
    then the probability of the event is <em>q</em>/(1+<em>q</em>). If the probability of an
    event is <em>p</em>, the odds in favor of the event are <em>p</em>/(1&minus;<em>p</em>) and the
    odds against the event are (1&minus;<em>p</em>)/<em>p</em>.
</dd>

<dt>
   <a id="one-sided_test"></a>One-sided Test.
</dt>
<dd>
    <em>C.f.</em> <a class="glossRef" href="#two-sided_test">two-sided test</a>.
    An hypothesis test of the <a class="glossRef" href="#null_hypothesis">null hypothesis</a>
    that the value of a parameter, &mu;, is equal to
    a null value, &mu;<sub>0</sub>, designed to have <a class="glossRef" href="#power">power</a> against either
    the <a class="glossRef" href="#alternative">alternative hypothesis</a> that &mu; &lt; &mu;<sub>0</sub>
    or the <a class="glossRef" href="#alternative">alternative</a> &mu; &gt; &mu;<sub>0</sub> (but not both).
    For example, a <a class="glossRef" href="#significance">significance level</a> 5%, one-sided
    <a class="glossRef" href="#z-test"><em>z</em> test</a>
    of the null hypothesis that the mean of a population equals zero against the alternative
    that it is greater than zero, would reject the null hypothesis for values of
    <div align="center"><center><table border="0" cellspacing="1">
      <tr>
    <td valign="middle" align="center"></td>
    <td valign="middle" align="center"></td>
    <td valign="middle" align="center"><a class="glossRef" href="#sample_mean">sample mean</a></td>
    <td valign="middle" align="center"></td>
    <td valign="middle" align="center"></td>
      </tr>
      <tr>
    <td valign="middle" align="center"><em>z</em></td>
    <td valign="middle" align="center">=</td>
    <td valign="middle" align="center">-------------------</td>
    <td valign="middle" align="center">&gt;</td>
    <td valign="middle" align="center">1.64.</td>
      </tr>
      <tr>
    <td valign="middle" align="center"></td>
    <td valign="middle" align="center"></td>
    <td valign="middle" align="center"><a class="glossRef" href="#se">SE</a>(<a class="glossRef" href="#sample_mean">sample
    mean</a>)</td>
    <td valign="middle" align="center"></td>
    <td valign="middle" align="center"></td>
      </tr>
    </table>
    </center></div>
</dd>

<dt>
    <a id="OR"></a>or, |, Disjunction, Logical Disjunction, &or;
</dt>
<dd>
    An operation on two <a class="glossRef" href="#proposition">logical propositions</a>.
    If <em>p</em> and <em>q</em> are two propositions, (<em>p</em> | <em>q</em>)
    is a proposition that is true if <em>p</em> is true or
    if <em>q</em> is true (or both); otherwise, it is false.  That is,
    (<em>p</em> | <em>q</em>) is true unless both <em>p</em> and <em>q</em>
    are false.
    The operation | is sometimes represented by the symbol &or; and sometimes by the
    word or. C.f.
    <a class="glossRef" href="#XOR">exclusive disjunction, XOR</a>.
</dd>

<dt>
   <a id="ordinal"></a>Ordinal Variable.
</dt>
<dd>
    A <a class="glossRef" href="#variable">variable</a> whose possible values have a natural order, such as
    {short, medium, long}, {cold, warm, hot}, or {0, 1, 2, 3, &hellip;}. In contrast, a variable
    whose possible values are {straight, curly} or {Arizona, California, Montana, New York}
    would not naturally be ordinal. Arithmetic with the possible values of an ordinal variable
    does not necessarily make sense, but it does make sense to say that one possible value is
    larger than another.
</dd>

<dt>
   <a id="outcome_space"></a>Outcome Space.
</dt>
<dd>
    The outcome space is the set of all possible outcomes of a given
    <a class="glossRef" href="#random_experiment">random experiment</a>.  The outcome space is often denoted
    by the capital letter <strong>S</strong>.
</dd>

<dt>
   <a id="outlier"></a>Outlier.
</dt>
<dd>
    An outlier is an observation that is many <a class="glossRef" href="#sd">SD</a>'s from the
    <a class="glossRef" href="#mean">mean</a>. It is sometimes tempting to discard outliers, but this is imprudent
    unless the cause of the outlier can be identified, and the outlier is determined to be
    spurious. Otherwise, discarding outliers can cause one to underestimate the true
    variability of the measurement process.
</dd>

<dd>
<p align="center"><!---P---> </p>
    <hr />
    <p><a id="p"></a>P </p>

</dd>

<dt>
   <a id="p-value"></a><em>P</em>-value.
</dt>
<dd>
    Suppose we have a family of <a class="glossRef" href="#hypothesis_test">hypothesis tests</a>
    of a <a class="glossRef" href="#null_hypothesis">null hypothesis</a> that let us test the
    hypothesis at any <a class="glossRef" href="#significance level">significance level</a>
    <em>p</em> between 0 and 100% we choose.
    The <em>P</em> value of the <a class="glossRef" href="#null_hypothesis">null hypothesis</a>
    given the data is the smallest significance level <em>p</em> for which
    any of the tests would have rejected the null hypothesis.
</dd>
<dd>
    For example, let <em>X</em> be a <a class="glossRef" href="#test_statistic">test statistic</a>,
    and for <em>p</em> between 0 and 100%, let <em>x</em><sub><em>p</em></sub> be
    the smallest number such that, under the null hypothesis,
</dd>
<dd>

    <p class="math">
    P( <em>X</em> &le; <em>x</em> ) &ge; <em>p</em>.
    </p>
</dd>
<dd>
    Then for any <em>p</em> between 0 and 100%, the rule
</dd>
<dd>
    <p class="math">
    reject the null hypothesis if <em>X</em> &lt; <em>x</em><sub><em>p</em></sub>
    </p>
</dd>
<dd>
    tests the null hypothesis at significance level <em>p</em>.
    If we observed <em>X</em>&nbsp;=&nbsp;<em>x</em>, the <em>P</em>-value of
    the null hypothesis given the data would be the smallest <em>p</em> such that
    <em>x</em>&nbsp;&lt;&nbsp;<em>x</em><sub><em>p</em></sub>.
</dd>

<dt>
   <a id="parameter"></a>Parameter.
</dt>
<dd>
    A numerical property of a <a class="glossRef" href="#population">population</a>, such as its
   <a class="glossRef" href="#mean">mean</a>.
</dd>

<dt>
    <a id="partition"></a>Partition.
</dt>
<dd>
    A <em>partition</em> of an <a class="glossRef" href="#event">event</a> <span class="math">A</span>
    is a collection of events
    {A<sub>1</sub>, A<sub>2</sub>, A<sub>3</sub>, &hellip; } such that the
    events in the collection are <a class="glossRef" href="#disjoint">disjoint</a>, and their
    <a class="glossRef" href="#union">union</a> is <span class="math">A</span>.
    That is,
</dd>
<p class="math">
    A<sub><em>j</em></sub>A<sub><em>k</em></sub> = {} unless <em>j</em> = <em>k</em>, and
</p>
<p class="math">
    A = A<sub>1</sub> &cup; A<sub>2</sub>
    &cup; A<sub>3</sub> &cup;
    &hellip; .
</p>
<dd>
    If the event <span class="math">A</span> is not specified, it is assumed to be the entire
    <a class="glossRef" href="#outcome_space">outcome space</a> <strong>S</strong>.
</dd>

<dt>
   <a id="payoff_matrix"></a>Payoff Matrix.
</dt>
<dd>
    A way of representing what each player in a game wins or loses, as a function of his and
    his opponent's <a class="glossRef" href="#strategy">strategies</a>.
</dd>

<dt>
   <a id="percentile"></a>Percentile.
</dt>
<dd>
    The <em>p</em>th percentile of a list is the smallest number such that at least
    <em>p</em>%
    of the numbers in the list are no larger than it.
    The <em>p</em>th percentile of a <a class="glossRef" href="#random_variable">random variable</a>
    is the smallest number such that the chance
    that the random variable is no larger than it is at least <em>p</em>%.
    <em>C.f.</em> <a class="glossRef" href="#quantile">quantile</a>.
</dd>

<dt>
   <a id="permutation"></a>Permutation.
</dt>
<dd>
    A permutation of a set is an arrangement of the elements of the set in some order. If
    the set has <em>n</em> things in it, there are <a class="glossRef" href="#factorial"><em>n</em>!</a>
    different orderings of its elements. For the first element in an ordering,
    there are <em>n</em>
    possible choices, for the second, there remain <em>n</em>&minus;1 possible choices, for the
    third, there are <em>n</em>&minus;2, <em>etc</em>., and for the <em>n</em>th element of the
    ordering, there is a single choice remaining. By the fundamental rule of counting, the
    total number of sequences is thus
    <em>n</em>&times;(<em>n</em>&minus;1)&times;(<em>n</em>&minus;2)&times;&hellip;&times;1.
    Similarly, the number of orderings of length <em>k</em> one
    can form from <em>n</em>&ge;<em>k</em> things
    is
    <em>n</em>&times;(<em>n</em>&minus;1)&times;(<em>n</em>&minus;2)&times;&hellip;&times;(<em>n</em>&minus;<em>k</em>+1) =
    <em>n</em>!/(<em>n&minus;k</em>)!. This
    is denoted <sub><em>n</em></sub>P<sub><em>k</em></sub>, the number of permutations of
    <em>n</em>
    things taken <em>k</em> at a time. <em>C.f.</em>
    <a class="glossRef" href="#combinations">combinations</a>.
</dd>

<dt>
   <a id="placebo"></a>Placebo.
</dt>
<dd>
    A &quot;dummy&quot; <a class="glossRef" href="#treatment">treatment</a> that has no pharmacological
    effect; <em>e.g.,</em> a sugar pill.
</dd>

<dt>
   <a id="placebo_effect"></a>Placebo effect.
</dt>
<dd>
    The belief or knowledge that one is being treated can itself have an effect that
    <a class="glossRef" href="#confounding">confounds</a> with the real effect of the treatment.
    Subjects given a <a class="glossRef" href="#placebo">placebo</a> as a pain-killer report statistically
    significant reductions
    in pain in randomized experiments that compare them with subjects who receive no treatment
    at all. This very real psychological effect of a placebo, which has no direct biochemical
    effect, is called the placebo effect. Administering a placebo to the
    <a class="glossRef" href="#control_group">control group</a> is thus important in experiments with human
    subjects; this is the essence of a <a class="glossRef" href="#blind">blind experiment</a>.
</dd>

<dt>
   <a id="point_of_averages"></a>Point of Averages.
</dt>
<dd>
    In a <a class="glossRef" href="#scatterplot">scatterplot</a>, the point whose coordinates are the
    <a class="glossRef" href="#mean">arithmetic means</a> of the corresponding variables. For example, if the
    variable X is plotted on the horizontal axis and the variable Y is plotted on the vertical
    axis, the point of averages has coordinates (mean of X, mean of Y).
</dd>

<dt>
   <a id="poisson"></a>Poisson Distribution.
</dt>
<dd>
    The Poisson distribution is a <a class="glossRef" href="#discrete">discrete</a>
    <a class="glossRef" href="#prob_distribution">probability distribution</a> that depends on one
    <a class="glossRef" href="#parameter">parameter</a>, <em>m</em>.
    If <em>X</em> is a <a class="glossRef" href="#random_variable">random variable</a> with
    the Poisson distribution with parameter <em>m</em>, then the probability that
    <em>X</em> = <em>k</em> is
</dd>
<p class="math">
    E<sup>&minus;<em>m</em></sup> &times; <em>m</em><sup><em>k</em></sup>/<em>k</em>!,
    <em>k</em> = 0, 1, 2, &hellip; ,
</p>
<dd>
    where E is the base of the natural logarithm and ! is the
    <a class="glossRef" href="#factorial">factorial</a> function.
    For all other values of <em>k</em>, the probability is zero.
</dd>
<dd>
    The <a class="glossRef" href="#expectation">expected value</a> the Poisson distribution with parameter
    <em>m</em> is <em>m</em>,
    and the <a class="glossRef" href="#se">standard error </a>of the Poisson distribution with parameter
    <em>m</em> is <em>m</em><sup>&frac12;</sup>.
</dd>

<dt>
   <a id="population"></a>Population.
</dt>
<dd>
    A collection of <a class="glossRef" href="#unit">units</a> being studied. <a class="glossRef" href="#unit">Units</a> can
    be people, places, objects, epochs, drugs, procedures, or many other things. Much of
    statistics is concerned with estimating numerical properties
    (<a class="glossRef" href="#parameter">parameters</a>)
    of an entire population from a <a class="glossRef" href="#random_sample">random sample</a> of
    <a class="glossRef" href="#unit">units</a> from the population.
</dd>

<dt>
   <a id="population_mean"></a>Population Mean.
</dt>
<dd>
    The <a class="glossRef" href="#mean">mean</a> of the numbers in a numerical population.
    For example, the population mean of a box of numbered tickets is the mean of
    the list comprised of all the numbers on all the tickets.
    The population mean is a <a class="glossRef" href="#parameter">parameter</a>. <em>C.f.</em>
    <a class="glossRef" href="#sample_mean">sample mean</a>.
</dd>

<dt>
   <a id="population_percentage"></a>Population Percentage.
</dt>
<dd>
    The percentage of <a class="glossRef" href="#unit">units</a> in a <a class="glossRef" href="#population">population</a>
    that possess a specified property. For example, the percentage of a given collection of
    registered voters who are registered as Republicans. If each unit that possesses the
    property is labeled with &quot;1,&quot; and each unit that does not possess the property
    is labeled with &quot;0,&quot; the population percentage is the same as the mean of that
    list of zeros and ones; that is, the population percentage is the
    <a class="glossRef" href="#population_mean">population mean</a> for a population of zeros and ones. The
    population percentage is a <a class="glossRef" href="#parameter">parameter</a>. <em>C.f.</em>
    <a class="glossRef" href="#sample_percentage">sample percentage</a>.
</dd>

<dt>
   <a id="population_sd"></a>Population Standard Deviation.
</dt>
<dd>
    The <a class="glossRef" href="#sd">standard deviation</a> of the values of a variable for a population.
    This is a <a class="glossRef" href="#parameter">parameter</a>, not a
    <a class="glossRef" href="#statistic">statistic</a>.
    <em>C.f.</em> <a class="glossRef" href="#sample_sd">sample standard deviation</a>.
</dd>

<dt>
   <a id="post_hoc"></a><em>Post hoc ergo propter hoc.</em>
</dt>
<dd>
    &quot;After this, therefore because of this.&quot; A fallacy of logic known since
    classical times: inferring a <a class="glossRef" href="#causation">causal relation</a> from
    <a class="glossRef" href="#correlation">correlation</a>. Don't do this at home!
</dd>

<dt>
   <a id="power"></a>Power.
</dt>
<dd>
    Refers to an <a class="glossRef" href="#hypothesis_test">hypothesis test</a>. The power of a test against
    a specific <a class="glossRef" href="#alternative">alternative hypothesis</a> is the chance that the test
    correctly rejects the <a class="glossRef" href="#null_hypothesis">null hypothesis</a> when the
    <a class="glossRef" href="#alternative">alternative hypothesis</a> is true.
</dd>

<dt>
   <a id="premise"></a>Premise, logical premise.
</dt>
<dd>
   A premise is a <a class="glossRef" href="#proposition">proposition</a> that is assumed to
   be true as part of a <a class="glossRef" href="#logicalArgument">logical argument</a>.
</dd>

<dt>
   <a id="prima_facie"></a><em>Prima facie.</em>
</dt>
<dd>
    Latin for &quot;at first glance.&quot; &quot;On the face of it.&quot;&nbsp; <em>Prima
    facie</em> evidence for something is information that at first glance supports the
    conclusion. On closer examination, that might not be true; there could be another
    explanation for the evidence.
</dd>

<dt>
   <a id="principle_of_insufficient_reason"></a>Principle of insufficient reason (Laplace)
</dt>
<dd>
   Laplace's <span class="termOfArt">principle of insufficient reason</span> says that if
   there is no reason to believe that the possible outcomes of an experiment are not
   <a class="glossRef" href="#equally_likely_outcomes">equally likely</a>, one should assume that the
   outcomes are equally likely.
   This is an example of a <a class="glossRef" href="#fallacy">fallacy</a> called
   <a class="glossRef" href="#appeal_to_ignorance">appeal to ignorance</a>.
</dd>

<dt>
   <a id="probability"></a>Probability.
</dt>
<dd>
The probability of an <a class="glossRef" href="#event">event</a> is a number between zero and 100%. The
    meaning (interpretation) of probability is the subject of
    <a class="glossRef" href="#probability_theories">theories
    of probability</a>, which differ in their interpretations. However, any rule for assigning
    probabilities to <a class="glossRef" href="#event">events</a> has to satisfy the
    <a class="glossRef" href="#axioms_of_probability">axioms of probability</a>.
</dd>

<dt>
   <a id="prob_density"></a>Probability density function.
</dt>
<dd>
The chance that a <a class="glossRef" href="#continuous">continuous random variable</a> is in any range
   of values can be calculated as the area under a curve over that range of values.  The
   curve is the probability density function of the random variable. That is, if X is
   a continuous random variable, there is a function <em>f</em>(x) such that for every
   pair of numbers <em>a</em>&le;<em>b</em>,

</dd>

   <p class="math">
   P(<em>a</em>&le; X &le;<em>b</em>) = (area under <em>f</em> between
   <em>a</em> and <em>b</em>);
   </p>
<dd>
  <em>f</em> is the probability density function of X. For example, the probability
   density function of a random variable with a <a class="glossRef" href="#normal_distrib">standard normal
   distribution</a> is the <a class="glossRef" href="#normal_curve">normal curve</a>.
   Only continuous
   random variables have probability density functions.
<dt>
   <a id="prob_distribution"></a>Probability Distribution.
</dt>
<dd>
    The probability distribution of a <a class="glossRef" href="#random_variable">random variable</a>
    specifies the chance that the variable takes a value in any subset of the real numbers.
    (The subsets have to satisfy some technical conditions that are not important for this
    course.) The probability distribution of a <a class="glossRef" href="#random_variable">random variable</a>
    is completely characterized by the <a class="glossRef" href="#cdf">cumulative probability distribution
    function</a>; the terms sometimes are used synonymously.
    The probability distribution
    of a <a class="glossRef" href="#discrete">discrete</a>
    <a class="glossRef" href="#random_variable">random variable</a> can be
    characterized by the chance that the <a class="glossRef" href="#random_variable">random variable</a> takes
    each of its possible values. For example, the probability distribution of the total number
    of spots S showing on the roll of two fair dice can be written as a table:
</dd>

</dl>
<div align="center">
<center>
<table border="1">
    <tr> <td>s</td> <td>P(S=s)</td> </tr> <tr> <td>2</td> <td>1/36</td> </tr> <tr> <td>3</td>
    <td>2/36</td> </tr> <tr> <td>4</td> <td>3/36</td> </tr> <tr> <td>5</td> <td>4/36</td> </tr>
    <tr> <td>6</td> <td>5/36</td> </tr> <tr> <td>7</td> <td>6/36</td> </tr> <tr> <td>8</td>
    <td>5/36</td> </tr> <tr> <td>9</td> <td>4/36</td> </tr> <tr> <td>10</td> <td>3/36</td> </tr>
    <tr> <td>11</td> <td>2/36</td> </tr> <tr> <td>12</td> <td>1/36</td> </tr>
</table>
</center>
</div>

<dl>
<dd>
The probability distribution
    of a <a class="glossRef" href="#continuous">continuous</a>
    <a class="glossRef" href="#random_variable">random variable</a> can be
    characterized by its <a class="glossRef" href="#prob_density">probability density function</a>.
</dd>

<dt>
   <a id="probability_histogram"></a>Probability Histogram.
</dt>
<dd>
A probability histogram for a <a class="glossRef" href="#random_variable">random variable</a> is
    analogous to a <a class="glossRef" href="#histogram">histogram</a> of data, but instead of plotting the
    area of the <a class="glossRef" href="#bin">bins</a> proportional to the relative frequency of observations
    in the <a class="glossRef" href="#class_interval">class interval</a>, one plots the area of the
    <a class="glossRef" href="#bin">bins</a> proportional to the probability that the
    <a class="glossRef" href="#random_variable">random
    variable</a> is in the <a class="glossRef" href="#class_interval">class interval</a>.
</dd>

<dt>
   <a id="probability_sample"></a>Probability Sample.
</dt>
<dd>
    A sample drawn from a population using a random mechanism so that every element of the
    population has a known chance of ending up in the sample.
</dd>

<dt>
   <a id="probability_theories"></a>Probability, Theories of.
</dt>
<dd>
A <em>theory of probability </em>is a way of assigning meaning to probability statements
    such as &quot;the chance that a thumbtack lands point-up is 2/3.&quot; That is, a theory
    of probability connects the mathematics of probability, which is the set of consequences
    of the <a class="glossRef" href="#axioms_of_probability">axioms of probability</a>, with the real world of
    observation and experiment. There are several common theories of probability. According to
    the <a id="frequency_theory"></a><em>frequency theory of probability</em>, the
    probability of an event is the limit of the percentage of times that the event occurs in
    repeated, independent trials under essentially the same circumstances.
    According to the <a id="subjective_theory"></a><em>subjective theory of
    probability</em>, a probability is a
    number that measures how strongly we believe an event will occur. The number is on a scale
    of 0% to 100%, with 0% indicating that we are completely sure it won't occur, and 100%
    indicating that we are completely sure that it will occur.
    According to the theory of <a id="equally_likely_outcomes"><span class="termOfArt">equally
    likely outcomes</span></a>, if an experiment has <em>n</em>
    possible outcomes, and (for example, by symmetry) there is no reason that any of the
    <em>n</em>
    possible outcomes should occur preferentially to any of the others, then the chance of
    each outcome is 100%/<em>n. </em>Each of these theories has its limitations, its
    proponents, and its detractors.
</dd>

<dt>
    <a id="proposition"></a>Proposition, logical proposition.
</dt>
<dd>
    A logical proposition is a statement that can be either true or false.
    For example, &quot;the sun is shining in Berkeley right now&quot; is a proposition.
    See also <a class="glossRef" href="#AND">&amp;</a>,
    <a class="glossRef" href="#IFF">&harr;</a>,
    <a class="glossRef" href="#IMPLIES">&rarr;</a>,
    <a class="glossRef" href="#OR">|</a>,
    <a class="glossRef" href="#XOR">XOR</a>,
    <a class="glossRef" href="#converse">converse</a>,
    <a class="glossRef" href="#contrapositive">contrapositive</a> and
    <a class="glossRef" href="#logicalArgument">logical argument</a>.
</dd>

<dt>
   <a id="prosecutors_fallacy"></a>Prosecutor's Fallacy.
</dt>
<dd>
   The prosecutor's fallacy consists of confusing two
   <a class="glossRef" href="#conditional_probability">conditional probabilities</a>:
   <span class="math">P(A|B)</span>
   and <span class="math">P(B|A)</span>.
   For instance, <span class="math">P(A|B)</span> could be the chance of observing the
   evidence if the accused is guilty, while <span class="math">P(B|A)</span> is the
   chance that the accused is guilty given the evidence.
   The latter might not make sense at all, but even when it does, the two numbers need not be equal.
   This fallacy is related to a common misinterpretation of <a class="glossRef" href="#p-value">P-values</a>.
</dd>

<dd>
<p align="center"><!---Q---> </p>
    <hr />
    <p><a id="q"></a>Q </p>
</dd>

<dt>
   <a id="qualitative"></a>Qualitative Variable.
</dt>
<dd>
A qualitative variable is one whose values are adjectives, such as colors, genders,
    nationalities, <em>etc</em>. <em>C.f. </em>
    <a class="glossRef" href="#quantitative">quantitative variable</a>
    and <a class="glossRef" href="#categorical">categorical variable</a>.
</dd>

<dt>
   <a id="quantile"></a>Quantile.
</dt>
<dd>
The <em>q</em>th quantile of a list (0 &lt; q &le; 1) is the smallest number such that
    the fraction <em>q </em>or more of the elements of the list are
    less than or equal to it. <em>I.e.,</em>
    if the list contains <em>n</em> numbers, the <em>q</em>th quantile, is the smallest number
    <em>Q</em> such that at least <em>n</em>&times;<em>q</em> elements of the
    list are less than or equal to <em>Q</em>.
</dd>

<dt>
   <a id="quantitative"></a>Quantitative Variable.
</dt>
<dd>
A variable that takes numerical values for which arithmetic makes sense, for example,
    counts, temperatures, weights, amounts of money, <em>etc.</em> For some variables that
    take numerical values, arithmetic with those values does not make sense; such variables
    are not quantitative. For example, adding and subtracting social security numbers does not
    make sense. Quantitative variables typically have units of measurement, such as inches,
    people, or pounds.
</dd>

<dt>
   <a id="quartiles"></a>Quartiles.
</dt>
<dd>
There are three quartiles. The first or lower quartile (LQ) of a list is a number (not
    necessarily a number in the list) such that at least 1/4 of the numbers in the list are no
    larger than it, and at least 3/4 of the numbers in the list are no smaller than it. The
    second quartile is the <a class="glossRef" href="#median">median</a>. The third or upper quartile (UQ) is a
    number such that at least 3/4 of the entries in the list are no larger than it, and at
    least 1/4 of the numbers in the list are no smaller than it. To find the quartiles, first
    sort the list into increasing order. Find the smallest integer that is at least as big as
    the number of entries in the list divided by four. Call that integer <em>k</em>.
    The <em>k</em>th
    element of the sorted list is the lower quartile. Find the smallest integer that is at
    least as big as the number of entries in the list divided by two. Call that integer
    <em>l</em>.
    The <em>l</em>th element of the sorted list is the median. Find the smallest integer that
    is at least as large as the number of entries in the list times 3/4.
    Call that integer <em>m</em>.
    The <em>m</em>th element of the sorted list is the upper quartile.
</dd>

<dt>
   <a id="quota"></a>Quota Sample.
</dt>
<dd>
    A quota sample is a sample picked to match the population with respect to some summary
    characteristics.
    It is not a <a class="glossRef" href="random_sample">random sample</a>.
    For example, in an opinion poll, one might select a sample so that the proportions of
    various ethnicities in the sample match the proportions of ethnicities in the overall
    population from which the sample is drawn.
    Matching on summary statistics does not guarantee that the sample comes close to
    matching the population with respect to the quantity of interest.
    As a result, quota samples are typically biased, and the size of the bias is generally
    impossible to determine unless the result can be compared with a known result for
    the whole population or for a random sample.
    Moreover, with a quota sample, it is impossible to quantify how
    representative of the population a quota sample is likely to be&mdash;quota sampling does not
    allow one to quantify the likely size of
    <a class="glossRef" href="#sampling_error">sampling error</a>.
    Quota samples are to be avoided, and results based on quota samples are to be viewed with
    suspicion.
    See also <a class="glossRef" href="#convenience_sample">convenience sample</a>.
<dd>

<p align="center"><!---R---> </p>
    <hr />
    <p><a id="r"></a>R </p>
</dd>

<dt>
   <a id="random_error"></a>Random Error.
</dt>
<dd>
    All measurements are subject to error, which can often be broken down into two
    components: a <a class="glossRef" href="#bias">bias</a> or <a class="glossRef" href="#systematic">systematic error</a>,
    which affects all measurements the same way; and a random error, which is in general
    different each time a measurement is made, and behaves like a number drawn with
    replacement from a box of numbered tickets whose average is zero.
</dd>

<dt>
   <a id="random_event"></a>Random Event.
</dt>
<dd>
See <a class="glossRef" href="#random_experiment">random experiment</a>.
</dd>

<dt>
   <a id="random_experiment"></a>Random Experiment.
</dt>
<dd>
    An experiment or trial whose outcome is not perfectly predictable, but for which the
    long-run relative frequency of outcomes of different types in repeated trials is
    predictable. Note that &quot;random&quot; is different from &quot;haphazard,&quot; which
    does not necessarily imply long-term regularity.
</dd>

<dt>
   <a id="random_sample"></a>Random Sample.
</dt>
<dd>
    A random sample is a <a class="glossRef" href="#sample">sample</a> whose members are chosen at random
    from a given <a class="glossRef" href="#population">population</a> in such a way that the chance of
    obtaining any particular sample can be computed.
    The number of units in the sample is called the <em>sample size</em>, often denoted
    <em>n</em>.
    The number of units in the population often is denoted <em>N</em>.
    Random samples can be drawn with or without replacing objects between draws; that is,
    drawing all <em>n</em> objects in the sample at once (a random sample without replacement),
    or drawing the objects one at a time, replacing them in the population between draws
    (a random sample with replacement).
    In a random sample with replacement, any given member of the population can occur in
    the sample more than once.
    In a random sample without replacement, any given member of the population can
    be in the sample at most once.
    A random sample without replacement in which every subset of <em>n</em> of the <em>N</em>
    units in the population is equally likely is also called a
    <a class="glossRef" href="#simple_random_sample">simple random sample</a>.
    The term <em>random sample with replacement</em> denotes a random sample drawn in such a
    way that every <em>n</em>-tuple of units in the population is equally likely.
    See also <a class="glossRef" href="#probability_sample">probability sample</a>.
</dd>

<dt>
   <a id="random_variable"></a>Random Variable.
</dt>
<dd>
   A random variable is an assignment of numbers to possible outcomes of a
   <a class="glossRef" href="#random_experiment">random experiment</a>. For example, consider tossing three
    coins. The number of heads showing when the coins land is a random variable: it assigns
    the number 0 to the outcome {T, T, T}, the number 1 to the outcome {T, T, H}, the number
    2 to the outcome {T, H, H}, and the number 3 to the outcome {H, H, H}.
</dd>

<dt>
   <a id="randomized"></a>Randomized Controlled Experiment.
</dt>
<dd>
An <a class="glossRef" href="#experiment">experiment</a> in which chance is deliberately introduced in
    assigning <a class="glossRef" href="#subject">subjects</a> to the <a class="glossRef" href="#treatment_group">treatment</a>
    and <a class="glossRef" href="#control_group">control</a> groups. For example, we could write an
    identifying number for each subject on a slip of paper, stir up the slips of paper, and
    draw slips without replacement until we have drawn half of them. The subjects identified
    on the slips drawn could then be assigned to treatment, and the rest to control.
    Randomizing the assignment tends to decrease <a class="glossRef" href="#confounding">confounding</a> of the
    treatment effect with other factors, by making the treatment and control groups roughly
    comparable in all respects but the treatment.
</dd>

<dt>
   <a id="range"></a>Range.
</dt>
<dd>
The range of a set of numbers is the largest value in the set minus the smallest value
    in the set. Note that as a statistical term, the range is a single number, not a range of
    numbers.
</dd>

<dt>
   <a id="real_number"></a>Real number
</dt>
<dd>
   Loosely speaking, the real numbers are all numbers that can be represented as fractions (rational numbers),
   whether proper or improper&mdash;and all numbers in between the rational numbers.
   That is, the real numbers comprise the rational numbers and all limits of Cauchy sequences of rational numbers,
   where the Cauchy sequence is with respect to the absolute value metric.
   (More formally, the real numbers are the completion of the set of rational numbers in the topology
   induced by the absolute value function.)
   The real numbers contain all integers, all fractions, and all irrational (and transcendental) numbers, such as
   <span class="math">&pi;</span>,
   <span class="math">e</span>, and <span class="math">2<sup>&frac12;</sup></span>.
   There are <a class="glossRef" href="#uncountable">uncountably</a> many real numbers between 0 and 1;
   in contrast, there are only
   <a class="glossRef" href="#countable">countably</a> many rational
   numbers between 0 and 1.
</dd>

<dt>
   <a id="regression"></a>Regression, Linear Regression.
</dt>
<dd>
    Linear regression fits a line to a <a class="glossRef" href="#scatterplot">scatterplot</a> in such a way
    as to minimize the sum of the squares of the <a class="glossRef" href="#residual">residuals</a>. The
    resulting regression line, together with the <a class="glossRef" href="#sd">standard deviations</a> of the
    two variables or their <a class="glossRef" href="#correlation_coef">correlation coefficient</a>, can be a
    reasonable summary of a scatterplot if the scatterplot is roughly football-shaped. In
    other cases, it is a poor summary. If we are regressing the variable Y on the variable X,
    and if Y is plotted on the vertical axis and X is plotted on the horizontal axis, the
    regression line passes through the <a class="glossRef" href="#point_of_averages">point of averages</a>, and
    has slope equal to the <a class="glossRef" href="#correlation_coef">correlation coefficient</a> times the
    <a class="glossRef" href="#sd">SD</a> of Y divided by the <a class="glossRef" href="#sd">SD</a> of X.
    <a href="../../Java/Html/ScatterPlot.htm" target="lablet">This page</a>
    shows a scatterplot, with a button to plot the regression line.
</dd>

<dt>
  <a id="regression_fallacy"></a>Regression Fallacy.
</dt>
<dd>
    The regression fallacy is to attribute the <a class="glossRef" href="#regression_toward_mean">regression
    effect</a> to an external cause.
</dd>

<dt>
   <a id="regression_toward_mean"></a>Regression Toward the Mean, Regression Effect.
</dt>
<dd>
    Suppose one measures two <a class="glossRef" href="#variable">variables</a> for each member of a group of
    individuals, and that the <a class="glossRef" href="#correlation_coef">correlation coefficient</a> of the
    variables is positive (negative). If the value of the first variable for that individual
    is above average, the value of the second variable for that individual is likely to be
    above (below) average, but by fewer <a class="glossRef" href="#sd">standard deviations</a> than the first
    variable is. That is, the second observation is likely to be closer to the mean in
    <a class="glossRef" href="#standard_units">standard units</a>. For example, suppose one measures the heights
    of fathers and sons. Each individual is a (father, son) pair; the two variables measured
    are the height of the father and the height of the son. These two variables will tend to
    have a positive correlation coefficient: fathers who are taller than average tend to have
    sons who are taller than average. Consider a (father, son) pair chosen at random from this
    group. Suppose the father's height is 3SD above the average of all the fathers' heights.
    (The SD is the <a class="glossRef" href="#sd">standard deviation</a> of the fathers' heights.) Then the
    son's height is also likely to be above the average of the sons' heights, but by fewer
    than 3SD (here the SD is the <a class="glossRef" href="#sd">standard deviation</a> of the sons' heights).

</dd>

<dt>
    <a id="rejection_region"></a>
    In an <a class="glossRef" href="#hypothesis_test">hypothesis test</a> using a <a class="glossRef" href="#test_statistic">test
    statistic, the </em>rejection region</em> is the set of values of the
    <a class="glossRef" href="#test_statistic">test statistic</a> for which
    we reject the <a class="glossRef" href="#null_hypothesis">null hypothesis</a>.

<dt>
   <a id="residual"></a>Residual.
</dt>
<dd>
    The difference between a datum and the value predicted for it by a model.
    In <a class="glossRef" href="#lin">linear regression</a> of a variable plotted on the vertical axis onto a
    variable plotted on the horizontal axis, a residual is the &quot;vertical&quot; distance
    from a datum to the line. Residuals can be positive (if the datum is above the line) or
    negative (if the datum is below the line). <a class="glossRef" href="#residual_plot">Plots of residuals</a>
    can reveal computational errors in linear regression, as well as conditions under which
    linear regression is inappropriate, such as <a class="glossRef" href="#nonlinearity">nonlinearity</a> and
    <a class="glossRef" href="#heteroscedasticity">heteroscedasticity</a>. If linear regression is performed
    properly, the sum of the residuals from the regression line must be zero; otherwise, there
    is a computational error somewhere.
</dd>

<dt>
   <a id="residual_plot"></a>Residual Plot.
</dt>
<dd>
A residual plot for a regression is a plot of the <a class="glossRef" href="#residual">residuals</a> from
    the regression against the <a class="glossRef" href="#explanatory_variable">explanatory</a> variable.
</dd>

<dt>
   <a id="resistant"></a>Resistant.
</dt>
<dd>
    A <a class="glossRef" href="#statistic">statistic</a> is said to be resistant if corrupting a datum
    cannot change the statistic much. The <a class="glossRef" href="#mean">mean</a> is not resistant; the
    <a class="glossRef" href="#median">median</a> is. See also <a class="glossRef" href="#breakdown_point">breakdown point</a>.

</dd>

<dt>
   <a id="rms"></a>Root-mean-square (RMS).
</dt>
<dd>
    The RMS of a list is the square-root of the mean of the squares of the elements in the
    list. It is a measure of the average &quot;size&quot; of the elements of the list. To
    compute the RMS of a list, you square all the entries, average the numbers you get, and
    take the square-root of that average.
</dd>

<dt>
   <a id="rmse"></a>Root-mean-square error (RMSE).
</dt>
<dd>
    The RMSE of an an <a class="glossRef" href="#estimator">estimator</a> of a
    <a class="glossRef" href="#parameter">parameter</a> is the square-root of the
    <a class="glossRef" href="#mse">mean squared error (MSE)</a>
    of the estimator.  In symbols, if X is an estimator of the parameter <em>t</em>, then
</dd>

<p class="math">
   RMSE(X) = <big>(</big>
   <a class="glossRef" href="#expectation">E</a><big>(</big> (X&minus;<em>t</em>)<sup>2</sup> <big>)</big>
   <big>)</big><sup>&frac12;</sup>.
</p>
<dd>
    The RMSE of an estimator is a measure of the expected error of the estimator.
    The units of RMSE are the same as the units of the estimator.
    See also <a class="glossRef" href="#mse" target="_self">mean squared error</a>.
</dd>

<dt>
     <a id="rms_error_of_regression"></a>   rms Error of Regression
</dt>
<dd>
    The rms error of regression is the <a class="glossRef" href="#rms">rms</a> of the
    vertical <a class="glossRef" href="#residual">residuals</a> from the regression line.
    For regressing Y on X, the rms error of regression is equal to
    (1&nbsp;&minus;&nbsp;<em>r</em><sup>2</sup>)<sup>&frac12;</sup>&times;SD<sub>Y</sub>,
    where <em>r</em> is the <a class="glossRef" href="#correlation_coef">correlation coefficient</a>
    between X and Y and SD<sub>Y</sub> is the <a class="glossRef" href="#sd">standard deviation</a>
    of the values of Y.
</dd>


<dd>
<p align="center"><!---S---> </p>
    <hr />
    <p><a id="s"></a>S</p>
</dd>

<dt>
  <a id="sample"></a>Sample.
</dt>
<dd>
     A sample is a collection of <a class="glossRef" href="#unit">units</a> from a
     <a class="glossRef" href="#population">population</a>.
     See also <a class="glossRef" href="#random_sample">random sample</a>.
</dd>

<dt>
  <a id="sample_mean"></a>Sample Mean.
</dt>
<dd>
    The arithmetic <a class="glossRef" href="#mean">mean</a> of a <a class="glossRef" href="#random_sample">random sample</a>
    from a population. It is a <a class="glossRef" href="#statistic">statistic</a> commonly used to estimate
    the <a class="glossRef" href="#population_mean">population mean</a>.
    Suppose there are <em>n</em> data, {<em>x</em><sub>1</sub>,
    <em>x</em><sub>2</sub>, &hellip; , <em>x</em><sub><em>n</em></sub>}.
    The sample mean is (<em>x</em><sub>1</sub> +
    <em>x</em><sub>2</sub> + &hellip; + <em>x</em><sub><em>n</em></sub>)/<em>n</em>.
    The <a class="glossRef" href="#expectation">expected
    value</a> of the sample mean is the <a class="glossRef" href="#population_mean">population mean</a>. For
    sampling with replacement, the SE of the sample mean is the population
    <a class="glossRef" href="#sd">standard
    deviation</a>, divided by the square-root of the <a class="glossRef" href="#sample_size">sample size</a>.
    For sampling without replacement, the SE of the sample mean is the
    <a class="glossRef" href="#finite_population_correction">finite-population correction</a>
    <big>(</big>(<em>N&minus;n</em>)/(<em>N</em>&minus;1)<big>)</big><sup>&frac12;</sup>
    times the SE of the sample mean for sampling with
    replacement, with <em>N</em> the size of the population and <em>n</em> the size of the
    sample.
</dd>

<dt>
   <a id="sample_percentage"></a>Sample Percentage.
</dt>
<dd>
    The percentage of a <a class="glossRef" href="#random_sample">random sample</a> with a certain property,
    such as the percentage of voters registered as Democrats in a
    <a class="glossRef" href="#simple_random_sample">simple random sample</a> of voters.
    The sample mean is a
    statistic commonly used to estimate the population percentage.
    The <a class="glossRef" href="#expectation">expected
    value</a> of the sample percentage from a <a class="glossRef" href="#simple_random_sample">simple random
    sample</a> or a random sample with replacement is the population percentage.
    The <a class="glossRef" href="#se">SE</a> of the sample percentage for sampling with replacement is
    <big>(</big><em>p</em>(1&minus;<em>p</em>)/<em>n</em>
    <big>)</big><sup>&frac12;</sup>, where <em>p</em> is
    the population percentage and <em>n</em> is the sample
    size. The <a class="glossRef" href="#se">SE</a> of the sample percentage for sampling without
    replacement is the <a class="glossRef" href="#finite_population_correction">finite-population correction</a>
    <big>(</big>(<em>N&minus;n</em>)/(<em>N</em>&minus;1)<big>)</big><sup>&frac12;</sup>
    times the SE of the sample percentage for sampling with
    replacement, with <em>N</em> the size of the population and <em>n</em> the size of the
    sample. The SE of the sample percentage is often estimated by the
    <a class="glossRef" href="#bootstrap">bootstrap</a>.
</dd>

<dt>
   <a id="sample_size"></a>Sample Size.
</dt>
<dd>
  The number of elements in a sample from a population.
</dd>

<dt>
   <a id="sound"></a>Sound argument.
</dt>
<dd>
  A <a class="glossRef" href="#logicalArgument">logical argument</a>
  is <em>sound</em> if it is <a class="glossRef" href="#validArgument">logically valid</a>
  and its <a class="glossRef" href="#premise">premises</a> are in fact true.
  An argument can be logically valid and yet not sound&mdash;if its premises
  are false.
</dd>

<dt>
   <a id="sample_sd"></a>Sample Standard Deviation, <em>S</em>.
</dt>
<dd>
    The sample standard deviation <em>S</em> is an estimator of the
    <a class="glossRef" href="#sd">standard deviation</a> of a population based on
    a <a class="glossRef" href="#random_sample">random sample</a> from the population.
    The sample standard deviation is a
    <a class="glossRef" href="#statistic">statistic</a>
    that measures how &quot;spread out&quot; the sample is around the
    <a class="glossRef" href="#sample_mean">sample
    mean</a>.
    It is quite similar to the <a class="glossRef" href="#sd">standard deviation</a> of the
    sample, but instead of averaging the squared <a class="glossRef" href="#deviation">deviations</a>
    (to get the <a class="glossRef" href="#rms">rms</a> of the
    <a class="glossRef" href="#deviation">deviations</a> of the data from the
    <a class="glossRef" href="#sample_mean">sample mean</a>) it divides the sum of the squared
    deviations by (number of data &minus; 1) before taking the square-root.
    Suppose there are <em>n</em> data, {<em>x</em><sub>1</sub>,
    <em>x</em><sub>2</sub>, &hellip; , <em>x</em><sub><em>n</em></sub>},
    with mean <em>M</em> = (<em>x</em><sub>1</sub> +
    <em>x</em><sub>2</sub> + &hellip; + <em>x</em><sub><em>n</em></sub>)/<em>n</em>.
    Then
  <p class="math">
   <em>s</em> = <big>(</big> ((<em>x</em><sub>1</sub> &minus; <em>M</em>)<sup>2</sup> +
   (<em>x</em><sub>2</sub> &minus; <em>M</em>)<sup>2</sup> + &hellip; +
   (<em>x</em><sub><em>n</em></sub> &minus; <em>M</em>)<sup>2</sup>)/(<em>n</em>&minus;1)
   <big>)</big><sup>&frac12;</sup>
  </p>
  <p>
    The square of the sample standard deviation,
    <a class="glossRef" href="#sample_variance">S<sup>2</sup> (the sample variance)</a>
    is an <a class="glossRef" href="#unbiased">unbiased</a>
    estimator of the square of the SD of the population (the variance of the population).
    </p>
</dd>

<dt>
   <a id="sample_sum"></a>Sample Sum.
</dt>
<dd>
    The sum of a <a class="glossRef" href="#random_sample">random sample</a> from a population.
    The <a class="glossRef" href="#expectation">expected value</a> of the sample sum is the
    <a class="glossRef" href="#sample_size">sample
    size</a> times the <a class="glossRef" href="#population_mean">population mean</a>.
    For sampling with replacement, the <a class="glossRef" href="#se">SE</a> of the sample sum is the
    population <a class="glossRef" href="#sd">standard deviation</a>, times the square-root of the
    <a class="glossRef" href="#sample_size">sample size</a>.
    For sampling without replacement, the SE of the sample sum is the
    <a class="glossRef" href="#finite_population_correction">finite-population correction</a>
    <big>(</big>(<em>N&minus;n</em>)/(<em>N</em>&minus;1)<big>)</big><sup>&frac12;</sup>
    times the SE of the sample sum for sampling with
    replacement, with <em>N</em> the size of the population and <em>n</em> the size of the
    sample.

</dd>

<dt>
  <a id="sample_survey"></a>Sample Survey.
</dt>
<dd>
    A survey based on the responses of a sample of individuals, rather than
    the entire population.
</dd>

<dt>
    <a id="sample_variance"></a>Sample Variance
</dt>
<dd>
    The sample variance is the square of the
    <a class="glossRef" href="#sample_sd">sample standard deviation <em>S</em></a>.
    It is an <a class="glossRef" href="#unbiased">unbiased</a>
    estimator of the square of the population <a class="glossRef" href="#sd">standard deviation</a>, which
    is also called the variance of the population.

</dd>

<dt>
  <a id="sampling_distribution"></a>Sampling distribution.
</dt>
<dd>
    The sampling distribution of an <a class="glossRef" href="#estimator">estimator</a> is the
    <a class="glossRef" href="#probability_distribution">probability distribution</a>
    of the estimator when it is applied to <a class="glossRef" href="#random_sample">random samples</a>.
    The tool on <a href="../../Java/Html/SampleDist.htm" target="lablet">this page</a>
    allows you to explore empirically the sampling distribution of the
    <a class="glossRef" href="#sample_mean">sample mean</a> and the
    <a class="glossRef" href="#sample_percentage">sample percentage</a> of
    random draws with or without replacement
    draws from a box of numbered tickets.
</dd>

<dt>
    <a id="sampling_error"></a>Sampling error.
</dt>
<dd>
    In estimating from a <a class="glossRef" href="#random_sample">random sample</a>, the difference between
    the <a class="glossRef" href="#estimator">estimator</a> and the <a class="glossRef" href="#parameter">parameter</a>
    can be written as the sum of two components: <a class="glossRef" href="#bias">bias</a> and
    sampling error.  The <a class="glossRef" href="#bias">bias</a> is the average error of the estimator
    over all possible samples. The bias is not random.
    Sampling error is the component of error that varies from sample to sample.
    The sampling error is random: it comes from &quot;the luck of the draw&quot;
    in which <a class="glossRef" href="#units">units</a> happen to be in the sample.
    It is the <a class="glossRef" href="#chance_variation">chance variation</a> of the estimator.
    The average of the sampling error over
    all possible samples (the <a class="glossRef" href="#expectation">expected value</a>
    of the sampling error) is zero.  The <a class="glossRef" href="#standard_error">standard error</a> of
    the estimator is a measure of the typical size of the sampling error.
</dd>


<dt>
    <a id="sampling_unit"></a>Sampling unit.
</dt>
<dd>
    A sample from a population can be drawn one <a class="glossRef" href="#unit">unit</a> at a time, or
    more than one unit at a time (one can sample clusters of units).
    The fundamental unit of the sample is called the <em>sampling unit</em>.
    It need not be a unit of the population.
</dd>

<dt>
   <a id="scatterplot"></a>Scatterplot.
</dt>
<dd>
    A scatterplot is a way to visualize <a class="glossRef" href="#bivariate">bivariate</a>
    data. A scatterplot is a plot of pairs of
    measurements on a collection of &quot;individuals&quot; (which need not be people). For
    example, suppose we record the heights and weights of a group of 100 people. The
    scatterplot of those data would be 100 points. Each point represents one person's height
    and weight. In a scatterplot of weight <em>against</em> height, the <em>x</em>-coordinate
    of each point would be height of one person, the <em>y</em>-coordinate of that point would
    be the weight of the same person. In a scatterplot of height against weight, the
    <em>x</em>-coordinates
    would be the weights and the <em>y</em>-coordinates would be the heights.
</dd>

<dt>
   <a id="scientific_method"></a><a id="scientific_method"></a>Scientific Method.
</dt>
<dd>
    The scientific method&hellip;.
</dd>

<dt>
   <a id="sd_line"></a>SD line.
</dt>
<dd>
   For a <a class="glossRef" href="#scatterplot">scatterplot</a>, a line that goes through the
  <a class="glossRef" href="#point_of_averages">point of averages</a>, with slope equal to the ratio of the
  <a class="glossRef" href="#sd">standard deviations</a> of the two plotted variables. If the variable plotted
    on the horizontal axis is called X and the variable plotted on the vertical axis is called
    Y, the slope of the SD line is the SD of Y, divided by the SD of X.
</dd>

<dt>
   <a id="secular_trend"></a>Secular Trend.
</dt>
<dd>
    A linear association (trend) with time.
</dd>

<dt>
   <a id="selection_bias"></a>Selection Bias.
</dt>
<dd>
    A systematic tendency for a sampling procedure to include and/or exclude
    <a class="glossRef" href="#unit">units</a>
    of a certain type. For example, in a <a class="glossRef" href="#quota">quota sample</a>, unconscious
    prejudices or predilections on the part of the interviewer can result in selection bias.
    Selection bias is a potential problem whenever a human has latitude in selecting
    individual units for the sample; it tends to be eliminated by
    <a class="glossRef" href="#probability_sample">probability sampling</a> schemes in which the interviewer is
    told exactly whom to contact (with no room for individual choice).
</dd>

<dt>
   <a id="self_selection"></a>Self-Selection.
</dt>
<dd>
    Self-selection occurs when individuals decide for themselves whether they
    are in the <a class="glossRef" href="#control_group">control group</a> or the
    <a class="glossRef" href="#treatment_group">treatment group</a>.
    Self-selection is quite common in studies of human behavior. For example, studies of the
    effect of smoking on human health involve self-selection: individuals choose for
    themselves whether or not to smoke. Self-selection precludes an
    <a class="glossRef" href="#experiment">experiment</a>;
    it results in an <a class="glossRef" href="#study">observational study</a>. When there is self-selection,
    one must be wary of possible confounding from factors that influence individuals'
    decisions to belong to the treatment group.
</dd>

<dt>
   <a id="set"></a>Set.
</dt>
<dd>
    A set is a collection of things, without regard to their order.
</dd>

<dt>
   <a id="significance"></a>Significance, Significance level, Statistical significance.
</dt>
<dd>
    The significance level of an <a class="glossRef" href="#hypothesis_test">hypothesis test</a> is
    the chance that the test erroneously rejects the <a class="glossRef" href="#null_hypothesis">null
    hypothesis</a> when the <a class="glossRef" href="#null_hypothesis">null hypothesis</a> is true.
</dd>

<dt>
   <a id="simple_random_sample"></a>Simple Random Sample.
</dt>
<dd>
    A simple random sample of <em>n</em> units from a population is a random sample drawn by
    a procedure that is equally likely to give every collection of <em>n</em> units from the
    population; that is, the probability that the sample will consist of any
    given subset of <em>n</em>
    of the <em>N</em> units in the population is 1/<sub><em>N</em></sub>C<sub><em>n</em></sub>.
    Simple random sampling is sampling at random without replacement (without replacing the
    units between draws).
    A simple random sample
    of size <em>n</em> from a population of <em>N</em> &ge;<em> n</em> units can be
    constructed by assigning a random number between zero and one to each unit in the
    population, then taking those units that were assigned the <em>n</em> largest random
    numbers to be the sample.
</dd>

<dt>
   <a id="simpson"></a>Simpson's Paradox.
</dt>
<dd>
   What is true for the parts is not necessarily true for the whole. See also
  <a class="glossRef" href="#confounding">confounding</a>.
</dd>

<dt>
   <a id="skew"></a>Skewed Distribution.
</dt>
<dd>
A distribution that is not <a class="glossRef" href="#symmetric_distribution">symmetrical</a>.
</dd>

<dt>
   <a id="spread"></a>Spread, Measure of.
</dt>
<dd>
See also <a class="glossRef" href="#IQR">inter-quartile range</a>, <a class="glossRef" href="#range">range</a>, and
  <a class="glossRef" href="#sd">standard deviation</a>.
</dd>

<dt>
   <a id="square_root_law"></a>Square-Root Law.
</dt>
<dd>
    The Square-Root Law says that the <a class="glossRef" href="#se">standard error</a> (<a class="glossRef" href="#se">SE</a>)
    of the <a class="glossRef" href="#sample_sum">sample sum</a> of <em>n</em> random draws with replacement
    from a box of tickets with numbers on them is
</dd>

<dd>
<p class="math"><a class="glossRef" href="#se">SE</a>(<a class="glossRef" href="#sample_sum">sample sum</a>) =
  <em>n</em><sup>&frac12;</sup>&times;SD(box),</p>

</dd>

<dd>
<p>and the <a class="glossRef" href="#se">standard error</a> of the
  <a class="glossRef" href="#sample_mean">sample
    mean</a> of <em>n</em> random draws with replacement from a box of tickets is</p>

</dd>

<dd>
<p class="math"><a class="glossRef" href="#se">SE</a>(<a class="glossRef" href="#sample_mean">sample mean</a>) =
  <em>n</em><sup>&minus;&frac12;</sup>&times;SD(box),</p>

</dd>

<dd>
where SD(box) is the <a class="glossRef" href="#sd">standard deviation</a> of the list of the numbers on
    all the tickets in the box (including repeated values).
</dd>

<dt>
   <a id="sd"></a>Standard Deviation (SD).
</dt>
<dd>
    The standard deviation of a set of numbers is the <a class="glossRef" href="#rms">rms</a> of the set of
    <a class="glossRef" href="#deviation">deviations</a> between each element of the set and the
    <a class="glossRef" href="#mean">mean</a>
    of the set.
    See also <a class="glossRef" href="#sample_sd">sample standard deviation</a>.
</dd>

<dt>
   <a id="se"></a>Standard Error (SE).
</dt>
<dd>
    The Standard Error of a <a class="glossRef" href="#random_variable">random variable</a> is a measure of
    how far it is likely to be from its <a class="glossRef" href="#expectation">expected value</a>; that is,
    its scatter in repeated experiments.
    The SE of a random variable X is defined to be
 <p class="math">
      SE(X) = <big>[</big><a class="glossRef" href="#expectation">E</a><big>(</big>
      (X &minus; <a class="glossRef" href="#expectation">E</a>(X))<sup>2</sup> <big>)</big><big>]
    </big><sup>&frac12;</sup>.
 </p>
   That is, the standard error is the square-root of the
  <a class="glossRef" href="#expectation">expected</a> squared difference between the random
  variable and its
    expected value. The SE of a random variable is analogous to the
    <a class="glossRef" href="#sd">SD</a> of a
    list.
</dd>

<dt>
   <a id="standard_normal"></a>Standard Normal Curve.
</dt>
<dd>
    See <a class="glossRef" href="#normal_curve">normal curve</a>.
</dd>

<dt>
   <a id="standard_units"></a>Standard Units.
</dt>
<dd>
    A variable (a set of data) is said to be in standard units if its
    <a class="glossRef" href="#mean">mean</a> is zero and
    its <a class="glossRef" href="#sd">standard deviation</a> is one. You
    <a class="glossRef" href="#transformation">transform</a>
    a set of data into standard units by subtracting the <a class="glossRef" href="#mean">mean</a> from each
    element of the list, and dividing the results by the <a class="glossRef" href="#sd">standard deviation</a>.
    A <a class="glossRef" href="#random_variable">random variable</a> is said to be in standard units
    if its <a class="glossRef" href="#expectation">expected value</a> is zero and its
    <a class="glossRef" href="#se">standard error</a> is
    one. You <a class="glossRef" href="#transformation">transform</a> a <a class="glossRef" href="#random_variable">random
    variable</a> to standard units by subtracting its <a class="glossRef" href="#expectation">expected value</a>
    then dividing by its <a class="glossRef" href="#se">standard error</a>.
</dd>

<dt>
   <a id="standardize"></a>Standardize.
</dt>
<dd>
To <a class="glossRef" href="#transformation">transform</a> into
  <a class="glossRef" href="#standard_units">standard units</a>.
</dd>

<dt>
   <a id="statistic"></a>Statistic.
</dt>
<dd>
    A number that can be computed from data, involving no unknown
    <a class="glossRef" href="#parameter">parameters</a>.
    As a function of a <a class="glossRef" href="#random_sample">random sample</a>,
    a statistic is a <a class="glossRef" href="#random_variable">random variable</a>.
    Statistics are used to estimate <a class="glossRef" href="#parameter">parameters</a>, and to
    <a class="glossRef" href="#hypothesis_test">test hypotheses</a>.
</dd>

<dt>
   <a id="stratified_sample"></a>Stratified Sample.
</dt>
<dd>
    In a stratified sample, subsets of sampling units are selected separately from different
    <a class="glossRef" href="#stratum">strata</a>, rather than from the
    <a class="glossRef" href="#frame">frame</a> as a whole.
</dd>

<dt>
    <a id="stratified_sample">Stratified sampling</a>
</dt>
<dd>
    The act of drawing a <a class="glossRef" href="#stratified_sample">stratified sample</a>.
</dd>

<dt>
    <a id="stratum"></a>Stratum
</dt>
<dd>
    In random sampling, sometimes the sample is drawn separately from different
    <a class="glossRef" href="#disjoint">disjoint</a>
    <a class="glossRef" href="#subset">subsets</a> of the population.
    Each such subset is called a <em>stratum</em>.
    (The plural of <em>stratum</em> is <em>strata</em>.)
    Samples drawn in such a way are called
    <a class="glossRef" href="#stratified_sample">stratified samples</a>.
    Estimators based on stratified random samples can have smaller
    <a class="glossRef" href="#sampling_error">sampling errors</a>
    than estimators computed from
    <a class="glossRef" href="#simple_random_sample">simple random samples</a>
    of the same size, if the average
    variability of the <a class="glossRef" href="#variable">variable</a> of interest within strata
    is smaller than it is across the entire <a class="glossRef" href="#population">population</a>; that is,
    if stratum membership is associated with the variable.
</dd>
<dd>
    For example, to determine average home prices in the U.S., it would be advantageous
    to <em>stratify</em> on geography, because average home prices vary enormously with
    location.
    We might divide the country into states, then divide each state into urban, suburban,
    and rural areas; then draw random samples separately from each such division.
</dd>

<dt>
  <a id="studentize"></a>Studentized score
</dt>
<dd>
    The observed value of a statistic, minus the expected value of the statistic,
    divided by the estimated standard error of the statistic.
</dd>

<dt>
  <a id="Student-t"></a>Student's <em>t</em> curve.
</dt>
<dd>
    Student's <em>t</em> curve is a family of curves indexed by a parameter called the
    <em>degrees of freedom</em>, which can take the values 1, 2, &hellip;
    Student's <em>t</em> curve is used to approximate some probability histograms.
    Consider a population of numbers that are
    <a class="glossRef" href="#nearly_normal">nearly normally distributed</a> and have
    <a class="glossRef" href="#population_mean">population mean</a> is &mu;.
    Consider drawing a <a class="glossRef" href="#random_sample">random sample</a>
    of size <em>n</em> with replacement from the
    population, and computing the <a class="glossRef" href="#sample_mean">sample mean</a>
    <em>M</em> and the <a class="glossRef" href="#sample_sd">sample standard deviation <em>S</em></a>.
    Define the <a class="glossRef" href="#random_variable">random variable</a>
</dd>

<p class="math">
    <em>T</em> = (<em>M</em> &minus; &mu;)/(<em>S</em>/<em>n</em><sup>&frac12;</sup>).
</p>

<dd>
    If the sample size <em>n</em> is large, the
    <a class="glossRef" href="#probability_histogram">probability histogram</a> of <em>T</em> can
    be approximated accurately by the <a class="glossRef" href="#normal_curve">normal curve</a>.
    However, for small and intermediate values of <em>n</em>, Student's <em>t</em> curve
    with <em>n</em>&nbsp;&minus;&nbsp;1 <em>degrees of freedom</em> gives a better approximation.
    That is,
</dd>

<p class="math">
    P(<em>a</em> &lt; <em>T</em> &lt; <em>b</em>) is approximately the area
    under Student's <em>T</em> curve with <em>n</em>&nbsp;&minus;&nbsp;1 degrees of freedom,
    from <em>a</em> to <em>b</em>.
</p>

<dd>
    Student's <em>t</em> curve can be used to test hypotheses about the population mean
    and construct confidence intervals for the population mean, when the population distribution
    is known to be <a class="glossRef" href="#nraely_normal">nearly normally distributed</a>.
    <a href="../../Java/Html/tHiLite.htm" target="lablet">This page</a>
    contains a tool that shows Student's <em>t</em> curve and lets you find the area under parts
    of the curve.
</dd>

<dt>
  <a id="subject"></a>Subject, Experimental Subject.
</dt>
<dd>
  A member of the <a class="glossRef" href="#control_group">control group</a> or the
  <a class="glossRef" href="#treatment_group">treatment group</a>.
</dd>

<dt>
   <a id="subset"></a>Subset.
</dt>
<dd>
A subset of a given set is a collection of things that belong to the original set. Every
    element of the subset must belong to the original set, but not every element of the
    original set need be in a subset (otherwise, a subset would always be identical to the set
    it came from).
</dd>

<dt>
   <a id="survey"></a>Survey.
</dt>
<dd>
See <a class="glossRef" href="#sample_survey">sample survey</a>.
</dd>

<dt>
   <a id="symmetric_distribution"></a>Symmetric Distribution.
</dt>
<dd>
The probability distribution of a random variable X is symmetric if
  there is a number <em>a</em>
    such that the chance that X&ge;<em>a</em>+<em>b</em> is the same as the chance that
    X&le;<em>a</em>&minus;<em>b</em> for every value of <em>b</em>. A list of numbers has a
    symmetric distribution if there is a number <em>a</em> such that the fraction of numbers
    in the list that are greater than or equal to <em>a</em>+<em>b</em> is the same as the
    fraction of numbers in the list that are less than or equal to <em>a</em>&minus;<em>b</em>, for
    every value of <em>b</em>. In either case, the histogram or the probability histogram will
    be symmetrical about a vertical line drawn at <em>x</em>=<em>a</em>.
</dd>

<dt>
   <a id="systematic"></a>Systematic error.
</dt>
<dd>
    An error that affects all the measurements similarly. For example, if a ruler is too
    short, everything measured with it will appear to be longer than it really is
    (ignoring <a class="glossRef" href="#random_error">random error</a>). If your watch runs fast,
    every time interval you
    measure with it will appear to be longer than it really is (again, ignoring
    <a class="glossRef" href="#random_error">random error</a>).
    Systematic errors do not tend to average out.
</dd>

<dt>
   <a id="systematic_sample"></a>Systematic sample.
</dt>
<dd>
    A systematic sample from a <a class="glossRef" href="#frame">frame</a> of
    <a class="glossRef" href="#units">units</a>
    is one drawn by listing the units and selecting every <em>k</em>th element of the list.
    For example, if there are <em>N</em> units in the frame, and we want a sample of size
    <em>N</em>/10, we would take every tenth unit:
    the first unit, the eleventh unit, the 21st unit, <em>etc</em>.
    Systematic samples are not <a class="glossRef" href="#simple_random_sample">random samples</a>,
    but they often behave essentially as if they were random, if the order in which
    the units appears in the list is haphazard.
    Systematic samples are a special case of cluster samples.
</dd>

<dt>
   <a id="systematic_random_sample"></a>Systematic random sample.
</dt>
<dd>
    A <a class="glossRef" href="#systematic_sample">systematic sample</a> starting at
    a random point in the listing of <a class="glossRef" href="#units">units</a>
    in the of <a class="glossRef" href="#frame">frame</a>, instead of starting at the first unit.
    Systematic random sampling is better than systematic sampling,
    but typically not as good as <a class="glossRef" href="#simple_random_sampling">simple random sampling</a>.
</dd>

<dd>
<p align="center"><!---T---> </p>
    <hr />
    <p><a id="t"></a>T </p>
</dd>

<dt>
    <a id="t_test"></a><em>t</em> test.
</dt>
<dd>
    An <a class="glossRef" href="#hypothesis_test">hypothesis test</a> based on approximating
    the <a class="glossRef" href="#probability_histogram">probability histogram</a>
    of the <a class="glossRef" href="#test_statistic">test statistic</a>
    by <a class="glossRef" href="#Student-t">Student's <em>t</em> curve</a>.
    <em>t</em> tests usually are used to test hypotheses about the
    <a class="glossRef" href="#population_mean">mean of a  population</a> when the
    <a class="glossRef" href="#sample_size">sample size</a> is intermediate and the distribution
    of the population is known to be <a class="glossRef" href="#nearly_normal">nearly normal</a>.
</dd>

<dt>
   <a id="test_statistic"></a>Test Statistic.
</dt>
<dd>
    A <a class="glossRef" href="#statistic">statistic</a> used to
    <a class="glossRef" href="#hypothesis_test">test hypotheses</a>.
    An hypothesis test can be constructed by deciding to reject the
    <a class="glossRef" href="#null_hypothesis">null
    hypothesis</a> when the value of the test statistic is in some range or collection of
    ranges.
    To get a test with a specified <a class="glossRef" href="#significance">significance level</a>, the
    chance when the null hypothesis is true that the test statistic falls in the range where
    the hypothesis would be rejected must be at most the specified significance level.
    The <a class="glossRef" href="#z-statistic"><em>Z</em> statistic</a> is a common test statistic.
</dd>

<dt>
   <a id="transformation"></a>Transformation.
</dt>
<dd>
Transformations turn lists into other lists, or variables into other variables. For
    example, to transform a list of temperatures in degrees Celsius into the corresponding
    list of temperatures in degrees Fahrenheit, you multiply each element by 9/5, and add 32 to
    each product. This is an example of an affine transformation: multiply by something and
    add something (<em>y</em> = <em>ax + b</em> is the general affine transformation of
    <em>x</em>;
    it's the familiar equation of a straight line). In a linear transformation, you only
    multiply by something (<em>y = ax</em>). Affine transformations are used to put variables
    in <a class="glossRef" href="#standard_units">standard units</a>. In that case, you subtract the
    <a class="glossRef" href="#mean">mean</a> and divide the results by the <a class="glossRef" href="#sd">SD</a>. This is
    equivalent to multiplying by the reciprocal of the <a class="glossRef" href="#sd">SD</a> and adding the
    negative of the <a class="glossRef" href="#mean">mean</a>, divided by the <a class="glossRef" href="#sd">SD</a>, so it is an
    affine transformation. Affine transformations with positive multiplicative constants have
    a simple effect on the <a class="glossRef" href="#mean">mean</a>, <a class="glossRef" href="#median">median</a>,
    <a class="glossRef" href="#mode">mode</a>, <a class="glossRef" href="#quartiles">quartiles</a>, and other
    <a class="glossRef" href="#percentile">percentiles</a>:
    the new value of any of these is the old one, transformed using exactly the same formula.
    When the multiplicative constant is negative, the <a class="glossRef" href="#mean">mean</a>,
    <a class="glossRef" href="#median">median</a>, <a class="glossRef" href="#mode">mode</a>, are still transformed by the same
    rule, but quartiles and percentiles are reversed: the <em>q</em>th quantile of the
    transformed distribution is the transformed value of the 1&minus;<em>q</em>th quantile of the
    original distribution (ignoring the effect of data spacing). The effect of an affine
    transformation on the <a class="glossRef" href="#sd">SD</a>, <a class="glossRef" href="#range">range</a>, and
    <a class="glossRef" href="#IQR">IQR</a>,
    is to make the new value the old value times the absolute value of the number you
    multiplied the first list by: what you added does not affect them.
</dd>

<dt>
   <a id="treatment"></a>Treatment.
</dt>
<dd>
    The substance or procedure studied in an <a class="glossRef" href="#experiment">experiment</a> or
    <a class="glossRef" href="#observational_study">observational study</a>.
    At issue is whether the treatment has an effect on the outcome or variable
    of interest.
</dd>

<dt>
   <a id="treatment_effect"></a>Treatment Effect.
</dt>
<dd>
    The effect of the <a class="glossRef" href="#treatment">treatment</a> on the variable of interest.
    Establishing whether the treatment has an effect is the point of an
    <a class="glossRef" href="#experiment">experiment</a>.
</dd>

<dt>
   <a id="treatment_group"></a>Treatment group.
</dt>
<dd>
    The individuals who receive the <a class="glossRef" href="#treat">treatment</a>, as opposed to those in
    the <a class="glossRef" href="#control_group">control group</a>, who do not.
</dd>

<dt>
   <a id="two-sided_test"></a>Two-sided Hypothesis test.
</dt>
<dd>
<em>C.f. </em><a class="glossRef" href="#one-sided_test">one-sided test</a>.
  An hypothesis test of the <a class="glossRef" href="#null_hypothesis">null hypothesis</a>
  that the value of a parameter, &mu;, is equal to
    a null value, &mu;<sub>0</sub>, designed to have <a class="glossRef" href="#power">power</a>
    against the
    <a class="glossRef" href="#alternative">alternative hypothesis</a> that either &mu; &lt; &mu;<sub>0</sub> or &mu;
    &gt; &mu;<sub>0</sub> (the <a class="glossRef" href="#alternative">alternative hypothesis</a> contains values
    on both sides of the null value).
    For example, a <a class="glossRef" href="#significance">significance level</a> 5%,
    two-sided <a class="glossRef" href="#z-test"><em>z</em> test</a> of the null hypothesis that
    the mean of a population equals zero against the alternative that it is greater than zero
    would reject the null hypothesis for values of
</dd>

<dd>
<div align="center"><center><table border="0" cellspacing="1">
      <tr>
    <td valign="middle" align="center"></td>
    <td valign="middle" align="center"></td>
    <td valign="middle" align="center">|(<a class="glossRef" href="#sample_mean">sample mean</a>)|</td>
    <td valign="middle" align="center"></td>
    <td valign="middle" align="center"></td>
      </tr>
      <tr>
    <td valign="middle" align="center"><em>|z|</em></td>
    <td valign="middle" align="center">=</td>
    <td valign="middle" align="center">-----------------</td>
    <td valign="middle" align="center">&gt;</td>
    <td valign="middle" align="center">1.96.</td>
      </tr>
      <tr>
    <td valign="middle" align="center"></td>
    <td valign="middle" align="center"></td>
    <td valign="middle" align="center"><a class="glossRef" href="#se">SE</a>(<a class="glossRef" href="#sample_mean">sample
    mean</a>)</td>
    <td valign="middle" align="center"></td>
    <td valign="middle" align="center"></td>
      </tr>
    </table>
    </center></div>
</dd>

<dt>
   <a id="type_error"></a>Type I and Type II errors.
</dt>
<dd>
    These refer to <a class="glossRef" href="#hypothesis_test">hypothesis testing</a>.
    A Type I error occurs when the <a class="glossRef" href="#null_hypothesis">null hypothesis</a>
    is rejected erroneously when it is in fact true.
    A Type II error occurs if the <a class="glossRef" href="#null_hypothesis">null hypothesis</a>
    is not rejected when it is in fact false.
    See also <a class="glossRef" href="#significance">significance level</a> and <a class="glossRef" href="#power">power</a>.
</dd>

<dd>
<p align="center"><!---U---> </p>
    <hr />
    <p><a id="u"></a>U </p>
</dd>

<dt>
   <a id="unbiased"></a>Unbiased.
</dt>
<dd>
Not <a class="glossRef" href="#biased">biased</a>; having zero bias.
</dd>

<dt>
   <a id="uncontrolled_experiment"></a>Uncontrolled Experiment.
</dt>
<dd>
    An <a class="glossRef" href="#experiment">experiment</a> in which there is no
    <a class="glossRef" href="#control_group">control group</a>; <em>i.e.</em>, in which the
    <a class="glossRef" href="#comparison">method of comparison</a> is not used: the experimenter
    decides who gets the <a class="glossRef" href="#treatment">treatment</a>, but
    the outcome of the treated group is not compared with the outcome of a
    <a class="glossRef" href="#control_group">control group</a> that does not receive
    <a class="glossRef" href="#treatment">treatment</a>.
</dd>

<dt>
  <a id="uncorrelated"></a>Uncorrelated.
</dt>
<dd>
    A set of <a class="glossRef" href="#bivariate">bivariate</a> data is uncorrelated if its
    <a class="glossRef" href="#correlation_coef">correlation coefficient</a> is zero.
    Two <a class="glossRef" href="#random_variable">random variables</a> are uncorrelated if the
    <a class="glossRef" href="#expectation">expected value</a> of their product equals the product
    of their <a class="glossRef" href="#expectation">expected values</a>.
    If two random variables are <a class="glossRef" href="#independent">independent</a>, they are
    uncorrelated.
    (The <a class="glossRef" href="#converse">converse</a> is not true in general.)
</dd>

<dt>
    <a id="uncountable"></a>Uncountable.
</dt>
<dd>
    A set is uncountable if it is not <a class="glossRef" href="#countable">countable</a>.
</dd>

<dt>
    <a id="unimodal"></a>Unimodal.
</dt>
<dd>
    Having exactly one <a class="glossRef" href="#mode">mode</a>.
</dd>

<dt>
    <a id="union"></a>Union.
</dt>
<dd>
    The union of two or more sets is the set of objects contained by at least one of the
    sets. The union of the <a class="glossRef" href="#event">events</a> A and B is denoted
    &quot;A&plus;B&quot;, &quot;A or B&quot;, and
    &quot;A&cup;B&quot;.
    <em>C.f. </em><a class="glossRef" href="#intersection">intersection</a>.
</dd>

<dt>
   <a id="unit"></a>Unit.
</dt>
<dd>
    A member of a <a class="glossRef" href="#population">population</a>.
</dd>

<dt>
   <a id="univariate"></a>Univariate.
</dt>
<dd>
    Having or having to do with a single <a class="glossRef" href="#variable">variable</a>.
    Some univariate techniques and
    <a class="glossRef" href="#statistic">statistics</a> include the <a class="glossRef" href="#histogram">histogram</a>,
    <a class="glossRef" href="#IQR">IQR</a>, <a class="glossRef" href="#mean">mean</a>, <a class="glossRef" href="#median">median</a>,
    <a class="glossRef" href="#percentile">percentiles</a>, <a class="glossRef" href="#quantile">quantiles</a>, and
    <a class="glossRef" href="#sd">SD</a>.
    <em>C.f. </em><a class="glossRef" href="#bivariate">bivariate</a>.
</dd>

<dt>
    <a id="uq"></a>Upper Quartile (UQ).
</dt>
<dd>
    See <a class="glossRef" href="#quartiles">quartiles</a>.
</dd>

<dd>
<p align="center"><!---V---> </p>
    <hr />
    <p><a id="v"></a>V </p>
</dd>

<dt>
    <a id="validArgument"></a>Valid (logical) argument.
</dt>
<dd>
    A valid <a class="glossRef" href="#logicalArgument">logical argument</a> is one in which
    the truth of the <a class="glossRef" href="#premises">premises</a> indeed guarantees the truth
    of the <a class="glossRef" href="#conclusion">conclusion</a>.
    For example, the following logical argument is valid:
    If the forecast calls for rain, I will not wear sandals.
    The forecast calls for rain.
    Therefore, I will not wear sandals.
    This argument has two premises which, together, guarantee the truth of
    the conclusion.
    An argument can be logically valid even if its premises are false.
    See also <a class="glossRef" href="#invalidArgument">invalid argument</a> and
    <a class="glossRef" href="#sound">sound argument</a>.
</dd>

<dt>
    <a id="variable"></a>Variable.
</dt>
<dd>
    A numerical value or a characteristic that can differ from individual to individual. See
    also <a class="glossRef" href="#categorical">categorical variable</a>,
    <a class="glossRef" href="#qualitative">qualitative variable</a>,
    <a class="glossRef" href="#quantitative">quantitative variable</a>,
    <a class="glossRef" href="#discrete">discrete variable</a>,
    <a class="glossRef" href="#continuous">continuous variable</a>, and
    <a class="glossRef" href="#random_variable">random variable</a>.
</dd>

<dt>
    <a id="variance"></a>Variance, population variance
</dt>
<dd>
    The variance of a list is the square of the <a class="glossRef" href="#sd">standard deviation</a>
    of the list, that is, the average of the squares of the deviations of the numbers
    in the list from their mean.
    The variance of a random variable X, Var(X),
    is the expected value of the squared difference between
    the variable and its expected value:
    Var(X) = E<big>(</big>(X &minus; E(X))<sup>2</sup><big>)</big>.
    The variance of a random variable is the square of the
    <a class="glossRef" href="#se">standard error (SE)</a>
    of the variable.
</dd>

<dt>
   <a id="venn_diagram"></a>Venn Diagram.
</dt>
<dd>
    A pictorial way of showing the relations among <a class="glossRef" href="#set">sets</a> or
    <a class="glossRef" href="#event">events</a>. The universal set or <a class="glossRef" href="#outcome_space">outcome space</a>
    is usually drawn as a rectangle; sets are regions within the rectangle. The overlap of the
    regions corresponds to the <a class="glossRef" href="#intersection">intersection</a> of the sets. If the
    regions do not overlap, the sets are <a class="glossRef" href="#disjoint">disjoint</a>. The part of the
    rectangle included in one or more of the regions corresponds to the
    <a class="glossRef" href="#union">union</a>
    of the sets.
    <a href="../../Java/Html/Venn.htm" target="lablet">This page</a>
    contains a tool that illustrates Venn diagrams; the tool represents the
    probability of an event by the area of the event.
</dd>

<dd>
<p align="center"><!---W---> </p>
    <hr />
    <p><a id="w"></a>W</p>
<hr />
</dd>

<dd>
<p><!---X--->
    <a id="x"></a>X
</p>
</dd>

<dt>
    <a id="XOR"></a>XOR, exclusive disjunction.
</dt>
<dd>
    XOR is an operation on two <a class="glossRef" href="#proposition">logical propositions</a>.
    If <em>p</em> and <em>q</em> are two <a class="glossRef" href="#proposition">propositions</a>,
    (<em>p</em> XOR <em>q</em>) is a proposition that
    is true if either <em>p</em> is true or if <em>q</em> is true, but not both.
    (<em>p</em> XOR <em>q</em>) is
     <a class="glossRef" href="#logicallyEquivalent">logically equivalent</a> to
    ((<em>p</em> | <em>q</em>) &amp; !(<em>p</em> &amp; <em>q</em>)).
</dd>

<dd>
<hr />
<p>
    <!---Y--->
    <a id="y"></a>Y
</p>
</dd>

<dd>
<hr />
<p>
     <!---Z--->
    <a id="z"></a>Z
</p>
</dd>

<dt>
    <a id="z-score"></a><em>z</em>-score
</dt>
<dd>
    The observed value of the <a class="glossRef" href="#z-statistic"><em>Z</em> statistic</a>.
</dd>

<dt>
   <a id="z-statistic"></a><em>Z</em> statistic
</dt>
<dd>
    A <em>Z</em> statistic is a <a class="glossRef" href="#test_statistic">test statistic</a>
    whose distribution under the <a class="glossRef" href="#null_hypothesis">null hypothesis</a>
    has <a class="glossRef" href="#expectation">expected value</a> zero and can be
    approximated well by the <a class="glossRef" href="#normal curve">normal curve</a>.
    Usually, <em>Z</em> statistics are constructed by
    <a class="glossRef" href="#standardize">standardizing</a>
    some other <a class="glossRef" href="#statistic">statistic</a>.
    The <em>Z</em> statistic is related to the
    original statistic by
</dd>

<dd>
<p class="math">
   <em>Z</em> = (original &minus;
  <a class="glossRef" href="#expectation">expected
    value</a> of original)/<a class="glossRef" href="#se">SE</a>(original).
</p>
</dd>

<dt>
   <a id="z-test"></a><em>z</em>-test
</dt>
<dd>
  An <a class="glossRef" href="#hypothesis_test">hypothesis test</a> based on approximating the
  <a class="glossRef" href="#probability_histogram">probability histogram</a> of the
  <a class="glossRef" href="#z-statistic"><em>Z</em> statistic</a>
  under the <a class="glossRef" href="#null_hypothesis">null hypothesis</a>
  by the <a class="glossRef" href="#normal_curve">normal curve</a>.
</dd>

</dl>

<hr />

<script language="JavaScript1.2"><!--
    writeMiscFooter(true);
// -->
</script>

</body>
</html>
